No,Title,Abstract,Abstract_gpt4o,Abstract_llama3,Abstract_gemini
32,Olympus: a GDPR compliant blockchain system,"Blockchain has been gaining significant interest in several domains. However, this technology also raises relevant challenges, namely in terms of data protection. After the General Data Protection Regulation (GDPR) has been published by the European Union, companies worldwide changed the way they process personal data. This project provides a model and implementation of a blockchain system to store personal data complying with GDPR. We examine the advantages and challenges and evaluate the system. We use Hyperledger Fabric as blockchain, Interplanetary File System to store personal data off-chain, and a Django REST API to interact with both the blockchain and the distributed file system. Olympus has three possible types of users: Data Subjects, Data Processors and Data Controllers and a fourth participant, Supervisor Authority, that, despite not being an explicit role, can perform all verifications that GDPR mandates. We conclude that it is possible to create a system that overcomes the major challenges of storing personal data in a blockchain (Right to be Forgotten and Right to Rectification), while maintaining its desirable characteristics (auditability, verifiability, tamper resistance, distributed—remove single points of failure) and complying with GDPR.","Several sectors have shown a growing interest in blockchain technology, though it also introduces considerable challenges, particularly regarding data privacy. Following the European Union's establishment of the General Data Protection Regulation (GDPR), global companies altered their personal data processing methods. This project offers a model and implementation for a blockchain system designed to store personal data in compliance with GDPR. We explore the benefits and difficulties, and assess the system's performance. The project utilizes Hyperledger Fabric for blockchain, the Interplanetary File System for off-chain personal data storage, and a Django REST API to facilitate communication between the blockchain and the distributed file system. Olympus identifies three user roles: Data Subjects, Data Processors, and Data Controllers, and includes a fourth entity, Supervisory Authority, which, though not an explicit role, can conduct all GDPR-required verifications. Our findings indicate that it is feasible to develop a system addressing the principal issues of storing personal data on a blockchain, such as the Right to be Forgotten and Right to Rectification, while preserving essential features like auditability, verifiability, tamper resistance, decentralized architecture, and GDPR compliance.","Across various industries, blockchain technology has garnered substantial attention, yet it also presents notable hurdles, particularly with regard to safeguarding sensitive information. Following the European Union's introduction of the General Data Protection Regulation (GDPR), companies globally have revamped their approach to handling personal data. This project develops and implements a blockchain-based system for storing personal data in compliance with GDPR regulations. We assess the benefits and drawbacks of this system, which leverages Hyperledger Fabric as the blockchain framework, Interplanetary File System for off-chain data storage, and a Django REST API to facilitate interactions between the blockchain and distributed file system. The Olympus system accommodates four distinct roles: Data Subjects, Data Processors, Data Controllers, and the Supervisor Authority, which, although not an explicit role, is responsible for conducting all GDPR-mandated verifications. Ultimately, our findings suggest that it is feasible to design a system that surmounts the primary challenges associated with storing personal data on a blockchain (namely, the Right to be Forgotten and Right to Rectification), while preserving its desirable attributes (auditability, verifiability, tamper resistance, and decentralization).","The increasing popularity of blockchain technology across various fields has been accompanied by significant data protection concerns. The European Union's General Data Protection Regulation (GDPR) prompted global companies to revamp their personal data processing practices. This initiative presents a GDPR-compliant model and implementation of a blockchain system for personal data storage. Through an evaluation of the system's strengths and limitations, we demonstrate its feasibility. The system leverages Hyperledger Fabric as the blockchain framework, the Interplanetary File System for off-chain personal data storage, and a Django REST API to facilitate interactions with both the blockchain and the distributed file system. Olympus accommodates three user types: Data Subjects, Data Processors, and Data Controllers. A fourth participant, the Supervisor Authority, while not an explicit role, is empowered to conduct all GDPR-mandated verifications. Our findings suggest that it is feasible to develop a system that addresses the key challenges of storing personal data on a blockchain (Right to be Forgotten and Right to Rectification) while preserving its advantageous attributes (auditability, verifiability, tamper resistance, and distributed nature for single point of failure elimination) and adhering to GDPR requirements."
67,"Opportunities, challenges, and benefits of AI innovation in government services: a review","Artificial intelligence (AI) has emerged as an excellent tool across multiple industries and holds great promise for the government, society, and economy. However, the absence of a distinct consensus regarding the definition and scope of artificial intelligence hinders its practical implementation in government settings. This article examines the various methodologies, emphases, and goals within artificial intelligence, emphasizing its ability to enhance human capabilities in critical situations. Considering the present advantages and enhanced productivity brought about by AI adoption in trailblazing government departments, this study explores the possible benefits and limitations of AI usage in the public sector. By looking at the cross-disciplinary difficulties of public AI applications, such as language hurdles and service delays, this study highlights the necessity for a thorough knowledge of the risks, impediments, and incentives of employing AI for government services. The study hopes to provide insight into AI research's ultimate aims, including object manipulation, natural language processing, and reasoning. This study emphasizes the potential for greater productivity, simplified procedures, and reduced obligations by analyzing the pros and cons of using AI in the public sector. Further, organizational theory is considered a tool for figuring out how to deal with challenges and maximize possibilities associated with AI deployment. The theory is used as the conceptual framework to understand the benefits, opportunities, and challenges involved in using AI when providing government services. The results of this research help us better understand how AI may revolutionize public service delivery by stimulating new ideas and improving efficiency. This study covers critical questions about organizational theory's role in improving government AI adoption, the challenges governments have in adopting AI, and the potential benefits AI might offer public service delivery. The research recommends a strategic approach to AI adoption in the public sector, considering organizational, ethical, and societal implications while recognizing the possibility of AI's transformative impacts on governments' service provision.","Artificial intelligence (AI) has gained recognition as a valuable asset across various industries and shows significant potential for government, society, and the economy. Nonetheless, the lack of a clear consensus on the definition and boundaries of AI obstructs its effective application in government contexts. This article delves into the diverse methodologies, focal points, and objectives within AI, underscoring its capacity to augment human capabilities in vital scenarios. Given the current benefits and increased efficiency achieved through AI adoption in pioneering government agencies, this study investigates the potential advantages and limitations of AI use in the public sector. By addressing the interdisciplinary challenges of public AI applications, such as language barriers and service delays, the study underscores the necessity for a comprehensive understanding of the risks, obstacles, and incentives associated with utilizing AI for government services. The study aims to shed light on the ultimate goals of AI research, including object manipulation, natural language processing, and reasoning. By evaluating the pros and cons of AI in the public sector, this study highlights the potential for enhanced productivity, streamlined processes, and reduced responsibilities. Additionally, organizational theory is employed as a framework to address challenges and capitalize on opportunities related to AI implementation. This theoretical approach helps to understand the benefits, prospects, and difficulties of integrating AI into government services. The findings of the research provide insights into how AI can revolutionize public service delivery by fostering innovation and boosting efficiency. The study addresses crucial questions about the role of organizational theory in facilitating government AI adoption, the challenges faced by governments in incorporating AI, and the potential benefits AI could bring to public service delivery. The research advocates for a strategic approach to AI in the public sector, taking into account organizational, ethical, and societal factors while acknowledging AI's transformative potential in enhancing government service delivery.","The advent of artificial intelligence (AI) has spawned a versatile instrument with far-reaching implications for governance, societal welfare, and economic growth. Nevertheless, the lack of a unified understanding of AI's definition and scope hampers its effective integration into government agencies. This examination delves into the diverse approaches, focal points, and objectives within AI, underscoring its capacity to augment human capabilities in high-stakes situations. Building upon the current advantages and enhanced efficiency achieved through AI adoption in pioneering government departments, this inquiry explores the potential benefits and limitations of AI utilization in the public sphere. By scrutinizing the interdisciplinary hurdles of public AI applications, such as linguistic barriers and service delays, this study underscores the imperative for a comprehensive grasp of the risks, obstacles, and incentives associated with AI deployment in government services. The study aims to provide insight into AI research's ultimate objectives, including object manipulation, natural language processing, and reasoning. This analysis highlights the potential for increased productivity, streamlined processes, and reduced liabilities by weighing the pros and cons of AI adoption in the public sector. Furthermore, organizational theory is leveraged as a tool to navigate challenges and maximize opportunities associated with AI implementation. The theory serves as the conceptual framework to understand the benefits, opportunities, and challenges inherent in AI-driven government service provision. The research findings facilitate a deeper understanding of how AI may revolutionize public service delivery by catalyzing innovation and enhancing efficiency. This study addresses critical questions regarding organizational theory's role in optimizing government AI adoption, the challenges governments face in adopting AI, and the potential benefits AI may offer public service delivery. The research advocates for a strategic approach to AI adoption in the public sector, considering organizational, ethical, and societal implications while acknowledging the possibility of AI's transformative impacts on governments' service provision.","Across various sectors, AI's potential is undeniable, promising significant advancements for government, societal, and economic landscapes. However, the lack of a unified understanding of AI's definition and reach creates obstacles for its practical integration within government. This research delves into the diverse methods, focuses, and objectives within the field of AI, highlighting its capacity to augment human capabilities in crucial situations. Examining the early successes and efficiency gains achieved by pioneering government agencies utilizing AI, this study investigates the potential advantages and drawbacks of AI implementation in public services. By analyzing the interdisciplinary challenges of public AI applications, such as linguistic barriers and service delays, this research stresses the need for a comprehensive understanding of the risks, obstacles, and motivators associated with employing AI in government. The study aims to shed light on the ultimate goals of AI research, encompassing areas like object manipulation, natural language processing, and logical reasoning. Through an assessment of the benefits and challenges of AI in the public sector, this research underscores the potential for increased productivity, streamlined processes, and minimized burdens. Furthermore, organizational theory is utilized as a framework to address challenges and capitalize on opportunities presented by AI integration. This theoretical lens provides insights into the benefits, opportunities, and challenges inherent in leveraging AI for government service delivery. This study's findings contribute to a deeper understanding of AI's potential to revolutionize public services by fostering innovation and enhancing efficiency. It addresses key questions regarding organizational theory's role in promoting AI adoption in government, the obstacles faced by governments in adopting AI, and the potential benefits AI offers to public service provision. The research advocates for a strategic approach to AI implementation in the public sector, taking into account organizational, ethical, and societal factors, while recognizing AI's transformative potential for government service delivery."
124,How to design an AI ethics board,"The development and deployment of artificial intelligence (AI) systems poses significant risks to society. To reduce these risks to an acceptable level, AI companies need an effective risk management process and sound risk governance. In this paper, we explore a particular way in which AI companies can improve their risk governance: by setting up an AI ethics board. We identify five key design choices: (1) What responsibilities should the board have? (2) What should its legal structure be? (3) Who should sit on the board? (4) How should it make decisions? (5) And what resources does it need? We break each of these questions down into more specific sub-questions, list options, and discuss how different design choices affect the board’s ability to reduce societal risks from AI. Several failures have shown that designing an AI ethics board can be challenging. This paper provides a toolbox that can help AI companies to overcome these challenges.","The creation and implementation of artificial intelligence (AI) systems present substantial dangers to society. To mitigate these risks to an acceptable threshold, AI firms need a robust risk management strategy and solid risk governance frameworks. In this article, we delve into a specific method for enhancing risk governance in AI firms: establishing an AI ethics board. We outline five crucial design considerations: (1) What duties should the board undertake? (2) What legal configuration should it have? (3) Who should be included on the board? (4) What decision-making processes should it follow? (5) And what resources are necessary for its function? We address each of these queries with detailed sub-questions, provide various options, and evaluate how different design choices impact the board's efficacy in mitigating AI-related societal risks. Numerous failures highlight the complexities in devising an AI ethics board. This paper equips AI firms with a toolkit to navigate these challenges effectively.","""The integration and implementation of artificial intelligence (AI) systems entails substantial perils to societal well-being. To mitigate these threats to a tolerable threshold, AI corporations require a robust risk mitigation framework and prudent governance structures. This study examines a specific strategy for enhancing risk governance in AI companies: the establishment of an AI ethics committee. We delineate five pivotal design considerations: (1) What authority should the committee wield? (2) What legal framework should it operate within? (3) Who should comprise the committee's membership? (4) How should it arrive at decisions? (5) And what resources are necessary for its functioning? We deconstruct each of these inquiries into more nuanced sub-inquiries, catalog potential options, and analyze how disparate design decisions influence the committee's capacity to alleviate societal perils associated with AI. The history of failed attempts underscores the complexity of designing an AI ethics committee. This paper offers a comprehensive guide to aid AI corporations in surmounting these obstacles.""","The burgeoning field of artificial intelligence presents considerable societal dangers that must be mitigated. To ensure these risks are minimized, organizations developing AI technologies require robust risk management strategies and ethical oversight. This study proposes the establishment of dedicated AI ethics boards as a crucial step towards achieving responsible AI development. We delve into five critical aspects of board design: the scope of their duties, their legal constitution, the composition of their membership, their decision-making processes, and the resources they require to function effectively. Each of these facets is further deconstructed into specific sub-questions, presenting diverse options and analyzing how different configurations influence the board's capacity to mitigate AI-associated risks to society. Past shortcomings underscore the intricacies inherent in designing such boards; this paper furnishes a pragmatic framework to navigate these challenges and empower AI companies to navigate the ethical dimensions of their work."
165,Should firms invest more in cybersecurity?,"We combine unique survey data on IT use and administrative tax record data on Dutch firms to understand how cybersecurity investments relate to the probability of cyber incidents and firm profitability. This dataset allows us to control for firm size, industry, and IT organization. We construct a new indicator to measure the degree of cyber maturity of firms and find that this maturity level tends to increase with firm size. Regression analyses suggest that the relation between maturity level and probability of a cyber incident is inverted U-shaped: a higher maturity level is initially associated with a higher incident probability, but the highest maturity level is associated with fewer reported incidents. This finding is consistent with the hypothesis that basic cybersecurity measures enable better detection of incidents and more sophisticated measures help to prevent incidents. We do not find, however, evidence for a positive relation between cybersecurity measures and profits.","We integrate distinct survey data regarding IT usage with administrative tax record data on Dutch companies to investigate the link between cybersecurity investments, the likelihood of cyber incidents, and company profitability. This dataset permits us to adjust for factors such as company size, industry, and IT infrastructure. We create a novel metric to gauge firms' cyber maturity and observe that larger firms typically exhibit higher levels of maturity. Our regression analysis indicates an inverted U-shaped relationship between cyber maturity and the likelihood of a cyber incident: as maturity levels increase, the probability of an incident initially rises but subsequently declines at the highest maturity levels. This observation aligns with the hypothesis that basic cybersecurity measures improve incident detection, whereas advanced measures contribute to incident prevention. However, we do not find evidence to support a positive correlation between cybersecurity measures and profitability.","By integrating distinctive survey insights on IT adoption with administrative tax records for Dutch companies, we examine the correlation between investments in cybersecurity and the likelihood of cyber breaches, as well as their impact on business profitability. This comprehensive dataset enables us to account for variations in company scale, sector, and IT infrastructure. We develop a novel metric to assess the cybersecurity sophistication of firms, revealing a positive correlation with company size. Our regression models indicate that the relationship between cybersecurity sophistication and incident probability follows an inverted U-curve, where higher maturity initially corresponds to increased incident likelihood, but the most advanced maturity levels are associated with fewer reported breaches. This outcome supports the notion that fundamental cybersecurity measures facilitate improved incident detection, while more advanced measures contribute to incident prevention. However, our analysis does not uncover a significant link between cybersecurity investments and enhanced profitability.","Leveraging a novel dataset that integrates survey responses regarding IT utilization with administrative tax records of Dutch companies, this study investigates the connection between cybersecurity expenditures, the likelihood of cyberattacks, and firm profitability. The dataset's richness enables us to account for variations in firm scale, industry, and IT infrastructure. A novel metric, quantifying the level of cyber preparedness within firms, was developed, revealing a positive correlation between this maturity level and firm size. Regression models indicate an inverted U-shaped relationship between cyber maturity and the probability of cyber incidents: initial increases in maturity correspond with a heightened incident probability, but the most mature firms report fewer incidents. This aligns with the theory that rudimentary cybersecurity practices enhance incident detection, while sophisticated measures focus on prevention. However, our analysis fails to identify a positive association between cybersecurity investments and profitability."
82,Creation mechanism of new media art combining artificial intelligence and internet of things technology in a metaverse environment,"The Metaverse is regarded as a brand-new virtual society constructed by deep media, and the new media art produced by new media technology will gradually replace the traditional art form and play an important role in the infinite Metaverse in the future. The maturity of the new media art creation mechanism must also depend on the help of artificial intelligence (AI) and Internet of Things (IoT) technology. The purpose of this study is to explore the image style transfer of digital painting art in new media art, that is, to reshape the image style by neural network technology in AI based on retaining the semantic information of the original image. Based on neural style transfer, an image style conversion method based on feature synthesis is proposed. Using the feature mapping of content image and style image and combining the advantages of traditional texture synthesis, a richer multi-style target feature mapping is synthesized. Then, the inverse transformation of target feature mapping is restored to an image to realize style transformation. In addition, the research results are analyzed. Under the background of integrating AI and IoT, the creation mechanism of new media art is optimized. Regarding digital art style transformation, the Tensorflow program framework is used for simulation verification and performance evaluation. The experimental results show that the image style transfer method based on feature synthesis proposed in this study can make the image texture more reasonably distributed, and can change the style texture by retaining more semantic structure content of the original image, thus generating richer artistic effects, and having better interactivity and local controllability. It can provide theoretical help and reference for developing new media art creation mechanisms.","The Metaverse is envisioned as a novel virtual society built by advanced media, with new media art generated through innovative media technologies expected to gradually replace traditional art forms and play a significant role in the limitless Metaverse of the future. The development of new media art creation mechanisms will also rely on the support of artificial intelligence (AI) and Internet of Things (IoT) technologies. This study aims to investigate the style transfer of digital painting in new media art, specifically by using neural network technology in AI to reshape image styles while preserving the original image's semantic information. An image style transformation method based on feature synthesis is proposed, leveraging the feature mapping of both content and style images, combined with traditional texture synthesis techniques, to create a richer multi-style feature mapping. The inverse transformation of this target feature mapping is then restored to an image to achieve style transformation. Additionally, the research results are analyzed, demonstrating that integrating AI and IoT optimizes new media art creation mechanisms. For digital art style transformation, the Tensorflow framework was employed for simulation and performance evaluation. Experimental outcomes indicate that the proposed feature synthesis-based image style transfer method can sensibly distribute image textures and alter style textures while retaining more of the original image's semantic structure, generating richer artistic effects, enhanced interactivity, and improved local controllability. It provides theoretical insights and references for the advancement of new media art creation mechanisms.","In the realm of the Metaverse, a novel virtual world crafted by cutting-edge media, innovative art forms spawned by new media technologies are poised to supplant traditional artistic expressions, assuming a pivotal role in the boundless expanse of the Metaverse. The evolution of new media art creation hinges on the synergy of artificial intelligence (AI) and Internet of Things (IoT) technologies. This investigation delves into the realm of digital painting art in new media, focusing on the neural network-driven image style transfer that preserves the original image's semantic essence. By harnessing the power of neural style transfer, a feature synthesis-based image style conversion method is proposed, which integrates the advantages of traditional texture synthesis to yield a richer, multi-style target feature mapping. This, in turn, enables the inverse transformation of the target feature mapping into an image, thereby achieving style transformation. Furthermore, the research outcomes are analyzed, optimizing the new media art creation mechanism under the umbrella of AI and IoT integration. The digital art style transformation is validated through simulation and performance evaluation using the Tensorflow program framework, demonstrating that the proposed feature synthesis-based image style transfer method can produce more reasonably distributed image textures, retain more semantic structure content, and generate richer artistic effects with enhanced interactivity and local controllability, thereby providing valuable theoretical insights for the development of new media art creation mechanisms.","The Metaverse, envisioned as a novel digital realm built upon advanced media technologies, is poised to witness a shift in artistic expression. New media art, powered by cutting-edge technologies, is expected to gradually supplant traditional art forms, assuming a pivotal role in the boundless expanse of the Metaverse. The evolution of this new creative landscape hinges on the integration of Artificial Intelligence (AI) and the Internet of Things (IoT). This study delves into the realm of style transfer in digital painting, aiming to reshape visual aesthetics using AI-driven neural networks while preserving the inherent meaning of the source image. A novel approach grounded in feature synthesis is introduced, leveraging the characteristics of both content and style images and drawing upon the strengths of conventional texture synthesis techniques to generate a multifaceted target feature map. Through inverse transformation, this map is then rendered into an image, effectively achieving style transfer. The integration of AI and IoT paves the way for an enhanced new media art creation mechanism. Utilizing the Tensorflow framework, the proposed style transfer method undergoes rigorous simulation and evaluation. Results demonstrate that this approach facilitates a more coherent distribution of image texture, allowing for style modification while faithfully retaining the semantic essence of the original image. Consequently, it yields a richer artistic output with enhanced interactivity and localized control, offering valuable insights and guidance for the future development of new media art creation mechanisms."
133,Simulation of dual-purpose chicken breeding programs implementing gene editing,"BackgroundIn spite of being controversial and raising ethical concerns, the application of gene editing is more likely to be accepted when it contributes to improving animal welfare. One of the animal welfare and ethical issues in chicken breeding is chick culling, the killing of the male layer chicks after hatching due to the poor fattening performance. Although establishing dual-purpose chicken lines could solve this problem, unfavorable genetic correlations between egg and meat production traits hindered their competitiveness. Although it is also controversial in ethical terms, gene editing may accelerate genetic progress in dual-purpose chicken and alleviate the ethical concerns from chick culling.ResultsThe simulation compared the utility improvement in dual-purpose use under two breeding schemes: one consisting in the improvement of the laying hens, and the second in the improvement of a synthetic line obtained from a layer broiler cross. In each breeding scheme, the breeding programs were simulated with and without gene editing. Polygenic breeding values and 500 simulated quantitative trait loci (QTL) with different levels of pleiotropy caused negative correlations between egg production, meat production, and overall health. The results of the simulation demonstrated that genetic gain could be accelerated by at most 81% for several generations if gene editing was used. The actual increase in genetic gain depended on the number of single nucleotide polymorphisms (SNPs) being edited per animal. The rate of genetic improvement became equal in scenarios with and without gene editing after 20 generations. This is because the remaining segregating QTL had small effects and their edition would have negative overall health effects from potential off-target edits. Although gene editing can improve genetic gain in quantitative traits, it can only be recommended as long as QTL with reasonable effect sizes are segregating and detectable.ConclusionsThis simulation demonstrates the potential of gene editing to accelerate the simultaneous improvement of negatively correlated traits. When the risk of negative consequences from gene editing persists, the number of SNPs to be edited should be chosen carefully to obtain the optimal genetic gain.","Despite ongoing ethical debates, gene editing's use may garner acceptance when it improves animal welfare. A significant ethical issue in chicken breeding is chick culling, wherein male layer chicks are culled post-hatching due to low fattening efficiency. Dual-purpose chicken lines could address this issue, but unfavorable genetic correlations between egg and meat production have limited their effectiveness. Although ethically contentious, gene editing has the potential to enhance genetic progress in dual-purpose chickens and mitigate ethical issues related to chick culling. The simulation examined utility improvements in dual-purpose chickens under two breeding strategies: enhancing laying hens and developing a synthetic line from a layer-broiler hybrid. Each strategy was simulated with and without gene editing. Polygenic breeding values and 500 simulated quantitative trait loci (QTL) with varying pleiotropic effects created negative correlations between egg production, meat production, and overall health. Results indicated that gene editing could boost genetic gains by up to 81% over several generations, contingent on the number of single nucleotide polymorphisms (SNPs) edited per animal. After 20 generations, the rate of genetic improvement in scenarios with and without gene editing equalized, as remaining segregating QTL had minor impacts, and editing them could negatively affect overall health due to potential off-target edits. While gene editing can enhance genetic gains in quantitative traits, it should be employed with caution, ensuring QTL with significant effect sizes are segregating and detectable. In conclusion, the simulation highlights gene editing's potential to accelerate the improvement of negatively correlated traits. To minimize risks associated with gene editing, careful selection of the number of SNPs to be edited is crucial for achieving optimal genetic gain.","Despite the controversy and ethical dilemmas surrounding gene editing, its application is more palatable when it enhances animal welfare. A pressing concern in chicken breeding is the culling of male layer chicks due to their poor meat production capabilities, which could be addressed by developing dual-purpose chicken lines. However, unfavorable genetic correlations between egg and meat production traits hinder their viability. Gene editing, although ethically contentious, may expedite genetic progress in dual-purpose chickens and mitigate the ethical concerns associated with chick culling. A simulation was conducted to compare the utility improvement in dual-purpose use under two breeding schemes: enhancing laying hens and developing a synthetic line from a layer-broiler cross. The results showed that genetic gain could be accelerated by up to 81% over several generations with gene editing, depending on the number of edited single nucleotide polymorphisms (SNPs) per animal. However, the rate of genetic improvement equalized in scenarios with and without gene editing after 20 generations, as the remaining segregating QTL had minimal effects and their editing could have detrimental overall health consequences. Gene editing can improve genetic gain in quantitative traits, but its recommendation is contingent upon the presence of segregating QTL with significant effect sizes.","Despite ethical debates surrounding its use, gene editing is seen as more acceptable when it enhances animal welfare. One such welfare issue in poultry farming is the culling of male chicks due to their poor meat yield. While dual-purpose breeds could address this, the inherent trade-off between egg and meat production traits limits their effectiveness. Gene editing, though ethically contentious, could expedite the development of competitive dual-purpose chickens, mitigating the ethical concerns associated with culling. A simulated study evaluated the potential benefits of gene editing in dual-purpose chicken breeding, comparing two approaches: enhancing existing laying hens and creating a synthetic line from a broiler-layer cross. Each approach was simulated with and without gene editing. The simulations incorporated polygenic breeding values and 500 quantitative trait loci (QTL) with varying degrees of pleiotropy, resulting in negative correlations between egg production, meat production, and overall health. The results showed that gene editing could accelerate genetic gain by up to 81% for several generations, depending on the number of single nucleotide polymorphisms (SNPs) edited per animal. However, after 20 generations, the rate of genetic improvement became comparable in scenarios with and without gene editing. This convergence occurred because the remaining QTL had small effects, and editing them could negatively impact overall health due to potential off-target edits. This study highlights the potential of gene editing to simultaneously improve negatively correlated traits. However, given the inherent risks associated with gene editing, the number of SNPs targeted for editing should be carefully considered to maximize genetic gain while minimizing potential adverse effects."
36,Salsal: blockchain for vetting cultural object collections,"Many modern cultural object collections suffer from the problem of being obtained in unethical and illegal circumstances. Additionally, information about collections, including their status, object descriptions, and other data need up-to-date information presented to users. We propose a novel blockchain tool called Salsal that enables the vetting of objects, individually or as part of more extensive collections, to meet required ethical and legal guidelines while informing users about relevant information regarding collections. Blockchain provides a better and more rapid way for users to know about collections using a decentralized and immutable ledger technology. Blockchain can be used to incentivize or even pressure collections to vet their objects for ethical and legal guidelines that can benefit the public who use object collections. The prototype software we have made is presented and compared to other blockchains, with code and demonstration provided. We present how our blockchain can enable benefit, providing a useful vetting process for cultural objects, and allowing a user community to contribute to collections in a transparent and secure manner.","Numerous contemporary cultural collections face issues due to acquisition under unethical and unlawful conditions. Furthermore, up-to-date information on collections, including their status, object descriptions, and other associated data, is essential for users. Our innovative blockchain tool, Salsal, proposes a solution by facilitating the verification of objects, either individually or as part of larger collections, ensuring compliance with ethical and legal standards while delivering pertinent collection information to users. Utilizing decentralized and immutable ledger technology, blockchain offers a superior and faster method for users to access information about collections. It can incentivize or exert pressure on collections to authenticate their objects according to ethical and legal criteria that benefit the public. We introduce our prototype software, complete with code and a demonstration, and compare it to other blockchain technologies. Our blockchain aims to provide a beneficial vetting process for cultural objects, promoting transparency and security, and allowing the user community to actively contribute to collections.","Numerous contemporary cultural repositories are plagued by the issue of acquiring artifacts through illicit and unethical means. Furthermore, users require access to up-to-date information about these collections, including their provenance, item descriptions, and supplementary data. To address this, we introduce Salsal, a pioneering blockchain-based solution that facilitates the verification of individual objects or entire collections against established ethical and legal standards, while providing users with pertinent information about these collections. By leveraging decentralized and immutable ledger technology, blockchain enables rapid and secure dissemination of information about collections to users. This technology can also be utilized to encourage or compel collections to adhere to ethical and legal guidelines, ultimately benefiting the public that engages with these repositories. Our prototype software is showcased, benchmarked against other blockchain solutions, and accompanied by code and a demonstration. We demonstrate how our blockchain can foster a transparent and secure environment, empowering a community of users to contribute to cultural collections while ensuring a rigorous vetting process for artifacts.","Numerous contemporary collections of cultural artifacts are marred by provenance issues, often acquired through illicit or ethically questionable means. Moreover, these collections frequently lack comprehensive and current documentation, encompassing their status, object details, and other pertinent information. To address these challenges, we introduce Salsal, a pioneering blockchain-based solution. Salsal facilitates the scrutiny of individual objects or entire collections, ensuring compliance with ethical and legal standards while disseminating relevant information to users. Leveraging the decentralized and immutable nature of blockchain technology, Salsal provides a more efficient and transparent means for users to access collection data. Furthermore, it can incentivize or even compel collections to undergo ethical and legal vetting, ultimately benefiting the public who engage with these artifacts. Our prototype software is presented alongside a comparative analysis of existing blockchain solutions, with accompanying code and demonstration. We elaborate on how our blockchain can facilitate a valuable vetting process for cultural objects, empowering a user community to contribute to collections in a secure and transparent manner."
114,A systematic review and meta-analysis of procedural justice and legitimacy in policing: the effect of social identity and social contexts,"ObjectivesTo systematically review the effect of social identity and social contexts on the association between procedural justice and legitimacy in policing.MethodsA meta-analysis synthesising data from 123 studies (N = 200,966) addressing the relationship between procedural justice and legitimacy in policing. Random effects univariate and two-stage structural equation modelling meta-analyses were performed.ResultsBoth procedural justice and social identity are found to be significantly correlated with police legitimacy. Moreover, social identity significantly mediates, but does not moderate, the association between procedural justice and legitimacy. People of younger age and from more developed countries tend to correlate procedural justice stronger with police legitimacy.ConclusionThis study demonstrates that social identity is an important antecedent of legitimacy and a critical factor in the dynamics of procedural fairness in policing. It also shows that the extent to which procedural justice and legitimacy are correlated varies across social groups and contexts. The theoretical implications of our findings are discussed.","Aims: To conduct a systematic review examining how social identity and contexts influence the link between procedural justice and police legitimacy. Methods: This meta-analysis combines results from 123 studies (N = 200,966) exploring the relationship between procedural justice and police legitimacy. The analysis utilized random effects univariate and two-stage structural equation modeling techniques. Findings: There is a significant correlation between both procedural justice and social identity with police legitimacy. Social identity notably mediates but does not moderate the relationship between procedural justice and legitimacy. Younger individuals and those from more developed countries are more likely to perceive a strong connection between procedural justice and police legitimacy. Conclusion: The study highlights social identity as a crucial precursor to legitimacy and a key element in the dynamics of procedural justice within policing. It also indicates that the correlation between procedural justice and legitimacy varies among different social groups and contexts. The theoretical implications of these findings are explored.","""This study aimed to investigate the interplay between social identity, social environments, and the relationship between fair policing procedures and public trust in law enforcement. A comprehensive meta-analysis of 123 studies involving over 200,000 participants was conducted, employing advanced statistical models to examine the correlations. The findings revealed that both fair procedures and social identity have a significant impact on police legitimacy, with social identity playing a mediating role in this association. Notably, the strength of this correlation varied across different age groups and countries, with younger individuals and those from more developed nations exhibiting a stronger link. The study's conclusions underscore the critical role of social identity in shaping public perceptions of police legitimacy and highlight the need to consider the diverse social contexts in which policing takes place.""","This research synthesized data from 123 studies (encompassing 200,966 participants) to examine how social identity and societal factors influence the link between procedural fairness and perceived legitimacy in law enforcement. Employing advanced statistical methods, the study revealed significant correlations between procedural justice, social identity, and police legitimacy. Notably, social identity acts as a key mediator, but not a moderator, in the relationship between procedural justice and legitimacy. The strength of the correlation between procedural justice and legitimacy varied based on demographic factors like age and national development. This investigation underscores the crucial role of social identity in shaping perceptions of legitimacy and highlights its significance in the interplay between procedural fairness and policing. Furthermore, it demonstrates that the connection between procedural justice and legitimacy is not uniform but rather is influenced by social contexts and group affiliations. The study's theoretical implications are explored in detail."
194,Disentropy in astronomy,"Astronomers often have to deal with randomness. For example, stars and radio telescopes are sources of randomness. On the other hand, randomly distributed stars in some astronomical images can make harder the task of an algorithm that aims the automatic identification of important structures in the image. Therefore, a randomness measure, like the disentropy of the autocorrelation function, can be a useful mathematical tool for astronomers. In this direction, in the present work we firstly show three applications of the disentropy of the autocorrelation in astronomy. Initially, we calculate the randomness of the images of cosmic microwave background maps produced by Planck satellite, providing for the first time a numerical value for that randomness. Following, we use the disentropy to build an algorithm that erases parts of astronomical images with large randomness, what is particularly useful to remove background stars. In the third application, the disentropy of the autocorrelation is used to calculate the randomness of the signal of a radio pulsar used as random number generator. At last, we used the relative disentropy as distance measure between probability distributions in order to find the parameters of the probability density function of the flux density of a pulsar.","Astronomers frequently encounter randomness in their work. Stars and radio telescopes, for instance, are sources of this randomness. Conversely, randomly scattered stars in some astronomical images can complicate an algorithm's task of automatically identifying key structures within the image. Hence, a measure of randomness, such as the disentropy of the autocorrelation function, can serve as a valuable mathematical tool for astronomers. In this context, this study presents three specific applications of disentropy in astronomy. First, we quantify the randomness in images of cosmic microwave background maps from the Planck satellite, providing a numerical value for the first time. Next, we utilize disentropy to develop an algorithm that removes highly random sections of astronomical images, which is particularly useful for eliminating background stars. Lastly, we apply the disentropy of the autocorrelation to measure the randomness in the signal of a radio pulsar used as a random number generator. Finally, we employ relative disentropy as a metric for comparing probability distributions to determine the parameters of the probability density function for a pulsar's flux density.","Astronomers frequently encounter stochasticity in their work, with celestial bodies and radio telescopes being prominent sources of uncertainty. Conversely, the random scattering of stars in certain astronomical images can hinder the performance of algorithms designed to automatically detect crucial features within those images. As a result, a stochasticity metric, such as the disentropy of the autocorrelation function, can serve as a valuable mathematical instrument for astronomers. In this study, we explore three novel applications of disentropy in astronomy. We begin by quantifying the stochasticity of cosmic microwave background maps generated by the Planck satellite, thereby providing a numerical estimate of this randomness for the first time. Next, we develop an algorithm that utilizes disentropy to eliminate regions of astronomical images exhibiting high stochasticity, which is particularly effective in removing background stars. In our third application, we employ disentropy to calculate the stochasticity of a radio pulsar's signal, which is used as a random number generator. Finally, we utilize relative disentropy as a distance metric between probability distributions to determine the parameters of a pulsar's flux density probability density function.","Astronomical research frequently encounters inherent randomness, originating from sources like stars and radio telescopes. While this randomness is intrinsic, it can complicate algorithmic identification of significant features in astronomical images, particularly when stars are randomly dispersed. A quantitative measure of randomness, such as the disentropy of the autocorrelation function, proves valuable in these scenarios. This study showcases three astronomical applications of autocorrelation disentropy. First, we quantify the randomness of cosmic microwave background maps from the Planck satellite, establishing a numerical value for this characteristic. Second, we develop an algorithm leveraging disentropy to selectively remove highly random sections of astronomical images, effectively filtering out background stars. Lastly, we employ autocorrelation disentropy to determine the randomness of a radio pulsar signal used for random number generation and utilize relative disentropy as a distance metric between probability distributions to estimate the parameters of a pulsar's flux density probability density function."
110,Fire simulation in the main control room of Bushehr nuclear power plant using CFAST software,"Fire protection is one of the most important issues to ensure safety and reduce risks of nuclear power plants. The main control room (MCR) is one of the parts of a nuclear power plant that is often identified with a high risk of fire in safety assessments. Although robust programs to shut down commercial reactors in any fires have been successfully maintained, the purpose of this paper is to simulate the fire in the MCR of Bushehr nuclear power plant unit‑1 (BNPP-1) using CFAST software. CFAST is a two-zone fire model used to calculate the evolving distribution of smoke, fire gases, and temperature throughout the compartments of a building during a fire. The data required for simulation have been extracted from final safety analysis reports and operating documents of BNPP‑1 and NUREG6850 reports. The results showed that the highest recorded temperature was related to the upper part of the standing cabinet about 1013 °C, which is the result of flames as well as heat transfer through smoke. Also, the other highest recorded temperature was in the lower part of the standing cabinet about 700 °C. Finally, the first ventilation system was closed for about 350 s and the second ventilation system was closed for about 860 s after the fire started.","Ensuring fire protection is crucial for the safety and risk mitigation of nuclear power plants. Within the safety evaluations, the main control room (MCR) is flagged as an area with significant fire risk. Despite effective programs for reactor shutdowns in fire scenarios, this study aims to model a fire in the MCR of Bushehr nuclear power plant unit‑1 (BNPP-1) using CFAST software. CFAST is a two-zone fire model designed to predict the progression of smoke, fire gases, and temperatures within building compartments amid a fire. Data for the simulation were sourced from the final safety analysis reports, operational documents of BNPP‑1, and NUREG6850 reports. Findings indicated that the highest temperature, around 1013°C, occurred in the upper section of the standing cabinet due to flames and heat transfer via smoke. Another significant temperature reading was approximately 700°C in the lower section of the standing cabinet. Additionally, the first ventilation system was shut for roughly 350 seconds, and the second for about 860 seconds after the fire ignited.","Ensuring the safety and mitigating the risks of nuclear power plants is crucial, and fire protection is a paramount concern in this regard. The main control room, a critical area prone to fire hazards, is a focal point in safety evaluations of nuclear facilities. Despite the existence of reliable shutdown protocols for commercial reactors in the event of a fire, this study aims to simulate a fire scenario in the main control room of Bushehr nuclear power plant unit-1 using the CFAST software, a two-zone fire modeling tool that predicts the spread of smoke, fire gases, and temperature within a building's compartments during a fire. The necessary data for the simulation were gleaned from the final safety analysis reports, operating documents of BNPP-1, and NUREG6850 reports. The simulation results revealed that the upper section of the standing cabinet reached a scorching temperature of approximately 1013°C, attributed to the combined effects of flames and heat transfer via smoke, while the lower section recorded a temperature of around 700°C. Furthermore, the first ventilation system remained shut for approximately 350 seconds, and the second ventilation system was closed for around 860 seconds after the fire broke out.","Ensuring fire safety is paramount for nuclear power plants to minimize risks. The main control room (MCR) is frequently flagged as a high fire-risk area in safety evaluations. While effective protocols exist to shut down reactors in the event of a fire, this study employed CFAST software to model a fire scenario within the MCR of Bushehr nuclear power plant unit-1 (BNPP-1). CFAST, a two-zone fire model, calculates the dynamic distribution of smoke, fire gases, and temperature throughout a building during a fire. The simulation utilized data from BNPP-1's final safety analysis reports, operational documents, and NUREG6850 reports. Results indicated that the highest temperature, approximately 1013°C, was observed in the upper section of the standing cabinet due to flames and heat transfer via smoke. Another elevated temperature, around 700°C, was recorded in the cabinet's lower part. Notably, the initial ventilation system closed approximately 350 seconds after the fire began, followed by the second system's closure at around 860 seconds."
138,Advances in miniature CRISPR-Cas proteins and their applications in gene editing,"The CRISPR-Cas system consists of Cas proteins and single-stranded RNAs that recruit Cas proteins and specifically target the nucleic acid. Some Cas proteins can accurately cleave the target nucleic acid under the guidance of the single-stranded RNAs. Due to its exceptionally high specificity, the CRISPR-Cas system is now widely used in various fields such as gene editing, transcription regulation, and molecular diagnosis. However, the huge size of the most frequently utilized Cas proteins (Cas9, Cas12a, and Cas13, which contain 950-1,400 amino acids) can limit their applicability, especially in eukaryotic gene editing, where larger Cas proteins are difficult to deliver into the target cells. Recently discovered miniature CRISPR-Cas proteins, consisting of only 400 to 800 amino acids, offer the possibility of overcoming this limitation. This article systematically reviews the latest research progress of several miniature CRISPR-Cas proteins (Cas12f, Cas12j, Cas12k, and Cas12m) and their practical applications in the field of gene editing.","The CRISPR-Cas mechanism comprises Cas enzymes and single-stranded RNAs that guide these enzymes to precisely target nucleic acids. Certain Cas enzymes can accurately cut the targeted nucleic acids under the direction of single-stranded RNAs. Owing to its remarkable precision, the CRISPR-Cas system has become widely employed in gene editing, transcriptional regulation, and molecular diagnostics. However, the large size of the commonly used Cas proteins (Cas9, Cas12a, and Cas13, which range from 950 to 1,400 amino acids) can restrict their utility, particularly in eukaryotic gene editing, where delivering larger Cas proteins into target cells is challenging. Recently identified smaller CRISPR-Cas proteins, containing only 400 to 800 amino acids, present a potential solution to this constraint. This article offers a comprehensive review of the latest advancements in research on several miniature CRISPR-Cas proteins (Cas12f, Cas12j, Cas12k, and Cas12m) and their practical applications in gene editing.","The CRISPR-Cas system is comprised of Cas enzymes and single-stranded RNA molecules that selectively bind to Cas enzymes and pinpoint specific nucleic acid sequences. Certain Cas enzymes can precisely excise targeted nucleic acid segments under the direction of these RNA molecules. Owing to its unparalleled precision, the CRISPR-Cas system has become a ubiquitous tool in diverse areas, including genetic modification, transcriptional control, and molecular diagnostics. However, the substantial size of commonly employed Cas enzymes (such as Cas9, Cas12a, and Cas13, which comprise 950-1,400 amino acid residues) can restrict their utility, particularly in eukaryotic gene editing, where the delivery of larger Cas enzymes into target cells poses significant challenges. The recent discovery of compact CRISPR-Cas enzymes, consisting of merely 400-800 amino acid residues, offers a potential solution to this limitation. This review provides a comprehensive overview of the latest advancements in several compact CRISPR-Cas enzymes (including Cas12f, Cas12j, Cas12k, and Cas12m) and their practical implications for gene editing applications.","Comprising Cas proteins and guiding single-stranded RNAs that direct the proteins to specific nucleic acid sequences, the CRISPR-Cas system enables precise cleavage of the targeted genetic material. This remarkable specificity has propelled its widespread adoption in diverse domains, including gene editing, transcriptional control, and molecular diagnostics. However, the substantial size of commonly employed Cas proteins like Cas9, Cas12a, and Cas13 (ranging from 950 to 1,400 amino acids) can hinder their application, particularly in eukaryotic gene editing, where introducing these large proteins into target cells proves challenging. Newly identified miniature CRISPR-Cas proteins, composed of a mere 400 to 800 amino acids, present a promising solution to this obstacle. This review provides a comprehensive overview of recent advances in the understanding and utilization of several miniature CRISPR-Cas proteins (Cas12f, Cas12j, Cas12k, and Cas12m), focusing on their potential for gene editing applications."
17,Virtual and augmented reality in biomedical engineering,"In the future, extended reality technology will be widely used. People will be led to utilize virtual reality (VR) and augmented reality (AR) technologies in their daily lives, hobbies, numerous types of entertainment, and employment. Medical augmented reality has evolved with applications ranging from medical education to picture-guided surgery. Moreover, a bulk of research is focused on clinical applications, with the majority of research devoted to surgery or intervention, followed by rehabilitation and treatment applications. Numerous studies have also looked into the use of augmented reality in medical education and training.MethodsUsing the databases Semantic Scholar, Web of Science, Scopus, IEEE Xplore, and ScienceDirect, a scoping review was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) criteria. To find other articles, a manual search was also carried out in Google Scholar. This study presents studies carried out over the previous 14 years (from 2009 to 2023) in detail. We classify this area of study into the following categories: (1) AR and VR in surgery, which is presented in the following subsections: subsection A: MR in neurosurgery; subsection B: spine surgery; subsection C: oral and maxillofacial surgery; and subsection D: AR-enhanced human-robot interaction; (2) AR and VR in medical education presented in the following subsections; subsection A: medical training; subsection B: schools and curriculum; subsection C: XR in Biomedicine; (3) AR and VR for rehabilitation presented in the following subsections; subsection A: stroke rehabilitation during COVID-19; subsection B: cancer and VR, and (4) Millimeter-wave and MIMO systems for AR and VR.ResultsIn total, 77 publications were selected based on the inclusion criteria. Four distinct AR and/or VR applications groups could be differentiated: AR and VR in surgery (N = 21), VR and AR in Medical Education (N = 30), AR and VR for Rehabilitation (N = 15), and Millimeter-Wave and MIMO Systems for AR and VR (N = 7), where N is number of cited studies. We found that the majority of research is devoted to medical training and education, with surgical or interventional applications coming in second. The research is mostly focused on rehabilitation, therapy, and clinical applications. Moreover, the application of XR in MIMO has been the subject of numerous research.ConclusionExamples of these diverse fields of applications are displayed in this review as follows: (1) augmented reality and virtual reality in surgery; (2) augmented reality and virtual reality in medical education; (3) augmented reality and virtual reality for rehabilitation; and (4) millimeter-wave and MIMO systems for augmented reality and virtual reality.","Looking ahead, it is anticipated that extended reality technology will see widespread adoption. Individuals will be encouraged to integrate virtual reality (VR) and augmented reality (AR) into their everyday activities, leisure pursuits, various forms of entertainment, and professional endeavors. The field of medical augmented reality has advanced, with applications ranging from educational purposes to image-guided surgeries. A significant portion of research is concentrated on clinical uses, predominantly focusing on surgical and interventional procedures, followed by rehabilitation and treatment applications. Additionally, numerous studies have investigated the role of augmented reality in medical education and training. In order to conduct this scoping review, databases such as Semantic Scholar, Web of Science, Scopus, IEEE Xplore, and ScienceDirect were utilized, adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A manual search on Google Scholar was also performed to identify additional articles. This review comprehensively examines studies from the past 14 years (2009 to 2023). The research is categorized as follows: (1) AR and VR in surgery, detailed in subsections: A: MR in neurosurgery; B: spine surgery; C: oral and maxillofacial surgery; and D: AR-enhanced human-robot interaction; (2) AR and VR in medical education, detailed in subsections: A: medical training; B: schools and curriculum; C: XR in Biomedicine; (3) AR and VR for rehabilitation, detailed in subsections: A: stroke rehabilitation during COVID-19; B: cancer and VR; and (4) Millimeter-wave and MIMO systems for AR and VR. Overall, 77 publications met the inclusion criteria, categorized into four groups: AR and VR in surgery (N = 21), AR and VR in medical education (N = 30), AR and VR for rehabilitation (N = 15), and Millimeter-wave and MIMO systems for AR and VR (N = 7), where N represents the number of studies cited. The findings reveal that most research is dedicated to medical training and education, with surgical or interventional applications being the second most prominent. The focus is primarily on rehabilitation, therapy, and clinical applications, with the application of XR in MIMO also being extensively researched. In summary, this review showcases the diverse applications of AR and VR in the following domains: (1) surgery, (2) medical education, (3) rehabilitation, and (4) millimeter-wave and MIMO systems.","In the years to come, the widespread adoption of extended reality technology is anticipated. As a result, individuals will increasingly incorporate virtual reality (VR) and augmented reality (AR) into their daily routines, leisure activities, diverse forms of entertainment, and professional pursuits. The medical sector has witnessed significant advancements in AR, with applications spanning from educational initiatives to image-guided surgical interventions. Furthermore, a substantial body of research is concentrated on clinical applications, with the majority focused on surgical or interventional procedures, followed by rehabilitation and treatment strategies. Additionally, numerous investigations have probed the utilization of AR in medical education and training.","The pervasive integration of extended reality (XR) technology, encompassing virtual reality (VR) and augmented reality (AR), into various facets of life, including work, leisure, and entertainment, is anticipated. The medical realm has witnessed a significant evolution in AR applications, spanning medical education to image-guided surgery. A comprehensive scoping review, adhering to PRISMA guidelines, was undertaken using databases like Semantic Scholar, Web of Science, and others, supplemented by a manual Google Scholar search. This review, encompassing studies from 2009 to 2023, categorized AR and VR applications into four primary domains: surgical interventions, medical education, rehabilitation, and millimeter-wave and MIMO systems. Analysis of the 77 selected publications revealed a predominance of research in medical training and education, followed by surgical applications. The emphasis on rehabilitation, therapy, and clinical applications was prominent, with considerable research exploring XR's role in MIMO systems. This review showcases diverse applications of AR and VR, encompassing surgery, medical education, rehabilitation, and millimeter-wave and MIMO systems."
74,A review of advanced architectural glazing technologies for solar energy conversion and intelligent daylighting control,"Efficient management of solar radiation through architectural glazing is a key strategy for achieving a comfortable indoor environment with minimum energy consumption. Conventional glazing consisting of a single or multiple glass pane(s) exhibits high visible light transmittance and solar heat gain coefficient, which can be a double-edged sword, i.e., it allows sufficient sunlight to enter the building interior space for passive heating and lighting; on the other hand, it can cause glare discomfort and large cooling energy consumption. Among the various advanced glazing technologies being developed, Building Integrated Photovoltaic (BIPV) glazing has a prominent position due to its ability to reduce cooling load and visual discomfort while simultaneously generating electricity from sunlight. Recent years have witnessed remarkable advances in low-concentration optics such as Dielectric based Compound Parabolic Concentrators (DiCPCs), with a growing interest in the development of Building Integrated Concentrating Photovoltaic (BICPV) glazing to improve light harvesting and electric power output. One of the challenges faced by traditional BIPV glazing systems is the lack of dynamic control over daylight and solar heat transmission to cope with variations in weather conditions and seasonal heating/cooling demands of buildings. A promising solution is to integrate an optically switchable smart material into a BIPV glazing system, which enables dynamic daylighting control in addition to solar power conversion. Thermotropic (TT) hydrogel materials such as poly(N-isopropylacrylamide) (PNIPAm) and Hydroxypropyl Cellulose (HPC) are potential candidates for hybrid BIPV smart glazing applications, due to their unique features such as high visible transparency (in the clear state), strong light-scattering capability (in the translucent state) and large solar energy modulation. This paper reviews various types of electricity-generating glazing technologies including BIPV glazing and BICPV glazing, as well as smart glazing technologies with a particular focus on TT hydrogel integrated glazing. The characteristics, benefits and limitations of hybrid BIPV smart glazing are also evaluated. Finally, the challenges and research opportunities in this emerging field are discussed.","Efficient regulation of solar radiation via architectural glazing is essential for creating a comfortable indoor environment while minimizing energy use. Traditional glazing, whether single or multi-pane, offers high visible light transmittance and solar heat gain coefficient, functioning as a double-edged sword. It permits ample sunlight into the building for passive heating and illumination, yet it can lead to glare discomfort and increased cooling energy demands. Building Integrated Photovoltaic (BIPV) glazing stands out among innovative glazing technologies due to its capacity to lower cooling loads and visual discomfort, while concurrently generating electricity from sunlight. Recently, significant progress has been made in low-concentration optics, such as Dielectric based Compound Parabolic Concentrators (DiCPCs), fostering interest in Building Integrated Concentrating Photovoltaic (BICPV) glazing to enhance light capture and electricity production. A notable challenge with conventional BIPV glazing systems lies in their inability to dynamically control daylight and solar heat transmission, essential for adapting to weather variations and seasonal heating/cooling needs. Integrating optically switchable smart materials into BIPV glazing offers a promising solution, providing dynamic daylighting control and solar power conversion. Thermotropic (TT) hydrogel materials like poly(N-isopropylacrylamide) (PNIPAm) and Hydroxypropyl Cellulose (HPC) are viable options for hybrid BIPV smart glazing due to their high visible transparency in a clear state, strong light-scattering in a translucent state, and substantial solar energy modulation. This paper reviews various electricity-generating glazing technologies, including BIPV and BICPV glazing, and examines smart glazing technologies with a focus on TT hydrogel integrated glazing. The attributes, advantages, and limitations of hybrid BIPV smart glazing are assessed, and the challenges and research opportunities in this emerging field are discussed.","Optimizing solar radiation management through innovative glazing designs is crucial for creating a comfortable indoor atmosphere while minimizing energy expenditure. Traditional glazing systems, comprising single or multiple glass panes, exhibit high visible light transmission and solar heat gain coefficients, which can be both beneficial and detrimental, as they allow for natural lighting and passive heating but also lead to glare discomfort and increased cooling energy consumption. Among the cutting-edge glazing technologies being developed, Building Integrated Photovoltaic (BIPV) glazing stands out for its ability to reduce cooling loads and visual discomfort while generating electricity from sunlight. Recent breakthroughs in low-concentration optics, such as Dielectric-based Compound Parabolic Concentrators (DiCPCs), have sparked growing interest in the development of Building Integrated Concentrating Photovoltaic (BICPV) glazing to enhance light harvesting and electric power output. However, traditional BIPV glazing systems face the challenge of lacking dynamic control over daylight and solar heat transmission to adapt to changing weather conditions and seasonal heating/cooling demands. A promising solution lies in integrating optically switchable smart materials into BIPV glazing systems, enabling dynamic daylighting control and solar power conversion. Thermotropic (TT) hydrogel materials, such as poly(N-isopropylacrylamide) (PNIPAm) and Hydroxypropyl Cellulose (HPC), are potential candidates for hybrid BIPV smart glazing applications due to their unique features, including high visible transparency, strong light-scattering capability, and large solar energy modulation. This review examines various electricity-generating glazing technologies, including BIPV and BICPV glazing, as well as smart glazing technologies with a focus on TT hydrogel-integrated glazing, evaluating their characteristics, benefits, and limitations. Finally, the challenges and research opportunities in this emerging field are discussed.","Optimizing the way buildings utilize sunlight through their windows is crucial for creating comfortable indoor spaces without excessive energy use. Traditional windows, whether single or multi-paned, let in a lot of light and heat from the sun. This has both advantages and disadvantages: while providing natural light and warmth, it can also lead to uncomfortable glare and high cooling costs. Among emerging window technologies, Building Integrated Photovoltaic (BIPV) glazing stands out for its ability to generate electricity from sunlight while mitigating cooling needs and visual discomfort. Recent breakthroughs in light-concentrating optics, particularly Dielectric based Compound Parabolic Concentrators (DiCPCs), have spurred interest in Building Integrated Concentrating Photovoltaic (BICPV) glazing, which further enhances light capture and electricity generation. A key limitation of traditional BIPV systems is their inability to dynamically adjust daylight and solar heat transmission to match changing weather and building needs. Integrating optically switchable smart materials into BIPV glazing offers a promising solution, enabling both dynamic daylight control and solar energy conversion. Thermotropic (TT) hydrogels, like poly(N-isopropylacrylamide) (PNIPAm) and Hydroxypropyl Cellulose (HPC), are attractive candidates for hybrid BIPV smart glazing. They possess unique properties, including high transparency when clear, strong light-scattering when translucent, and significant solar energy modulation capabilities. This review examines various electricity-generating window technologies, including BIPV and BICPV glazing, as well as smart glazing technologies, with a focus on TT hydrogel integration. It also assesses the characteristics, advantages, and drawbacks of hybrid BIPV smart glazing. Finally, the paper discusses challenges and research opportunities within this burgeoning field."
171,Deep learning techniques to detect cybersecurity attacks: a systematic mapping study,"ContextRecent years have seen a lot of attention into Deep Learning (DL) techniques used to detect cybersecurity attacks. DL techniques can swiftly analyze massive datasets, and automate the detection and mitigation of a wide variety of cybersecurity attacks with superior results. However, no systematic study exists that summarizes these DL techniques since most studies are informal literature surveys or focus on different subjects.ObjectiveTo deliver a comprehensive and systematic summary of the existing DL techniques used to detect cybersecurity attacks as they are described in the literature. To identify open challenges for future research.MethodWe conducted a systematic mapping study about DL techniques to detect cybersecurity attacks driven by eleven research questions. We followed existing guidelines when defining our research protocol to increase the repeatability and reliability of our results.ResultsFrom an initial set of 1839 papers, we identified 116 relevant primary studies, primarily published in the last three years. We investigated multiple aspects of the DL techniques, such as the cybersecurity attack types to detect, their application domains, the programming languages, libraries, operating systems, and frameworks used to implement the DL techniques, the datasets used to train the DL models, the types of research carried out (academic or industrial), the performance of the techniques, and the advantages and disadvantages of each technique. We present a new taxonomy comprising 36 different DL techniques. We identified 14 application domains, eight cybersecurity attacks, and 93 publicly available datasets, among other results.ConclusionsWe provide six lessons learned along with recommendations for future research directions. The most active research areas in DL techniques for the identification of cybersecurity attacks discuss CNN and LSTM techniques. DL techniques in cybersecurity is a rapidly growing and developing research area, with many open challenges, including the lack of (a) research conducted in industrial settings, (b) real-time datasets, (c) studies focusing on promising DL techniques and relevant cybersecurity attacks.","Over recent years, substantial focus has been directed towards the use of Deep Learning (DL) methods to identify cybersecurity threats. These approaches are capable of rapidly processing vast amounts of data and autonomously detecting and addressing a variety of cyber attacks with high efficiency. Nonetheless, a systematic review that consolidates these DL methods is lacking, as most existing studies are informal surveys or concentrate on different topics.ObjectiveTo construct a comprehensive and methodical overview of current DL methods for identifying cybersecurity threats as documented in the literature, highlighting areas needing further research.MethodWe performed a systematic mapping study on DL techniques for identifying cybersecurity threats, guided by eleven research questions. We adhered to current guidelines to develop our research protocol, which aimed at enhancing the reproducibility and reliability of our findings.ResultsOut of an initial pool of 1839 papers, we pinpointed 116 pertinent primary studies, mainly from the past three years. We examined various facets of the DL methods, such as the types of cybersecurity threats they address, their fields of application, the programming languages, libraries, operating systems, and frameworks used in their implementation, the datasets for training DL models, the nature of the research (academic or industrial), the efficacy of the techniques, and the pros and cons of each method. We introduce a novel taxonomy that categorizes 36 distinct DL techniques. Our findings include 14 application domains, eight types of cybersecurity threats, and 93 publicly accessible datasets, among other insights.ConclusionsWe offer six key insights along with suggestions for future research directions. The most actively researched areas in DL techniques for cybersecurity focus on CNN and LSTM methods. The field of DL in cybersecurity is rapidly expanding, with numerous unresolved challenges, such as insufficient industrial research, lack of real-time datasets, and a need for studies on promising DL methods and relevant cybersecurity threats.","In recent times, the realm of cybersecurity has witnessed a surge in the application of Deep Learning (DL) methodologies for detecting and mitigating cyber threats. The allure of DL lies in its ability to rapidly process vast datasets and automate the identification of a broad spectrum of cyber attacks with unparalleled accuracy. Despite this, a comprehensive and systematic review of these DL techniques is conspicuously absent, with existing studies often taking the form of informal literature surveys or focusing on disparate topics. This study aims to bridge this knowledge gap by providing an exhaustive and structured overview of DL techniques employed in cybersecurity, as described in the literature, and identifying areas that warrant further investigation. Through a systematic mapping study guided by eleven research questions, we analyzed 1839 papers, yielding 116 pertinent primary studies, predominantly published within the last three years. Our investigation encompassed various facets of DL techniques, including the types of cyber attacks detected, application domains, programming languages, and datasets utilized. We propose a novel taxonomy comprising 36 distinct DL techniques and highlight 14 application domains, eight cyber attack types, and 93 publicly available datasets, among other findings. Our conclusions distill into six key takeaways, accompanied by recommendations for future research directions, with a focus on CNN and LSTM techniques, and the need to address the lack of industrial research, real-time datasets, and studies exploring promising DL techniques and relevant cyber threats.","The domain of cybersecurity has witnessed a surge in Deep Learning (DL) applications for attack detection. While DL excels in rapid analysis of large datasets and automated threat mitigation, a cohesive understanding of these techniques remains absent, with existing studies often fragmented or lacking systematic review. This work addresses this gap by conducting a rigorous mapping study guided by eleven research questions to provide a comprehensive overview of DL-based cybersecurity attack detection techniques. From an extensive pool of publications, 116 primary studies were meticulously selected, primarily from recent years. This analysis delved into various facets of DL techniques, encompassing attack types, application domains, implementation tools, training datasets, research nature, performance metrics, and advantages and disadvantages. The outcome is a novel taxonomy encompassing 36 distinct DL techniques, alongside the identification of 14 application domains, 8 cybersecurity attack categories, and 93 publicly accessible datasets. The study culminates in six key insights and recommendations for future research trajectories, highlighting CNN and LSTM as prominent areas within DL-driven cybersecurity. Despite its rapid evolution, the field faces notable challenges, including the scarcity of industrial research, real-time datasets, and investigations focusing on emerging DL techniques and pertinent cyber threats."
197,“People Gathered by Sorghum”: Cultural Practices and sorghum Diversity in Northern Ethiopia,"Sorghum is an important crop in the livelihoods of Kunama and Tigrayan farmers in Northern Ethiopia and we present here a study of what factors have shaped the genetic diversity of the varieties cultivated in neighboring communities of the two ethnolinguistic groups. Using a combination of methods from crop science and cultural anthropology, we investigate patterns of historical and contemporary relationships between crop genetic diversity and cultural and social factors. The spatial genetic structure reveals patterns of ethnolinguistic differentiation and admixture that reflect deep affinities between cultures and crops, but the seed systems of the two communities are also open to exchange and gene-flow. Our findings highlight the importance of understanding cultural factors for genetic resource conservation, as well as for plant breeding and seed system development efforts.","Sorghum holds significant value for the livelihoods of Kunama and Tigrayan farmers in Northern Ethiopia. This study explores the factors influencing the genetic diversity of sorghum varieties grown by these neighboring ethnolinguistic communities. By employing methods from both crop science and cultural anthropology, we examine the historical and current connections between crop genetic diversity and cultural and social elements. The spatial genetic structure reveals patterns of ethnolinguistic differentiation and admixture, indicating deep cultural and crop-related ties. However, the seed systems in both communities remain open to exchange and gene flow. Our results underscore the critical role of cultural factors in the conservation of genetic resources, as well as in plant breeding and seed system development initiatives.","In Northern Ethiopia, the cultivation of sorghum is vital to the well-being of Kunama and Tigrayan farmers, and this research examines the factors that have influenced the genetic variation of the crop varieties grown in adjacent communities of these two ethnolinguistic groups. By integrating approaches from agronomy and cultural anthropology, we explore the historical and contemporary correlations between crop genetic diversity and sociocultural factors. The geographic distribution of genetic traits reveals a complex pattern of ethnolinguistic segregation and hybridization, reflecting the profound connections between cultures and crops, yet the seed exchange systems of both communities remain receptive to external influences and genetic exchange. Our results underscore the significance of considering cultural dynamics in efforts to conserve genetic resources, as well as in plant breeding and seed system development initiatives.","This study explores the factors influencing the genetic diversity of sorghum, a vital crop for Kunama and Tigrayan farmers in Northern Ethiopia. Employing a multidisciplinary approach integrating agricultural science and cultural anthropology, we analyze the historical and current interplay between sorghum genetic variability and socio-cultural elements. The results indicate that while the spatial distribution of genetic variations mirrors distinct ethnolinguistic lineages and intermingling, reflecting deep-seated connections between cultural practices and crop evolution, the seed exchange networks of both communities remain receptive to gene flow and cross-pollination. Our research underscores the significance of incorporating cultural considerations in genetic resource preservation initiatives, plant breeding strategies, and seed system advancement endeavors."
68,Recent progress in organic waste recycling materials for solar cell applications,"Organic waste-derived solar cells (OWSC) are a classification of third-generation photovoltaic cells in which one or more constituents are fabricated from organic waste material. They are an inspirational complement to the conventional third-generation solar cell with the potential of revolutionizing our future approach to solar cell manufacture. This article provides a study and summary of solar cells that fall under the category of OWSC. OWSC own their merit to low cost of manufacturing and environmental friendliness. This review article reveals different organic waste raw materials, preparation-to-assembly methodologies, and novel approaches to solar cell manufacturing. Ideas for the optimization of the performance of OWSC are presented. The assembly configurations and photovoltaic parameters of reported OWSC are compared in detail. An overview of the trends in the research regarding OWSC in the past decade is given. Also, the advantages and disadvantages of the different solar cell technologies are discussed, and possible trends are proposed. Industrial organic waste raw materials such as paper, coal, and plastics are among the least explored and yet most attractive for solar cell fabrication. The power conversion efficiencies for the cited works are mentioned while emphasizing the products and functions of the organic waste raw materials used.Graphical Abstract","Organic waste-derived solar cells (OWSC) represent a segment of third-generation photovoltaic cells where one or more components are produced from organic waste. They serve as an inspiring adjunct to traditional third-generation solar cells, holding promise for a transformative impact on future solar cell production methods. This article examines and synthesizes information on OWSCs. The advantages of OWSCs include their affordability in production and their eco-friendliness. The review addresses various organic waste materials, the processes from preparation to assembly, and innovative solar cell manufacturing techniques. Strategies for enhancing the performance of OWSCs are discussed. Detailed comparisons of assembly designs and photovoltaic parameters of existing OWSCs are presented. The research trends over the past ten years regarding OWSCs are summarized. Furthermore, the pros and cons of different solar cell technologies are evaluated, and future trends are suggested. Industrial organic waste materials like paper, coal, and plastics, while less studied, present significant potential for solar cell creation. The power conversion efficiencies of the cited studies are highlighted, focusing on the products and roles of the organic waste materials used. Graphical Abstract.","Here is a paraphrased version of the original text, exhibiting both syntactic and lexical diversity while preserving semantic similarity: ""A novel breed of photovoltaic devices, dubbed Organic Waste-Derived Solar Cells (OWSC), leverages organic waste materials to fabricate one or more components, thereby offering a groundbreaking alternative to traditional third-generation solar cells. This innovative approach has the potential to transform the solar cell manufacturing landscape. This comprehensive review delves into the realm of OWSC, highlighting their dual benefits of cost-effectiveness and eco-friendliness. The article explores diverse organic waste sources, fabrication methodologies, and pioneering techniques for solar cell production. Strategies for optimizing OWSC performance are also presented. A detailed comparison of the assembly configurations and photovoltaic parameters of reported OWSC is provided, along with an overview of the past decade's research trends. The advantages and limitations of various solar cell technologies are discussed, and potential future directions are proposed. Industrial organic waste materials, such as paper, coal, and plastics, remain underutilized yet promising resources for solar cell fabrication. The power conversion efficiencies of the cited studies are reported, with a focus on the products and functions of the organic waste materials employed.""","Third-generation photovoltaic cells that utilize organic waste materials in their construction are known as organic waste-derived solar cells (OWSCs). These innovative cells offer a sustainable alternative to traditional third-generation solar cells, potentially transforming the future of solar cell production. This review examines various OWSCs, highlighting their cost-effectiveness and environmentally friendly nature. It explores the diverse organic waste materials used, the fabrication and assembly processes involved, and innovative solar cell manufacturing techniques. The study delves into strategies for enhancing OWSC performance, provides a detailed comparison of assembly configurations and photovoltaic parameters, and traces the evolution of OWSC research over the past ten years. Furthermore, it analyzes the strengths and weaknesses of different solar cell technologies, suggesting potential future directions. The review emphasizes the untapped potential of industrial organic waste materials like paper, coal, and plastics for solar cell fabrication, reporting power conversion efficiencies and highlighting the specific roles played by the utilized organic waste materials."
131,Multiplexed gene editing in citrus by using a multi-intron containing Cas9 gene,"Several expression systems have been developed in clustered regularly interspaced short palindromic repeats (CRISPR)-associated protein 9 (CRISPR/Cas9) framework allowing for gene editing of disease-associated genes across diverse citrus varieties. In this study, we present a new approach employing a multi-intron containing Cas9 gene plus multiple gRNAs separated with tRNA sequences to target the phytoene desaturase gene in both ‘Carrizo’ citrange and ‘Duncan’ grapefruit. Notably, using this unified vector significantly boosted editing efficiency in both citrus varieties, showcasing mutations in all three designated targets. The implementation of this multiplex gene editing system with a multi-intron-containing Cas9 plus a gRNA-tRNA array demonstrates a promising avenue for efficient citrus genome editing, equipping us with potent tools in the ongoing battle against several diseases such as canker and huanglongbing.","Various expression platforms have been devised within the CRISPR/Cas9 framework to enable gene modification of disease-related genes in different citrus species. This research introduces a novel method utilizing a Cas9 gene with multiple introns along with several gRNAs separated by tRNA sequences to target the phytoene desaturase gene in both ‘Carrizo’ citrange and ‘Duncan’ grapefruit. Remarkably, this comprehensive vector substantially enhanced editing efficiency in both citrus species, resulting in mutations at all three intended sites. Adopting this multi-intron Cas9 and gRNA-tRNA array-based multiplex gene editing system signifies a promising strategy for effective citrus genome editing, providing valuable tools in the fight against diseases like canker and huanglongbing.","""A range of gene editing platforms have been engineered within the CRISPR/Cas9 framework, enabling the modification of disease-linked genes across a broad spectrum of citrus cultivars. This investigation introduces a novel strategy that leverages a Cas9 gene harboring multiple introns, accompanied by multiple guide RNAs interspersed with tRNA sequences, to target the phytoene desaturase gene in both 'Carrizo' citrange and 'Duncan' grapefruit. Remarkably, the utilization of this consolidated vector resulted in a substantial enhancement of editing efficacy in both citrus varieties, yielding mutations in all three designated targets. The deployment of this multiplex gene editing system, featuring a multi-intron-containing Cas9 and a gRNA-tRNA array, presents a promising route for efficient citrus genome modification, furnishing us with powerful tools in the ongoing struggle against diseases such as canker and huanglongbing.""","Utilizing the CRISPR/Cas9 system, researchers have devised numerous gene editing techniques for manipulating disease-related genes in various citrus cultivars. This research introduces a novel methodology that employs a Cas9 gene with multiple introns and several gRNAs interspersed with tRNA sequences to target the phytoene desaturase gene in ‘Carrizo’ citrange and ‘Duncan’ grapefruit. Remarkably, this integrated vector significantly enhanced editing efficacy in both citrus varieties, resulting in mutations in all three targeted sites. This multiplex gene editing system, featuring a multi-intron Cas9 gene and a gRNA-tRNA array, presents a promising strategy for effective citrus genome modification, providing powerful tools to combat diseases like canker and huanglongbing."
41,Face-to-face more important than digital communication for mental health during the pandemic,"During the lockdowns associated with the COVID-19 pandemic, many people tried to compensate for limited face-to-face interaction by increasing digital communication. Results of a four-week experience sampling study in the German-speaking countries (N = 411 participants; k = 9791 daily questionnaires) suggest, however, that digital communication was far less relevant for lockdown mental health than face-to-face communication. Digital text-based communication (e.g., e-mail, WhatsApp, SMS) nevertheless was meaningfully associated with mental health, and both face-to-face and digital text communication were more predictive of mental health than either physical or outdoor activity. Our results underscore the importance of face-to-face communication for mental health. Our results also suggest that videoconferencing was only negligibly associated with mental health, despite providing more visual and audible cues than digital text communication.","During the COVID-19 lockdowns, numerous individuals sought to offset the lack of in-person interactions by boosting their digital communications. Findings from a four-week experience sampling study in German-speaking countries (N = 411 participants; k = 9791 daily questionnaires) indicate that digital communication played a much smaller role in mental health during lockdowns compared to face-to-face interactions. However, digital text-based communication (e.g., email, WhatsApp, SMS) had a significant association with mental health, with both face-to-face and digital text communication being stronger predictors of mental health than physical or outdoor activities. The results highlight the critical role of face-to-face interactions for mental wellbeing. Additionally, the study found that videoconferencing had a minimal impact on mental health, despite offering more visual and audio cues than digital text communication.","Throughout the COVID-19 pandemic's restrictive periods, individuals sought to offset the dearth of in-person connections by ramping up their online interactions. However, a four-week daily diary study involving 411 participants from German-speaking regions, who completed 9791 questionnaires, revealed that digital communication played a relatively minor role in maintaining lockdown-era mental wellbeing compared to face-to-face exchanges. Notably, digital text-based interactions, such as emailing, WhatsApp messaging, and texting, exhibited a significant correlation with mental health outcomes, while both face-to-face and digital text-based communication proved more influential in predicting mental health than physical activity or outdoor pursuits. The findings emphasize the paramount importance of in-person connections for mental health, and also indicate that videoconferencing, despite offering enhanced visual and auditory cues, had a negligible impact on mental wellbeing.","When COVID-19 lockdowns restricted in-person interactions, individuals turned to digital communication as a substitute. However, a four-week study (N = 411 participants, k = 9791 daily questionnaires) conducted in German-speaking countries revealed that face-to-face interaction was significantly more crucial for mental well-being during this period than digital communication. While text-based digital communication (e.g., email, WhatsApp, SMS) showed a notable link to mental health, both in-person and text-based digital communication were stronger predictors of mental well-being than physical activity or time spent outdoors. These findings highlight the paramount importance of face-to-face interaction for mental health. Interestingly, despite offering more visual and auditory cues, videoconferencing exhibited only a negligible association with mental health."
5,Exploring Temporal Analytics in Fog-Cloud Architecture for Smart Office HealthCare,"Ever since the boost realized in Information and Communication Technology (ICT), market is flooded with high-end multi-tasking devices, presenting a real-time computational environment for technologies like Internet of Things (IoT). With computation at user-end, it provides a fog-based computing paradigm to generate time senstive results, which along with cloud storage presents a comprehensive Fog-Cloud computing paradigm. Because of these reasons, the work presented in this paper focuses on utilizing the potential of IoT Technology to provide a novel Fog-Cloud architecture for efficient healthcare services in smart office. Specifically, a Fog-Cloud architecture has been proposed to monitor and analyze various health attributes of a person during his working hours. Moreover, the framework indulges various activities in the ambient office environment with the purpose of analyzing it for health severity. In order to realize this, a probabilistic measure, named as Severity Index (SI) is defined to evaluate the adverse effects of different activities on personal health. Finally, an application scenario of temporal healthcare predictive monitoring and alert generation is discussed to depict the ideology of Smart Office Healthcare. In order to validate the system, an experimental implmentation is performed on heterogenous datasets. The results obtained in comparison to state-of-the-art techniques show that the proposed model is highly efficient and accurate for providing appropriate healthcare environment during working hours of a person in a smart office.","Since the advancements in Information and Communication Technology (ICT), the market has been inundated with high-end multi-tasking devices, creating a real-time computational environment for technologies such as the Internet of Things (IoT). By enabling user-end computation, this fosters a fog-based computing paradigm that generates time-sensitive results, which, coupled with cloud storage, forms a comprehensive Fog-Cloud computing framework. Therefore, this paper focuses on leveraging IoT technology to introduce an innovative Fog-Cloud architecture aimed at delivering efficient healthcare services within a smart office. Specifically, a Fog-Cloud framework has been developed to monitor and analyze various health metrics of individuals during their working hours. Additionally, the system incorporates various activities in the office environment to assess their impact on health severity. To achieve this, a probabilistic measure called the Severity Index (SI) is defined to evaluate the negative effects of different activities on personal health. Finally, a use case scenario of temporal healthcare predictive monitoring and alert generation is discussed to illustrate the concept of Smart Office Healthcare. To validate the system, an experimental implementation is conducted on heterogeneous datasets, and the results, when compared with state-of-the-art techniques, demonstrate that the proposed model is highly efficient and accurate in providing a suitable healthcare environment during office hours in a smart office.","""Following the significant advancements in Information and Communication Technology (ICT), the market has been inundated with sophisticated, multi-functional devices, thereby creating a dynamic, real-time computational ecosystem conducive to the proliferation of innovative technologies such as the Internet of Things (IoT). By harnessing the power of edge computing, these devices enable the generation of time-critical results, which, when combined with cloud storage, form a comprehensive Fog-Cloud computing framework. This paper capitalizes on the potential of IoT technology to design a novel, efficient Fog-Cloud architecture tailored to provide high-quality healthcare services in intelligent office environments. Specifically, a proposed Fog-Cloud framework is designed to monitor and analyze various physiological parameters of individuals during their working hours, while also integrating ambient office activities to assess their impact on health. A probabilistic metric, dubbed the Severity Index (SI), is introduced to quantify the adverse effects of different activities on personal health. The paper also explores a scenario involving temporal healthcare predictive monitoring and alert generation, exemplifying the concept of Smart Office Healthcare. To validate the system, an experimental implementation is conducted on diverse datasets, yielding results that surpass those of existing techniques, thereby demonstrating the proposed model's exceptional efficiency and accuracy in creating an optimal healthcare environment during working hours in intelligent offices.""","The rapid evolution of Information and Communication Technology (ICT) has led to an abundance of sophisticated, multi-functional devices, fostering a real-time computational landscape that enables technologies like the Internet of Things (IoT). This user-end computational power facilitates a fog-based computing model for generating time-sensitive data, which, when combined with cloud storage, forms a comprehensive Fog-Cloud computing paradigm. This paper leverages the capabilities of IoT technology to propose a novel Fog-Cloud architecture aimed at delivering efficient healthcare services within a smart office environment. The proposed architecture focuses on monitoring and examining various health indicators of individuals during their work hours. Additionally, the framework incorporates diverse activities occurring within the office environment to assess their potential impact on health. To achieve this, a probabilistic measure, termed the Severity Index (SI), is introduced to quantify the negative effects of different activities on individual well-being. A practical application scenario involving temporal healthcare prediction and alert generation is then presented to illustrate the concept of Smart Office Healthcare. The system's efficacy is validated through an experimental implementation using diverse datasets. The results, benchmarked against established techniques, demonstrate the proposed model's superior efficiency and accuracy in providing a suitable healthcare environment during working hours within a smart office."
48,Tactile Codec with Visual Assistance in Multi-modal Communication for Digital Health,"In the digital health, with the development of communication, medical information in all modalities is growing exponentially. Therefore, an effective communication for multi-modal data including tactile and visual information is paramount. In this paper, we propose a novel method to compress the tactile video data from GelSight sensors for the applications of digital health. Firstly, our method combines the visual and tactile modalities to extract the saliency information for the tactile videos. A target recognition network is designed as the visual assistance, which helps tactile videos to extract the effective information frames by recognizing whether objects are touching or not. Secondly, we design a special coding for inter- and intra-frame prediction to further extract the saliency information and compress the tactile signal. Intra-frame prediction utilizes a dynamic group of pictures (GOP) strategy to reduce time redundancy. And intra-frame prediction based on low-rank sparse decomposition (LRSD) is then used to further achieve efficient compression. Finally, Through extensive evaluation of metrics, such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and learned perceptual image patch similarity (LPIPS), our method obtains the better results than advanced video coding (AVC) and high efficiency video coding (HEVC). Our method achieves an average bitrate savings of 23.6% compared to HEVC and 61.4% compared to AVC. The results show that the proposed method can greatly compress the amount of haptic data with high reconstruction quality.","In digital healthcare, advancements in communication are leading to a rapid increase in medical data across various forms. Consequently, effective communication for multi-modal data, which includes tactile and visual information, is crucial. This paper introduces a new approach to compress tactile video data captured by GelSight sensors for digital health applications. Our method initially combines visual and tactile modalities to extract salient information from the tactile videos. A target recognition network, serving as visual assistance, aids tactile videos in identifying effective information frames by detecting whether objects are making contact. Additionally, we design a specialized coding scheme for predicting and compressing tactile signals both within and between frames. The intra-frame prediction employs a dynamic group of pictures (GOP) strategy to minimize temporal redundancy, while the intra-frame prediction relies on low-rank sparse decomposition (LRSD) for further compression efficiency. Comprehensive evaluations using metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and learned perceptual image patch similarity (LPIPS) demonstrate that our method outperforms advanced video coding (AVC) and high-efficiency video coding (HEVC). Specifically, our approach achieves average bitrate reductions of 23.6% compared to HEVC and 61.4% compared to AVC, proving its capability to significantly compress haptic data while maintaining high reconstruction quality.","In the realm of digital healthcare, the proliferation of communication technologies has led to an exponential surge in medical information across various modalities. Consequently, the need for effective communication of multi-modal data, encompassing both tactile and visual information, has become crucial. This paper introduces a novel approach to compressing tactile video data from GelSight sensors, specifically designed for digital health applications. Our methodology involves a two-pronged approach: firstly, we integrate visual and tactile modalities to extract salient information from tactile videos, leveraging a target recognition network as visual assistance to identify frames with object contact. Secondly, we employ a specialized coding scheme that incorporates inter- and intra-frame prediction to further extract salient information and compress tactile signals. This involves utilizing a dynamic group of pictures strategy to reduce temporal redundancy and low-rank sparse decomposition to achieve efficient compression. Our approach is validated through a comprehensive evaluation of metrics, including peak signal-to-noise ratio, structural similarity index, and learned perceptual image patch similarity, demonstrating superior results compared to advanced video coding and high efficiency video coding. Notably, our method achieves an average bitrate reduction of 23.6% and 61.4% compared to HEVC and AVC, respectively, while maintaining high reconstruction quality, thereby significantly compressing haptic data.","The rapid expansion of multi-modal medical data in the digital health realm necessitates efficient communication strategies. This paper introduces a novel approach for compressing tactile video data acquired from GelSight sensors, specifically for digital health applications. The method leverages visual cues through a target recognition network to identify frames containing touch interactions, thereby highlighting salient information within the tactile video stream. Subsequently, a specialized coding scheme incorporating inter- and intra-frame prediction is employed. Inter-frame redundancy is reduced using a dynamic group of pictures (GOP) strategy, while intra-frame compression is achieved through low-rank sparse decomposition (LRSD). Rigorous evaluation using metrics like peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and learned perceptual image patch similarity (LPIPS) demonstrates the superior performance of the proposed method compared to established video coding standards like AVC and HEVC. Notably, the method achieves average bitrate savings of 23.6% and 61.4% relative to HEVC and AVC, respectively. This underscores its ability to significantly compress haptic data while maintaining high reconstruction quality."
52,"Digital communication strategies used by private companies, entrepreneurs, and public entities to attract long-stay tourists: a review","Globalization and the changes in the technological and social industries have facilitated international mobility in such a way that the distinctions between tourism and other forms of mobility closer to migration have practically become blurred. Improvements in digital communication have promoted people’s search for a better quality of life in terms of health, work, leisure, and other aspects. New forms of communication have started to gradually displace the more conventional media. In this context, private companies, entrepreneurs, and public entities have grown aware of the importance of digital communication to achieve a greater scope in their population attraction policies. In this context, the present study aims to identify which digital communication strategies are used to generate interest and attraction to the destination. In our Systematic Literature Review (SLR), we examine the relationship between the terms “lifestyle migration” and “digital communication”, obtaining 294 articles found in Web of Science (WOS) and Scopus. After the analysis, we found 16 potential articles that were linked to the research objectives. The results of the methodology are classified according to the different digital communication actions identified, namely (i) Social Network Relationship; (ii) Digital Marketing, and (iii) Virtual Reality and Augmented Reality. Therefore, this this study contributes to link lifestyle migration and digital communication to destination management offices, companies and entrepreneurs. The paper finishes with a discussion of theoretical and practical implications of research on digital communication strategies in the field of long-stay tourism.","Globalization and transformations in technological and social sectors have made international mobility easier, to the point where the distinctions between tourism and migration-like mobility have nearly vanished. Advances in digital communication have encouraged people to seek improved quality of life concerning health, employment, leisure, and other facets. These new communication methods are slowly replacing traditional media. In this scenario, private companies, entrepreneurs, and public organizations have recognized the critical role of digital communication in enhancing their strategies to attract populations. This study aims to explore the digital communication tactics employed to generate interest in destinations. Through a Systematic Literature Review (SLR), we explored the terms ""lifestyle migration"" and ""digital communication,"" identifying 294 articles from the Web of Science (WOS) and Scopus. Our analysis highlighted 16 key papers relevant to our research goals. The outcomes of our study are categorized based on the digital communication actions identified: (i) Social Network Relationships; (ii) Digital Marketing; and (iii) Virtual Reality and Augmented Reality. Consequently, this research links lifestyle migration with digital communication for destination management offices, businesses, and entrepreneurs. The paper concludes by discussing the theoretical and practical implications of researching digital communication strategies within long-term tourism.","The convergence of globalization and advancements in technological and social sectors has led to a significant increase in international mobility, rendering the boundaries between tourism and migration-like movements increasingly ambiguous. The proliferation of digital communication has sparked a quest for enhanced well-being, encompassing aspects such as health, employment, leisure, and beyond. Novel communication platforms are gradually supplanting traditional media, prompting private enterprises, entrepreneurs, and public institutions to recognize the pivotal role of digital communication in expanding their reach and appeal to target populations. Against this backdrop, this study seeks to determine the digital communication strategies employed to foster interest and attraction to a given destination. A systematic review of existing literature, yielding 294 articles from Web of Science and Scopus, reveals 16 pertinent studies aligned with our research objectives. Our findings are categorized according to three primary digital communication approaches: (i) cultivating social network relationships, (ii) leveraging digital marketing, and (iii) harnessing virtual and augmented reality. Ultimately, this research bridges the gap between lifestyle migration and digital communication, providing valuable insights for destination management offices, companies, and entrepreneurs.","The interconnected world, shaped by rapid technological and societal shifts, has blurred the lines between tourism and migration, fostering unprecedented international movement. Driven by enhanced digital communication, individuals seek an improved quality of life encompassing health, work, and leisure. Traditional media are being overshadowed by these new forms of communication, prompting private and public sectors alike to recognize their power in attracting people. This research delves into the digital communication strategies employed to spark interest and entice individuals to specific destinations. A systematic review of 294 articles from Web of Science and Scopus, focusing on the intersection of ""lifestyle migration"" and ""digital communication,"" yielded 16 relevant studies. The findings are categorized into three digital communication approaches: social network engagement, digital marketing, and virtual/augmented reality. This study bridges the gap between lifestyle migration and digital communication, offering valuable insights for destination management organizations, businesses, and entrepreneurs. The concluding discussion explores the theoretical and practical implications of digital communication strategies within the realm of long-term tourism."
180,Measurement and spatiotemporal characteristics of China’s green finance,"To study the extent of green finance development in China, this article constructs a green finance index system and employs the entropy value method to measure China’s green finance by using a yearly provincial panel data from 2001 to 2020. The Thiel and Moran indices are then used to systematically analyze the temporal and spatial distribution of China’s regional green finance. The findings are summarized as follows. Firstly, the overall green finance index in China experiences an upward trend. The development of green finance in the eastern region is superior to that in other regions in terms of absolute value and growth rate. Moreover, the differences in China’s green finance index have shown an increasing trend over the last two decades, which is mostly contributed by the intra-regional differences. Finally, the inter-regional distribution of green finance index demonstrates that green finance development has a spatial spillover effect.","To investigate the progression of green finance in China, this article develops a green finance index framework and utilizes the entropy value method to assess China's green finance through annual provincial panel data spanning from 2001 to 2020. The Thiel and Moran indices are applied to systematically examine the temporal and spatial distribution of regional green finance in China. The results indicate that, firstly, the green finance index across China shows an increasing trend overall. In absolute terms and growth rate, green finance development in the eastern region outperforms other regions. Additionally, disparities in the green finance index across China have increased over the past two decades, largely due to intra-regional variations. Lastly, the inter-regional distribution analysis reveals that the development of green finance exhibits a spatial spillover effect.","""This research assesses the progression of China's green finance sector by devising a comprehensive index framework and applying the entropy value approach to a 20-year provincial dataset spanning 2001-2020. A systematic examination of the temporal and spatial patterns of regional green finance in China is conducted using the Thiel and Moran indices. The key takeaways are as follows: China's overall green finance index exhibits a steady ascent. The eastern region outperforms other areas in terms of both absolute value and growth rate, indicating a superior development trajectory. Furthermore, the disparities in China's green finance index have widened over the past two decades, primarily driven by intra-regional variations. Lastly, the spatial distribution of the green finance index reveals a significant spillover effect, underscoring the importance of regional coordination in green finance development.""","This study investigates the progress of green finance in China by developing a comprehensive index system and utilizing the entropy value method to quantify it using annual provincial panel data spanning two decades (2001-2020). The temporal and spatial patterns of regional green finance in China are then systematically examined using the Thiel and Moran indices. Key findings reveal a consistent upward trajectory in China's overall green finance index, with the eastern region exhibiting superior development in both absolute terms and growth rate compared to other regions. Furthermore, disparities in China's green finance index have widened over the past twenty years, primarily driven by intra-regional variations. Finally, the spatial distribution of the green finance index suggests the presence of a spillover effect in the development of green finance across regions."
141,Government size and automation,"This paper explores the consequences of automation for public finance. We find that as the automation rate increases, the government size, measured as the fiscal revenues to output ratio, declines. This is due to the substitution of traditional inputs, which bear the burden of taxes, by the new automatic technology. These results are explained by the effects of automation on labor, where taxation of labor income (including social security contributions) represents the most important source of fiscal revenues in most advanced economies. The paper conducts two additional counterfactual experiments. First, we calculate how individual tax rates should be changed in response to automation in order to maintain constant fiscal revenues from the different sources of taxes. This experiment reveals that this fiscal policy would have significantly detrimental effects on output and labor, and indicates that a comprehensive reform of the current tax mix is necessary to counterbalance the effects of automation on public finance. Second, we calculate the tax rate on capital, without modifying the other tax rates, required to keep constant the size of the government, resulting in a capital income tax rate of around 0.77 for an automation rate of \(45 \%\).","This study investigates the impact of automation on public finances. The findings indicate that as the automation rate rises, the government's size—defined by the ratio of fiscal revenues to output—diminishes. This trend results from the replacement of conventional, tax-bearing inputs with new automated technologies. The decline is primarily attributed to the effects of automation on labor, where labor income taxes (including social security contributions) constitute the main source of fiscal revenue in most developed countries. The research also conducts two further hypothetical scenarios. Firstly, it examines how individual tax rates would need to be adjusted in response to automation to maintain stable fiscal revenues from various tax sources. This scenario shows that such fiscal adjustments would have profoundly negative impacts on output and labor, suggesting that a thorough reform of the existing tax structure is essential to mitigate automation's effects on public finance. Secondly, the study calculates the capital tax rate necessary to preserve the government's size, assuming other tax rates remain unchanged, which results in a capital income tax rate of approximately 0.77 for an automation rate of 45%.","This study delves into the repercussions of automation on government finances, revealing that as automation intensifies, the ratio of fiscal revenues to output diminishes, primarily due to the replacement of traditional, tax-burdened inputs with novel automated technologies. The findings are attributed to automation's impact on labor, which is the primary source of fiscal revenues in advanced economies, largely comprising taxation of labor income and social security contributions. The paper presents two counterfactual scenarios: one examines the necessary adjustments to individual tax rates to maintain consistent fiscal revenues, highlighting the need for a comprehensive tax reform to mitigate automation's effects on public finance, and the other calculates the required capital income tax rate, approximately 0.77 at a 45% automation rate, to preserve the government's size without modifying other tax rates.","This study investigates the impact of automation on government finances. The findings suggest that as automation becomes more prevalent, the relative size of the government, as indicated by the ratio of fiscal revenue to economic output, shrinks. This trend arises from the replacement of conventional, taxable inputs with automated technologies. The authors attribute these results to the influence of automation on labor markets, given that taxes on labor income, encompassing social security contributions, constitute the primary source of government revenue in most developed economies. Two hypothetical scenarios are examined. In the first, the authors determine how individual tax rates would need to be adjusted in response to automation to maintain consistent fiscal revenue streams from various tax sources. This analysis reveals that such a fiscal policy would have significant negative consequences for both economic output and employment, emphasizing the need for a comprehensive overhaul of the existing tax structure to mitigate the effects of automation on government finances. In the second scenario, the authors calculate the capital tax rate necessary to preserve the size of the government without altering other tax rates. This calculation yields a capital income tax rate of approximately 0.77, assuming an automation rate of 45%."
161,Cybersecurity knowledge graphs,"Cybersecurity knowledge graphs, which represent cyber-knowledge with a graph-based data model, provide holistic approaches for processing massive volumes of complex cybersecurity data derived from diverse sources. They can assist security analysts to obtain cyberthreat intelligence, achieve a high level of cyber-situational awareness, discover new cyber-knowledge, visualize networks, data flow, and attack paths, and understand data correlations by aggregating and fusing data. This paper reviews the most prominent graph-based data models used in this domain, along with knowledge organization systems that define concepts and properties utilized in formal cyber-knowledge representation for both background knowledge and specific expert knowledge about an actual system or attack. It is also discussed how cybersecurity knowledge graphs enable machine learning and facilitate automated reasoning over cyber-knowledge.","Cybersecurity knowledge graphs, employing a graph-based data framework to depict cyber-information, provide comprehensive strategies for handling large volumes of intricate cybersecurity data from various origins. They aid security analysts in acquiring cyberthreat intelligence, enhancing cyber-situational awareness, discovering novel cyber-insights, visualizing networks, data movements, and attack trajectories, and discerning data relationships through the integration and amalgamation of information. This paper examines the foremost graph-based data frameworks utilized in this field and the knowledge organization systems that define the concepts and attributes employed in formal cybersecurity knowledge representation for both foundational knowledge and specialized expert insight regarding actual systems or attacks. Additionally, it explores how cybersecurity knowledge graphs support machine learning and bolster automated reasoning in relation to cyber-information.","Graph-structured cybersecurity repositories, which encapsulate complex cyber-information within a network-based framework, offer comprehensive solutions for handling enormous amounts of heterogeneous cybersecurity data emanating from varied sources. By leveraging these repositories, security experts can glean actionable threat insights, attain enhanced situational awareness, uncover novel cyber-insights, illustrate network topologies, data trajectories, and attack vectors, and discern correlations among data points through aggregation and fusion. This study surveys the most influential graph-based frameworks employed in this realm, alongside knowledge taxonomy systems that delineate concepts and attributes utilized in formal cyber-knowledge codification for both foundational knowledge and specialized expertise pertaining to specific systems or attacks. Furthermore, it explores how cybersecurity knowledge graphs facilitate machine learning applications and enable automated inference across cyber-knowledge domains.","Cybersecurity knowledge graphs, employing a graph-structured data model to depict cyber-related information, offer comprehensive methodologies for managing vast amounts of intricate cybersecurity data originating from various sources. These graphs empower security professionals to extract cyberthreat intelligence, attain heightened cyber-situational awareness, uncover novel cyber insights, generate visualizations of networks, data flows, and attack vectors, and discern data correlations through aggregation and fusion. This study examines the prevailing graph-based data models employed in the cybersecurity realm, alongside knowledge organization systems that delineate concepts and attributes employed in formal cyber-knowledge representation, encompassing both foundational knowledge and specialized expert knowledge concerning specific systems or attacks. Additionally, the paper explores how cybersecurity knowledge graphs enable machine learning applications and streamline automated reasoning processes applied to cyber-knowledge."
30,Blockchain-Technologie im Supply Chain Management – Anwendungspotenziale und Kompetenzlücken,"Unternehmen und anwendungsorientierte Forschung weisen der Blockchain-Technologie enorme Potenziale in diversen Sektoren zu. Besonders erfolgversprechend erscheint dabei der Einsatz von Blockchain-Technologien im Kontext des Supply Chain-Managements, da hiermit die Effizienz, die Transparenz und das Vertrauen zwischen den unterschiedlichen Akteuren über die gesamte Lieferkette gesteigert werden können. Ein Erfolgsfaktor für die Einführung und den Betrieb produktiver Blockchain-Lösungen bildet dabei ein adäquates Kompetenzmanagement, mit dem sichergestellt wird, dass die Mitarbeiter die notwendigen Fähigkeiten und Kenntnisse besitzen. Bestehende Kompetenzrahmen umfassen zwar eine Vielzahl digitaler Kompetenzen, jedoch adressieren diese bislang keine dedizierten Kompetenzen zum Einsatz von Blockchain-Technologien. Dieses Defizit wird mit dem vorliegenden Beitrag aufgegriffen, indem aufbauend auf einer kurzen Einführung und Darstellung der Blockchain-Technologie deren Anwendungspotenziale im Supply Chain-Management identifiziert werden. Anschließend wird ein Kompetenzrahmen zum Blockchain-basierten Supply Chain-Management entwickelt. Schließlich werden Kompetenzlücken in der Praxis aufgezeigt, Handlungsempfehlungen für Unternehmen formuliert sowie eine Schlussbetrachtung durchgeführt.","Unternehmen und praxisorientierte Forschung sehen in der Blockchain-Technologie großes Potenzial in verschiedenen Branchen. Besonders vielversprechend erscheint die Anwendung von Blockchain im Bereich des Supply Chain Managements, da sie die Effizienz, Transparenz und das Vertrauen zwischen den verschiedenen Beteiligten entlang der gesamten Lieferkette verbessern kann. Ein entscheidender Erfolgsfaktor für die Einführung und den Betrieb effektiver Blockchain-Lösungen ist ein geeignetes Kompetenzmanagement, das sicherstellt, dass die Mitarbeiter über die erforderlichen Fähigkeiten und Kenntnisse verfügen. Obwohl bestehende Kompetenzrahmen eine Vielzahl digitaler Kompetenzen abdecken, berücksichtigen sie bisher keine speziellen Fähigkeiten für den Einsatz von Blockchain-Technologien. Dieses Manko wird in dem vorliegenden Beitrag behandelt, indem nach einer kurzen Einführung und Darstellung der Blockchain-Technologie deren Anwendungsmöglichkeiten im Supply Chain Management aufgezeigt werden. Daraufhin wird ein Kompetenzrahmen für blockchainbasiertes Supply Chain Management entwickelt. Abschließend werden bestehende Kompetenzlücken in der Praxis analysiert, Handlungsempfehlungen für Unternehmen vorgestellt und eine abschließende Betrachtung durchgeführt.","""Companies and application-oriented research attribute immense potential to blockchain technology across various sectors. The deployment of blockchain technology in supply chain management appears particularly promising, as it can enhance efficiency, transparency, and trust among diverse stakeholders throughout the entire supply chain. A crucial success factor for introducing and operating productive blockchain solutions lies in adequate competence management, ensuring that employees possess the necessary skills and knowledge. While existing competence frameworks encompass a range of digital skills, they currently fail to address dedicated competencies for blockchain technology deployment. This gap is addressed in this contribution, which identifies blockchain technology's application potential in supply chain management following a brief introduction and overview. A competence framework for blockchain-based supply chain management is then developed, and competence gaps in practice are highlighted, along with recommendations for companies and a concluding reflection.""","In vielen Branchen erkennen Unternehmen und praxisorientierte Forschung das enorme Potenzial der Blockchain-Technologie. Insbesondere im Bereich des Supply Chain Managements verspricht der Einsatz dieser Technologie signifikante Verbesserungen in puncto Effizienz, Transparenz und Vertrauen zwischen den verschiedenen Akteuren entlang der gesamten Lieferkette. Für die erfolgreiche Implementierung und den Betrieb leistungsfähiger Blockchain-Lösungen ist ein angemessenes Kompetenzmanagement von entscheidender Bedeutung, um sicherzustellen, dass die Mitarbeiter über die erforderlichen Fähigkeiten und Kenntnisse verfügen. Obwohl bestehende Kompetenzrahmen eine Bandbreite digitaler Kompetenzen abdecken, fehlt es bisher an spezifischen Kompetenzen für den Einsatz von Blockchain-Technologien. Dieser Beitrag adressiert diesen Mangel, indem er nach einer kurzen Einführung in die Blockchain-Technologie und deren Funktionsweise die Anwendungsmöglichkeiten im Supply Chain Management aufzeigt. Darauf aufbauend wird ein Kompetenzrahmen für Blockchain-basiertes Supply Chain Management entwickelt. Abschließend werden Kompetenzdefizite in der Praxis beleuchtet, Handlungsempfehlungen für Unternehmen formuliert und eine Schlussfolgerung gezogen."
54,Internet of things connecting digital world through visible light communication,"This study focuses on the use of Light Emitting Diodes (LEDs) for lighting, incorporating Visible Light Communication (VLC) technology for data transmission. The Internet of Things (IoT), an inventive technology can connect physical objects to the digital world through VLC. In smart environments, IoT interacts with Wireless Sensor Networks (WSN) and Mobile Ad-Hoc Networks (MANET), enhancing user experience and economic viability. This interaction forms new MANET-IoT systems in VLC-based networks, offering increased user mobility, energy saving and reduced network deployment costs. This work compares the performance of Light Fidelity technology of VLC and Radio Frequency (RF) technology, considering factors such as data rate, Bit Error Rate, and power consumption. The focus extends beyond energy efficiency to emphasize the creation of a hazard-free environment in the near future by using LEDs in indoors and outdoors. The obtainable outcomes of this investigation is energy consumption using global MANET and WSN in IoT VLC based system, marking a significant step towards providing reliable services over internet. Packet Delivery Rate is estimated for 100 node counts of Cluster Nodes Reinforcement Scheme in comparison with Reliable Data Transmission Model and EA-based Imperialist Competitive Algorithm (MANET) schemes in the work.","This research investigates the application of Light Emitting Diodes (LEDs) for illumination, integrating Visible Light Communication (VLC) technology for data exchange. The Internet of Things (IoT), a novel technology, enables the connection of physical devices to the digital realm via VLC. In intelligent environments, IoT engages with Wireless Sensor Networks (WSNs) and Mobile Ad-Hoc Networks (MANETs), improving user experience and economic feasibility. This synergy creates new MANET-IoT frameworks within VLC-based networks, which offer enhanced user mobility, energy efficiency, and reduced network setup costs. This study evaluates the performance of Light Fidelity (Li-Fi) technology of VLC against Radio Frequency (RF) technology, examining parameters such as data speed, Bit Error Rate, and energy consumption. The emphasis goes beyond energy efficiency to promoting a safe environment for both indoor and outdoor LED use. The study's results highlight energy consumption using global MANET and WSN in an IoT VLC-based system, representing a significant advancement towards delivering reliable internet services. Packet Delivery Rate is estimated for 100 node counts of Cluster Nodes Reinforcement Scheme compared with Reliable Data Transmission Model and EA-based Imperialist Competitive Algorithm (MANET) schemes.","This research explores the application of Light Emitting Diodes (LEDs) in illumination systems, leveraging Visible Light Communication (VLC) technology to facilitate data exchange. The innovative Internet of Things (IoT) paradigm enables the interconnection of physical entities with the digital realm via VLC, fostering seamless interactions with Wireless Sensor Networks (WSN) and Mobile Ad-Hoc Networks (MANET) in intelligent environments. This synergy gives rise to novel MANET-IoT systems in VLC-based infrastructures, yielding benefits such as enhanced user mobility, reduced energy expenditure, and lower network deployment expenditures. A comparative analysis of the performance of VLC-based Light Fidelity technology and Radio Frequency (RF) technology is conducted, considering key metrics including data transmission rates, error rates, and power consumption. The investigation's scope extends beyond energy efficiency to prioritize the creation of a safe and hazard-free environment in the near future, both indoors and outdoors, through the strategic deployment of LEDs. The anticipated outcomes of this study include the optimization of energy consumption in global MANET and WSN systems integrated with IoT VLC, thereby paving the way for the provision of reliable internet services. The Packet Delivery Rate is evaluated for 100 node counts of the Cluster Nodes Reinforcement Scheme, benchmarked against the Reliable Data Transmission Model and EA-based Imperialist Competitive Algorithm (MANET) schemes.","This research explores the utilization of LEDs for illumination, integrating VLC technology to facilitate data transfer. VLC, coupled with the innovative IoT, bridges the physical and digital realms by connecting objects. Within intelligent environments, IoT interfaces with WSN and MANET, optimizing user satisfaction and cost-effectiveness. This synergy gives rise to novel MANET-IoT systems within VLC networks, characterized by enhanced user mobility, energy conservation, and minimized deployment expenses. The study benchmarks Light Fidelity technology (VLC) against RF technology, evaluating metrics such as data transfer speed, error rate, and power usage. The scope transcends energy efficiency, aiming to establish a secure environment using LEDs for both indoor and outdoor illumination. This investigation determines energy consumption within a global MANET and WSN integrated IoT VLC system, paving the way for dependable internet services. Packet delivery rates are projected for 100 nodes utilizing the Cluster Nodes Reinforcement Scheme, contrasted with the Reliable Data Transmission Model and the EA-based Imperialist Competitive Algorithm (MANET) schemes."
12,Towards Education 4.0 in Geotechnical Engineering Using a Virtual Reality/Augmented Reality Visualization Platform,"Education 4.0 can play a significant role in the future of geotechnical engineering education. It can provide personalized and equitable learning experiences and be used to develop skills for the fourth industrial revolution in geotechnical engineering. In this paper, we explore the application of Augmented Reality (AR) and Virtual Reality (VR) models, as part of Education 4.0, to enhance the presentation and communication of soil mechanics concepts. A detailed step-by-step process for creating 3D representations of geotechnical concepts, including texturing, UV mapping, animation, and export and publishing techniques, is introduced. For this purpose, two powerful 3D modeling and animation software programs, Autodesk Maya and Blender, are employed to develop geotechnical avatars in a soil mechanics laboratory. In Autodesk Maya, a detailed representation of the soil phase diagram is presented, while Blender is utilized to create a Consolidated Undrained (CU) triaxial laboratory experiment. The geotechnical testing avatars are then uploaded to Sketchfab, a popular publishing website that supports AR/VR formats. We also explore the potential for establishing a virtual laboratory for geotechnical engineering, highlighting the transformative possibilities it offers in terms of practical learning experiences and educational accessibility.","Geotechnical engineering education stands to benefit greatly from Education 4.0, which can offer tailored and fair learning opportunities while fostering skills relevant to the fourth industrial revolution. This study examines the utilization of Augmented Reality (AR) and Virtual Reality (VR) within Education 4.0 to improve the understanding and communication of soil mechanics principles. It outlines a comprehensive, step-by-step method for generating 3D visualizations of geotechnical concepts, detailing processes such as texturing, UV mapping, animation, and the techniques for export and publication. Two advanced 3D modeling and animation tools, Autodesk Maya and Blender, are used to create geotechnical avatars for a soil mechanics lab. Autodesk Maya is employed to produce an intricate representation of the soil phase diagram, while Blender is used to simulate a Consolidated Undrained (CU) triaxial test. These geotechnical avatars are then published on Sketchfab, a widely-used platform that supports AR/VR formats. Additionally, the paper discusses the potential creation of a virtual laboratory for geotechnical engineering, emphasizing the significant improvements it could bring to practical learning and educational inclusivity.","The future of geotechnical engineering education can be revolutionized by the integration of Education 4.0, which has the potential to offer tailored and inclusive learning environments, thereby equipping students with the skills necessary to thrive in the fourth industrial revolution. This study delves into the application of immersive technologies, such as Augmented Reality (AR) and Virtual Reality (VR), as a key component of Education 4.0, to revitalize the teaching and communication of complex soil mechanics concepts. A comprehensive, step-by-step guide is presented for crafting interactive 3D models of geotechnical concepts, encompassing techniques such as texturing, UV mapping, animation, and exportation. To achieve this, two industry-leading 3D modeling and animation software programs, Autodesk Maya and Blender, are leveraged to design geotechnical avatars within a simulated soil mechanics laboratory setting. Autodesk Maya is utilized to create a detailed, interactive representation of the soil phase diagram, whereas Blender is employed to recreate a Consolidated Undrained (CU) triaxial laboratory experiment. The resulting geotechnical testing avatars are then uploaded to Sketchfab, a prominent online platform supporting AR/VR formats. Furthermore, this study explores the feasibility of establishing a virtual geotechnical engineering laboratory, highlighting its potential to transform practical learning experiences and expand educational accessibility.","The integration of Education 4.0 technologies holds immense potential for revolutionizing geotechnical engineering education. By tailoring learning experiences and equipping students with skills essential for the fourth industrial revolution, this approach promises to significantly enhance pedagogy. This study delves into the utilization of Augmented Reality (AR) and Virtual Reality (VR) models within the framework of Education 4.0 to enrich the conveyance of soil mechanics principles. A comprehensive, sequential methodology for generating 3D visualizations of geotechnical concepts is outlined, encompassing texturing, UV mapping, animation, and dissemination techniques. Leveraging the capabilities of two robust 3D modeling and animation software packages, Autodesk Maya and Blender, geotechnical avatars are meticulously crafted within a virtual soil mechanics laboratory setting. Autodesk Maya facilitates the intricate depiction of the soil phase diagram, while Blender is employed to simulate a Consolidated Undrained (CU) triaxial laboratory test. These geotechnical testing avatars are subsequently disseminated via Sketchfab, a widely recognized publishing platform compatible with AR/VR formats. Furthermore, the study examines the prospects of establishing a virtual laboratory environment for geotechnical engineering, emphasizing its transformative potential in delivering immersive practical learning experiences and fostering educational accessibility."
119,From AI Ethics Principles to Practices: A Teleological Methodology to Apply AI Ethics Principles in The Defence Domain,"This article provides a methodology for the interpretation of AI ethics principles to specify ethical criteria for the development and deployment of AI systems in high-risk domains. The methodology consists of a three-step process deployed by an independent, multi-stakeholder ethics board to: (1) identify the appropriate level of abstraction for modelling the AI lifecycle; (2) interpret prescribed principles to extract specific requirements to be met at each step of the AI lifecycle; and (3) define the criteria to inform purpose- and context-specific balancing of the principles. The methodology presented in this article is designed to be agile, adaptable, and replicable, and when used as part of a pro-ethical institutional culture, will help to foster the ethical design, development, and deployment of AI systems. The application of the methodology is illustrated through reference to the UK Ministry of Defence AI ethics principles.","This article outlines a method for interpreting AI ethics principles to determine specific ethical standards for developing and implementing AI systems in high-risk areas. The method involves a three-stage process conducted by an independent, diverse ethics committee to: (1) establish the appropriate abstraction level for modeling the AI lifecycle; (2) translate the given principles into detailed requirements for each stage of the AI lifecycle; and (3) develop criteria to guide the context- and purpose-specific balancing of these principles. The described methodology is meant to be flexible, adaptable, and reproducible, and when implemented within a pro-ethical institutional culture, it aims to promote the ethical creation, development, and use of AI systems. The methodology’s application is demonstrated using the AI ethics principles from the UK Ministry of Defence.","This paper outlines a framework for operationalizing AI ethics guidelines to establish clear benchmarks for the creation and implementation of artificial intelligence in high-stakes environments. The approach involves a tripartite process, overseen by a diverse, autonomous ethics committee, which: (1) determines the optimal level of granularity for mapping the AI development cycle; (2) distills core principles into actionable directives for each lifecycle stage; and (3) establishes guidelines for balancing competing principles in context-dependent scenarios. The proposed framework is intentionally flexible, scalable, and repeatable, and when integrated into an organization's ethical DNA, it facilitates the responsible design, development, and deployment of AI solutions, as exemplified by its application to the UK Ministry of Defence's AI ethics framework.","This paper outlines a three-step, adaptable process for translating broad AI ethics principles into practical, context-specific guidelines for creating and using AI systems in sensitive areas. This process, intended to be executed by an impartial committee of diverse stakeholders, first determines the appropriate level of detail needed to represent the AI system's lifespan. Then, it deciphers the given principles, extracting precise requirements for each stage of the AI lifecycle. Finally, it establishes criteria for achieving a balanced application of these principles based on the AI system's purpose and operational context. This agile and reproducible methodology, when embedded in an ethically-conscious organizational culture, can contribute to the development and implementation of AI systems that prioritize ethical considerations. Its practical application is exemplified through its alignment with the AI ethics principles of the UK Ministry of Defence."
182,Municipal finance shapes urban climate action and justice,"Implementing climate policies and programmes in cities requires substantial investments that inevitably entangle climate action with urban climate finance—the mechanisms and practices city governments use to pay for climate efforts. Here we use US cities as a case study to examine how climate finance impacts, and is impacted by, the pursuit of urban climate action and climate justice. Drawing on 34 expert interviews, we show how municipal financial decisions and budgetary practices are shaping how, when and for whom cities are responding to climate change. We demonstrate how public spending decisions are intertwined with the logics of debt financing and examine the impacts of these relationships on cities’ climate investments. We showcase the structuring impacts of finance on climate action and the built environment, and we introduce pathways through which climate and justice considerations are already being integrated into, and potentially transforming, municipal finance in the United States.","Implementing climate initiatives and programs within urban settings necessitates significant investments, inherently linking climate actions with municipal climate finance—the strategies and operations through which city administrations fund their climate endeavors. In this study, we use U.S. cities as a focal point to investigate the reciprocal effects between climate finance and the execution of urban climate actions along with climate justice. By analyzing 34 expert interviews, our findings reveal how municipal fiscal decisions and budgetary methodologies influence the timing, manner, and demographic targeting of climate responses by cities. We highlight the critical role of public expenditure choices and their connection to debt financing principles, examining how these interactions affect urban climate funding. Our study demonstrates the determining influence of financial structures on climate initiatives and urban infrastructure, and it presents avenues where climate and justice factors are being, and have the potential to be, integrated into, and thus transform, city finance in the United States.","""Effecting meaningful climate initiatives in urban centers necessitates significant outlays, inextricably linking climate mitigation with municipal financial strategies—the fiscal frameworks and instruments employed by city administrations to fund climate-related endeavors. This study utilizes US cities as a paradigm to investigate the reciprocal influences between climate finance and the quest for urban climate resilience and social equity. Through an analysis of 34 in-depth expert consultations, we reveal how local budgetary allocations and fiscal policies are dictating the pace, scope, and beneficiaries of urban climate responses. We elucidate the intricate relationships between public expenditure decisions and debt financing modalities, and assess the implications of these dynamics on municipal climate investments. Furthermore, we highlight the profound impact of financial structures on climate action and the built environment, while identifying avenues through which climate and social justice considerations are being incorporated into, and potentially revolutionizing, municipal financial practices in the United States.""","Navigating the financial complexities of climate action in urban environments necessitates a deep dive into the mechanisms that underpin climate finance. Taking US cities as an illustrative example, this study explores the symbiotic relationship between climate finance and the pursuit of both climate action and climate justice. By synthesizing insights from 34 expert interviews, we illuminate how municipal budget allocation and financial practices profoundly influence the timing, nature, and beneficiaries of urban climate responses. Furthermore, we unpack the intricate interplay between public expenditure and debt financing, shedding light on its implications for climate-related investments. Our analysis reveals the significant influence of financial frameworks on both climate action and the physical characteristics of cities. Finally, we identify avenues through which considerations of climate and justice are being interwoven into municipal finance, highlighting their potential to reshape financial practices in the US."
57,"The underuse of AI in the health sector: Opportunity costs, success stories, risks and recommendations","PurposeThis contribution explores the underuse of artificial intelligence (AI) in the health sector, what this means for practice, and how much the underuse can cost. Attention is drawn to the relevance of an issue that the European Parliament has outlined as a ""major threat"" in 2020. At its heart is the risk that research and development on trusted AI systems for medicine and digital health will pile up in lab centers without generating further practical relevance. Our analysis highlights why researchers, practitioners and especially policymakers, should pay attention to this phenomenon.MethodsThe paper examines the ways in which governments and public agencies are addressing the underuse of AI. As governments and international organizations often acknowledge the limitations of their own initiatives, the contribution explores the causes of the current issues and suggests ways to improve initiatives for digital health.ResultsRecommendations address the development of standards, models of regulatory governance, assessment of the opportunity costs of underuse of technology, and the urgency of the problem.ConclusionsThe exponential pace of AI advances and innovations makes the risks of underuse of AI increasingly threatening.Graphical Abstract","Objective: This article investigates the insufficient application of artificial intelligence (AI) in the healthcare sector, its implications for practice, and the associated costs of this underutilization. The discussion underscores the importance of an issue identified by the European Parliament in 2020 as a ""major threat."" Central to this concern is the risk that research and development of reliable AI systems for medical and digital health may remain confined to laboratories, thereby failing to achieve significant practical impact. Our analysis illuminates the reasons for researchers, practitioners, and especially policymakers, to focus on this issue. Approach: The study explores how governments and public entities are managing the insufficient use of AI. Recognizing that governments and international bodies often concede the limitations of their efforts, the paper delves into the root causes of the prevailing challenges and proposes improvements for digital health initiatives. Findings: The recommendations include the creation of standards, models for regulatory oversight, evaluation of the opportunity costs of technology underuse, and the pressing nature of the problem. Conclusions: The rapid advancement of AI innovations accentuates the increasingly perilous nature of underutilization in this field.","""This study delves into the phenomenon of artificial intelligence (AI) being underutilized in the healthcare industry, its implications for real-world applications, and the substantial economic burden it entails. The significance of this issue is underscored by the European Parliament's designation of it as a ""major threat"" in 2020, with the core concern being that research on trustworthy AI systems for medicine and digital health may stagnate in laboratory settings without yielding tangible benefits. Our investigation emphasizes the need for researchers, practitioners, and policymakers to acknowledge this phenomenon. An examination of government and public agency initiatives reveals the limitations of current approaches, prompting an exploration of the underlying causes and potential solutions. The proposed recommendations encompass the establishment of standards, regulatory governance frameworks, and the assessment of opportunity costs associated with underutilizing technology, as well as the urgency of addressing this issue. The rapid acceleration of AI advancements heightens the risks of underutilization, making it an increasingly pressing concern.""","This paper investigates the insufficient implementation of artificial intelligence (AI) within healthcare, examining its practical implications and potential economic repercussions. Highlighting a concern deemed a ""major threat"" by the European Parliament in 2020, the study focuses on the risk of promising AI research for medicine and digital health remaining confined to laboratories without tangible real-world application. The analysis underscores the importance of this issue for researchers, practitioners, and particularly policymakers. By exploring governmental and public agency approaches to tackling this underutilization, and acknowledging the limitations of existing initiatives, the paper delves into the root causes of the problem. It proposes solutions for enhancing digital health initiatives, including the establishment of standards, frameworks for regulatory governance, evaluation of missed opportunities stemming from technology underuse, and emphasizes the pressing nature of the matter. The authors conclude that the rapid evolution of AI necessitates urgent action, as the risks associated with insufficient implementation become increasingly significant."
15,Designing for collaborative learning in immersive virtual reality: a systematic literature review,"Immersive learning technologies such as virtual reality have long been deemed as the next generation of digital learning environments. There is a limited number of studies addressing how immersive technologies can be designed, applied, and studied in collaborative learning settings. This paper presents a systematic review of empirical studies reporting on use of immersive virtual reality in collaborative learning within educational and professional learning settings. 11 studies have been grouped and coded in a textual narrative synthesis, outlining the pedagogical concepts behind the learning design, as well as the design of virtual reality environments and the collaborative learning activities in which the technology is employed. The results suggest that collaborative learning in virtual reality can currently be conceptualised as a shared experience in an immersive, virtually mediated space, where there is a shared goal/problem which learners must attend to collaboratively. This conceptualisation implies a need to design technologies, environments, and activities that support participation and social interaction, fostering collaborative learning processes. Based on the outlined conceptualisation, we present a series of recommendations for designing for collaborative learning in immersive virtual reality. The paper concludes that collaborative learning in virtual reality creates a practice- and reflection space, where learning is perceived as engaging, without the risk of interfering with actual practices. Current designs however struggle with usability, realism, and facilitating social interaction. The paper further identifies a need for future research into what happens within virtual reality, rather than only looking at post-virtual reality evaluations.","Advanced digital learning environments are increasingly seen as the future, with immersive learning technologies like virtual reality at the forefront. However, research on the design, application, and study of these technologies in collaborative learning settings remains limited. This paper systematically reviews empirical studies that investigate the use of immersive virtual reality for collaborative learning in both educational and professional contexts. It categorizes and synthesizes 11 studies, highlighting the pedagogical principles underlying learning design, the creation of virtual reality environments, and the collaborative activities facilitated by this technology. The findings indicate that collaborative learning in virtual reality is currently viewed as a shared immersive experience in a digitally mediated space, where learners collaborate on a common goal or problem. This view necessitates the development of technologies, environments, and activities that enhance participation and social interaction, thereby supporting collaborative learning. Based on this conceptualization, the paper offers several recommendations for designing immersive virtual reality for collaborative learning. It concludes that virtual reality promotes an engaging learning experience, allowing for practice and reflection without disrupting real-world activities. However, current designs face challenges in terms of usability, realism, and fostering social interaction. The paper also emphasizes the need for future research to focus on the in-VR experience, rather than solely on post-VR evaluations.","Advanced digital learning platforms, exemplified by virtual reality, have been hailed as the future of educational environments. Despite this, there is a scarcity of research exploring the design, implementation, and analysis of immersive technologies in collaborative educational settings. This study presents a comprehensive review of empirical research on the utilization of immersive virtual reality in collaborative learning within academic and professional contexts. A total of 11 studies were categorized and analyzed through a narrative synthesis, highlighting the pedagogical principles underlying learning design, virtual environment development, and collaborative activities facilitated by the technology. The findings indicate that collaborative learning in virtual reality can be characterized as a shared, immersive experience where learners collectively address a common objective or challenge. This understanding underscores the need to develop technologies, environments, and activities that promote participation, social interaction, and collaborative learning processes. Based on this conceptual framework, we offer a set of guidelines for designing collaborative learning experiences in immersive virtual reality. The study concludes that virtual reality-based collaborative learning creates a simulated environment for practice and reflection, enhancing engagement without compromising real-world practices. However, current designs face challenges related to usability, realism, and facilitating social interaction. Furthermore, the study identifies a need for future research to investigate the dynamics within virtual reality, moving beyond post-experience evaluations.","Cutting-edge technologies like virtual reality (VR) have been hailed as the future of digital education. However, research on designing, implementing, and evaluating VR for collaborative learning remains scarce. This review examines empirical studies on VR-supported collaborative learning in educational and professional contexts. Eleven studies were analyzed, revealing pedagogical approaches, VR environment design, and collaborative activities. Findings suggest that collaborative VR learning involves shared experiences within immersive virtual spaces, where learners work together on common goals or challenges. This necessitates designing technologies, environments, and activities that promote participation, social interaction, and collaborative processes. Based on this conceptualization, the paper offers recommendations for designing collaborative VR learning experiences. It concludes that VR provides a safe space for practice and reflection, engaging learners without real-world consequences. However, current designs face challenges related to usability, realism, and fostering social interaction. The paper emphasizes the need for research that delves into the intricacies of VR interactions, going beyond post-experience evaluations."
118,AI ethics and ordoliberalism 2.0: towards a ‘Digital Bill of Rights’,"This article analyzes AI ethics from a distinct business ethics perspective, i.e., ‘ordoliberalism 2.0.’ It argues that the ongoing discourse on (generative) AI relies too much on corporate self-regulation and voluntary codes of conduct and thus lacks adequate governance mechanisms. To address these issues, the paper suggests not only introducing hard-law legislation with a more effective oversight structure but also merging already existing AI guidelines with an ordoliberal-inspired regulatory and competition policy. However, this link between AI ethics, regulation, and antitrust is not yet adequately discussed in the academic literature and beyond. The paper thus closes a significant gap in the academic literature and adds to the predominantly legal-political and philosophical discourse on AI governance. The paper’s research questions and goals are twofold: first, it identifies ordoliberal-inspired AI ethics principles that could serve as the foundation for a ‘digital bill of rights.’ Second, it shows how those principles could be implemented at the macro level with the help of ordoliberal competition and regulatory policy.","This paper examines AI ethics from a unique perspective rooted in business ethics, specifically 'ordoliberalism 2.0.' It contends that the current debate on (generative) AI overly depends on corporate self-regulation and voluntary guidelines, thereby lacking proper governance mechanisms. To remedy this, the paper proposes enacting stringent legislation with a more effective oversight system, in addition to integrating existing AI guidelines with an ordoliberal-based regulatory and competitive policy. However, the connection between AI ethics, regulation, and antitrust issues has not been sufficiently explored in scholarly literature or beyond. By doing so, the paper fills a crucial gap in academic research and contributes to the predominantly legal-political and philosophical discussions on AI governance. The research aims are twofold: first, to identify ordoliberal-inspired AI ethics principles that could form the basis for a 'digital bill of rights'; second, to demonstrate how these principles could be applied on a macro level through ordoliberal competition and regulatory policies.","This study examines the ethics of artificial intelligence through the lens of a unique business ethics framework, namely, 'ordoliberalism 2.0.' It contends that the current conversation surrounding AI, including generative AI, is overly reliant on self-policing by corporations and voluntary codes of conduct, resulting in inadequate governance structures. To rectify this, the paper proposes the introduction of stringent legislation with enhanced oversight mechanisms, as well as the integration of existing AI guidelines with an ordoliberal-influenced regulatory and competition policy. Notably, the connection between AI ethics, regulation, and antitrust policy remains underexplored in both academic and non-academic circles. This paper fills a significant knowledge gap in the literature, contributing to the predominantly legal, political, and philosophical discussions on AI governance. The research objectives are twofold: firstly, to establish ordoliberal-inspired AI ethics principles that could form the basis of a 'digital bill of rights,' and secondly, to demonstrate how these principles can be implemented on a large scale through ordoliberal competition and regulatory policies.","This study explores the ethical dimensions of artificial intelligence through the lens of 'ordoliberalism 2.0', a unique business ethics framework. It posits that the current dialogue surrounding (generative) AI overemphasizes self-regulation by corporations and voluntary ethical guidelines, leading to insufficient governance structures. To rectify this, the paper proposes not only the establishment of legally binding regulations with robust oversight mechanisms but also the integration of existing AI guidelines into a regulatory and competition policy informed by ordoliberal principles. This crucial connection between AI ethics, regulation, and antitrust remains underexplored in both scholarly and broader discussions. Consequently, this work addresses a significant void in the academic literature, enriching the predominantly legal, political, and philosophical discourse on AI governance. The paper's research objectives are twofold: firstly, it identifies ordoliberal-inspired ethical principles for AI that could underpin a 'digital bill of rights'. Secondly, it demonstrates how these principles can be applied at a systemic level through ordoliberal competition and regulatory policy."
185,Breaking the chains of traditional finance: A taxonomy of decentralized finance business models,"Recently, the looming bankruptcy of Credit Suisse, which ultimately led to its merger with UBS, has intensified the discussion surrounding the need for more transparent and democratic financial markets. Decentralized finance (DeFi) represents a departure from traditional financial intermediation by leveraging blockchain technology and smart contracts. Despite its growing importance, limited research has been conducted on the business models of DeFi services. This study aims to address this gap by examining the business models of various DeFi services, identifying key drivers of innovation, differentiation, and value creation. Using a rigorous taxonomy development framework, we identify 12 dimensions and 47 characteristics that operationalize business models in DeFi. Our findings contribute to a better understanding of the transformation of financial services through blockchain technology and provide valuable insights for DeFi entrepreneurs, investors, and policymakers.","The recent threat of Credit Suisse’s insolvency, which culminated in its merger with UBS, has heightened the debate over the necessity for more open and democratic financial markets. Decentralized finance (DeFi) stands apart from conventional financial intermediation by utilizing blockchain technology and smart contracts. Although its significance is increasing, there is a lack of extensive research on DeFi business models. This study seeks to fill this void by analyzing the business models of different DeFi services, pinpointing major drivers of innovation, differentiation, and value creation. Through a comprehensive taxonomy development framework, we identify 12 dimensions and 47 characteristics essential for DeFi business models. Our results enhance the understanding of financial service transformation via blockchain technology and offer critical insights for DeFi entrepreneurs, investors, and policymakers.","The recent collapse of Credit Suisse, culminating in its acquisition by UBS, has sparked a renewed debate about the necessity for more open and participatory financial systems. DeFi, which harnesses blockchain and smart contracts to revolutionize traditional financial brokerage, has emerged as a promising alternative. However, despite its increasing significance, there is a dearth of research on the revenue streams and value propositions of DeFi platforms. This investigation seeks to bridge this knowledge gap by analyzing the business strategies of diverse DeFi providers, pinpointing the primary catalysts of innovation, distinctiveness, and value generation. Through the application of a systematic typology development framework, we uncover 12 key facets and 47 attributes that define DeFi business models. Our discoveries offer fresh insights into the blockchain-driven transformation of financial services, providing actionable guidance for DeFi startups, investors, and regulatory bodies.","The recent near-collapse and subsequent absorption of Credit Suisse by UBS has ignited debate on the imperative for greater clarity and inclusivity within financial systems. As a disruptive force in finance, DeFi bypasses conventional intermediaries, utilizing blockchain technology and automated contracts. Despite its burgeoning relevance, there's a dearth of research on DeFi's operational models. This investigation seeks to rectify this by scrutinizing the business structures of various DeFi services, pinpointing the core factors driving innovation, distinctiveness, and value generation. Employing a robust taxonomic framework, we define 12 dimensions and 47 attributes that operationalize DeFi business models. Our discoveries enhance comprehension of blockchain-powered financial service transformation, offering valuable insights to DeFi stakeholders – entrepreneurs, investors, and policymakers alike."
20,Immersive virtual reality in orthopaedics—a narrative review,"This narrative review explores the applications and benefits of immersive virtual reality (VR) in orthopaedics, with a focus on surgical training, patient functional recovery, and pain management.MethodsThe review examines existing literature and research studies on immersive VR in orthopaedics, analyzing both experimental and clinical studies.ResultsImmersive VR provides a realistic simulation environment for orthopaedic surgery training, enhancing surgical skills, reducing errors, and improving overall performance. In post-surgical recovery and rehabilitation, immersive VR environments can facilitate motor learning and functional recovery through virtual embodiment, motor imagery during action observation, and virtual training. Additionally VR-based functional recovery programs can improve patient adherence and outcomes. Moreover, VR has the potential to revolutionize pain management, offering a non-invasive, drug-free alternative. Virtual reality analgesia acts by a variety of means including engagement and diverting patients’ attention, anxiety reduction, and specific virtual-body transformations.ConclusionImmersive virtual reality holds significant promise in orthopaedics, demonstrating potential for improved surgical training, patient functional recovery, and pain management but further research is needed to fully exploit the benefits of VR technology in these areas.","This narrative review investigates the uses and advantages of immersive virtual reality (VR) within the field of orthopaedics, concentrating on training for surgeries, functional recovery for patients, and the management of pain. The review analyzes existing literature and research concerning immersive VR in orthopaedics, including both experimental and clinical studies. Immersive VR delivers a realistic simulation environment that enhances orthopaedic surgical training by improving surgical skills, minimizing errors, and boosting overall performance. For post-surgical rehabilitation, immersive VR environments aid motor learning and functional recovery through virtual embodiment, motor imagery during action observation, and virtual training, leading to better patient adherence and outcomes. Additionally, VR offers a revolutionary approach to pain management, providing a non-invasive, drug-free alternative. VR-based pain relief operates through mechanisms such as patient engagement, distraction, anxiety reduction, and specific virtual-body transformations. In conclusion, immersive VR shows great promise in orthopaedics, demonstrating potential advancements in surgical training, patient recovery, and pain management, though additional research is needed to fully realize VR’s capabilities in these areas.","This comprehensive analysis delves into the multifaceted advantages and uses of immersive virtual reality technology in the field of orthopaedics, with a particular emphasis on its role in refining surgical proficiency, expediting patient rehabilitation, and alleviating pain. The study synthesizes findings from existing research and clinical trials to assess the efficacy of immersive VR in orthopaedics. The results indicate that immersive VR simulations can significantly enhance surgical competence by providing a realistic and interactive environment, thereby minimizing errors and optimizing overall performance. Furthermore, VR-based rehabilitation programs can facilitate motor skill acquisition, functional recovery, and patient compliance through virtual embodiment, motor imagery, and simulated training exercises. Additionally, VR technology has the potential to transform pain management by offering a non-pharmacological, non-invasive alternative that leverages distraction, anxiety reduction, and virtual-body transformations to mitigate pain perception. In conclusion, immersive virtual reality technology exhibits considerable potential in orthopaedics, with promising applications in surgical training, patient rehabilitation, and pain management, although further investigation is necessary to fully harness its benefits.","This comprehensive analysis delves into the diverse applications and advantages of immersive virtual reality (VR) within the field of orthopaedics, emphasizing its utility in surgical education, patient rehabilitation, and pain alleviation. By meticulously examining existing literature and research, encompassing both experimental and clinical investigations, the review illuminates the profound impact of immersive VR. The technology offers a remarkably realistic simulated setting for orthopaedic surgical training, demonstrably enhancing surgical proficiency, diminishing errors, and elevating overall performance. In the realm of post-surgical recovery and rehabilitation, immersive VR environments serve as catalysts for motor learning and functional restoration through virtual embodiment, motor imagery evoked during action observation, and virtual training regimens. Notably, VR-based functional recovery programs exhibit the potential to bolster patient adherence and optimize outcomes. Furthermore, VR emerges as a revolutionary force in pain management, presenting a non-invasive, drug-free alternative to traditional methods. Virtual reality analgesia operates through a multifaceted approach, encompassing patient engagement and attention diversion, anxiety mitigation, and specific virtual-body transformations. While immersive virtual reality undeniably holds immense promise in orthopaedics, underscoring its potential to revolutionize surgical training, patient functional recovery, and pain management, further research is imperative to fully harness the benefits of this transformative technology."
79,Blurring the lines: how AI is redefining artistic ownership and copyright,"The rapid advancement of AI technology has brought about significant changes across various industries, including the realm of art and creative endeavors. Presently, AI demonstrates the capability to produce high-quality works that closely resemble those crafted by humans. However, amidst these advancements, ownership rights remain ambiguous, given that existing copyright laws do not explicitly confer copyright protection to works not originating from human creators. The primary objective of this research is to present a spectrum of opinions and perspectives on a pivotal question: should the ownership of AI-generated works belong to individuals or AI entities? Simultaneously, the study aims to scrutinize the suitability of the existing copyright laws in addressing the complexities of AI-generated works. Additionally, this research paper endeavors to pinpoint the necessary adjustments and enhancements required within the current legal framework to harmonize with the contemporary landscape. To attain these objectives, two surveys were conducted. The first, a quantitative survey, involved a sample size of 100 participants, while the second, a qualitative survey, featured professionals within the creative field and legal experts. Each group comprised five individuals. The research findings gleaned from these surveys strongly suggest that AI-generated works exhibit quality on par with human creations and, therefore, merit classification as human works. Furthermore, there is a compelling argument that these works should be afforded copyright protection to provide clarity regarding their utilization by other individuals. Consequently, the prevailing copyright laws should undergo revision and amendment to reflect the demands of the modern era, establishing precise regulations tailored for AI-generated works.","The swift progress of AI technology has introduced substantial changes in numerous sectors, including the domain of art and creative activities. Nowadays, AI is capable of generating high-quality works that closely mimic those created by humans. Yet, alongside these developments, the question of ownership remains unclear, as current copyright laws do not specifically grant protection to works not made by human creators. This research primarily aims to explore various opinions and perspectives on a crucial issue: whether ownership of AI-created works should be ascribed to individuals or AI entities. Additionally, the study examines the adequacy of existing copyright laws in handling the intricacies of AI-generated creations. Furthermore, this paper seeks to identify the necessary revisions and improvements within the current legal framework to align with contemporary realities. To achieve these aims, two surveys were conducted. The first was a quantitative survey with a sample size of 100 participants, and the second was a qualitative survey involving five professionals from the creative field and five legal experts. Results from these surveys strongly indicate that AI-generated works are of comparable quality to human-made creations and therefore should be classified as such. Additionally, there is a strong case for granting copyright protection to these works to clarify their use by others. Consequently, current copyright laws need to be revised and updated to reflect modern needs, establishing explicit regulations for AI-generated works.","The exponential growth of artificial intelligence has triggered a paradigm shift across diverse sectors, notably in the realm of artistic expression and innovation. Currently, AI has demonstrated the ability to generate exceptional pieces that are virtually indistinguishable from those produced by human creatives. Nevertheless, the issue of ownership remains shrouded in uncertainty, as existing intellectual property laws fail to explicitly extend copyright protection to works not originating from human authors. This study seeks to present a kaleidoscope of viewpoints on a crucial question: should the proprietary rights of AI-generated works be vested in individuals or AI systems? Concurrently, it aims to examine the efficacy of current copyright laws in addressing the intricacies of AI-generated works. Furthermore, this research endeavors to identify the necessary refinements and augmentations required within the existing legal framework to align with the contemporary landscape. To achieve these objectives, two surveys were conducted: a quantitative survey involving 100 participants and a qualitative survey featuring five creative professionals and five legal experts. The findings of this research strongly indicate that AI-generated works exhibit a level of excellence comparable to human creations, thereby warranting their classification as human works. Moreover, a compelling case can be made for affording these works copyright protection to provide clarity regarding their utilization by other individuals. As a result, the prevailing copyright laws must undergo revision and amendment to reflect the demands of the modern era, establishing precise regulations tailored to AI-generated works.","The swift progress of artificial intelligence has revolutionized various sectors, notably the artistic and creative domains. AI now possesses the capacity to generate impressive outputs that mirror human creations. However, despite these developments, ownership rights remain unclear, as current copyright laws lack explicit provisions for works not originating from human creators. This study explores diverse viewpoints on a crucial question: should ownership of AI-produced works lie with individuals or AI entities? It also analyzes the adequacy of existing copyright laws in navigating the intricacies of AI-generated creations, proposing modifications to align the legal framework with contemporary realities. Two surveys were employed: a quantitative survey with 100 participants and a qualitative survey involving five creative professionals and five legal experts. Findings highlight the high quality of AI-generated works, suggesting they should be categorized as human creations and granted copyright protection to clarify their usage by others. Consequently, current copyright laws necessitate revision and amendment to meet modern demands, establishing specific regulations for AI-generated works."
101,A global conservation basic income to safeguard biodiversity,"Biodiversity conservation supporting a global sustainability transformation must be inclusive, equitable, just and embrace plural values. The conservation basic income (CBI), a proposed unconditional cash transfer to individuals residing in important conservation areas, is a potentially powerful mechanism for facilitating this radical shift in conservation. This analysis provides comprehensive projections for potential gross costs of global CBI using spatial analyses of three plausible future conservation scenarios. Gross costs vary widely, depending on the areas and populations included, from US$351 billion to US$6.73 trillion annually. A US$5.50 per day CBI in existing protected areas in low- and middle-income countries would cost US$478 billion annually. These costs are large compared with current government conservation spending (~US$133 billion in 2020) but represent a potentially sensible investment in safeguarding incalculable social and natural values and the estimated US$44 trillion in global economic production dependent on nature.","To support a global shift toward sustainable development, biodiversity conservation must be inclusive, equitable, just, and recognize diverse values. The Conservation Basic Income (CBI), an unconditional cash transfer to individuals living in key conservation regions, is proposed as a potent strategy for driving this transformative change in conservation efforts. This study offers detailed cost projections for implementing a global CBI, derived from spatial analyses of three potential future conservation scenarios. The estimated gross costs vary significantly, ranging from $351 billion to $6.73 trillion annually, based on the areas and populations covered. Implementing a $5.50 per day CBI in existing protected areas within low- and middle-income countries would amount to $478 billion annually. Although these figures are substantially higher compared to current governmental conservation expenditures (around $133 billion in 2020), they could be considered a prudent investment in preserving invaluable social and natural assets, as well as the roughly $44 trillion in global economic output that relies on nature.","""Fostering a sustainable future through biodiversity conservation demands an approach that is all-encompassing, fair, and respectful of diverse perspectives. The concept of a conservation basic income (CBI), which involves providing unconditional financial support to individuals living in crucial conservation zones, has the potential to drive a profound paradigm shift in conservation efforts. This study presents a thorough examination of the projected annual expenditures for a global CBI initiative, exploring three plausible future conservation scenarios through spatial analysis. The estimated costs vary significantly, ranging from $351 billion to $6.73 trillion, depending on the areas and populations targeted. Implementing a $5.50 daily CBI in existing protected areas within low- and middle-income countries would require an annual investment of $478 billion. While these figures are substantial compared to current government conservation expenditures (approximately $133 billion in 2020), they represent a potentially wise investment in preserving invaluable social and natural assets, as well as the estimated $44 trillion in global economic output reliant on natural resources.""","To achieve a truly sustainable future, biodiversity protection efforts must prioritize inclusivity, fairness, and the recognition of diverse values. The conservation basic income (CBI), a novel concept involving direct cash transfers to residents of critical conservation zones, could be instrumental in driving this fundamental change in our approach to conservation. This study offers a thorough cost analysis of implementing CBI globally, utilizing spatial modeling across three plausible future conservation scenarios. The projected gross costs exhibit significant variability based on the targeted areas and populations, ranging from US$351 billion to US$6.73 trillion per year. For example, providing a daily CBI of US$5.50 within existing protected areas in low- and middle-income nations would amount to US$478 billion annually. While these figures are substantial compared to current governmental conservation expenditures (approximately US$133 billion in 2020), they represent a potentially prudent investment in preserving the invaluable social and ecological benefits derived from nature, including the estimated US$44 trillion in global economic output reliant on natural resources."
123,Unveiling public perception of AI ethics: an exploration on Wikipedia data,"Artificial Intelligence (AI) technologies have exposed more and more ethical issues while providing services to people. It is challenging for people to realize the occurrence of AI ethical issues in most cases. The lower the public awareness, the more difficult it is to address AI ethical issues. Many previous studies have explored public reactions and opinions on AI ethical issues through questionnaires and social media platforms like Twitter. However, these approaches primarily focus on categorizing popular topics and sentiments, overlooking the public’s potential lack of knowledge underlying these issues. Few studies revealed the holistic knowledge structure of AI ethical topics and the relations among the subtopics. As the world’s largest online encyclopedia, Wikipedia encourages people to jointly contribute and share their knowledge by adding new topics and following a well-accepted hierarchical structure. Through public viewing and editing, Wikipedia serves as a proxy for knowledge transmission. This study aims to analyze how the public comprehend the body of knowledge of AI ethics. We adopted the community detection approach to identify the hierarchical community of the AI ethical topics, and further extracted the AI ethics-related entities, which are proper nouns, organizations, and persons. The findings reveal that the primary topics at the top-level community, most pertinent to AI ethics, predominantly revolve around knowledge-based and ethical issues. Examples include transitions from Information Theory to Internet Copyright Infringement. In summary, this study contributes to three points, (1) to present the holistic knowledge structure of AI ethics, (2) to evaluate and improve the existing body of knowledge of AI ethics, (3) to enhance public perception of AI ethics to mitigate the risks associated with AI technologies.","Artificial Intelligence (AI) technologies have increasingly highlighted ethical concerns while delivering services to individuals. People often find it difficult to recognize the occurrence of these AI ethical dilemmas. Generally, the lower the level of public awareness, the more challenging it becomes to resolve AI ethical issues. Numerous prior studies have examined public reactions and opinions on AI ethics through surveys and social media platforms like Twitter. Nonetheless, these methods mainly emphasize identifying popular topics and sentiments, neglecting the public’s potential ignorance of these issues. Few studies have uncovered the comprehensive knowledge structure of AI ethical topics and the relationships among subtopics. As the world’s largest online encyclopedia, Wikipedia encourages collective contributions and knowledge sharing by adding new topics and adhering to a widely accepted hierarchical structure. Through public viewing and editing, Wikipedia acts as a proxy for knowledge dissemination. This study aims to examine how the public understands the corpus of AI ethical knowledge. We employed a community detection approach to identify the hierarchical structure of AI ethical topics and further identified AI ethics-related entities, such as proper nouns, organizations, and individuals. The findings indicate that the primary topics within the top-level community, most related to AI ethics, mainly focus on knowledge-based and ethical issues. Examples include shifts from Information Theory to Internet Copyright Infringement. In conclusion, this study contributes to three areas: (1) presenting the comprehensive knowledge structure of AI ethics, (2) evaluating and enhancing the existing body of knowledge on AI ethics, and (3) improving public understanding of AI ethics to mitigate risks associated with AI technologies.","""As AI technologies increasingly interact with humans, they have unveiled a multitude of ethical concerns. However, the general public often remains oblivious to these issues, and the lack of awareness hinders efforts to address them. Previous research has probed public sentiments and opinions on AI ethics through surveys and social media, but these studies have primarily focused on categorizing popular themes and emotions, neglecting the underlying knowledge gaps. Few investigations have delved into the comprehensive knowledge framework of AI ethics and the relationships between subtopics. Leveraging Wikipedia's collaborative knowledge-sharing platform, this study seeks to understand how the public grasps the complex body of knowledge surrounding AI ethics. By employing community detection methods, we identified a hierarchical network of AI ethical topics and extracted relevant entities, including proper nouns, organizations, and individuals. Our findings indicate that the core topics at the highest level of the community, most closely tied to AI ethics, predominantly center around knowledge-centric and ethical concerns, such as shifts from Information Theory to Internet Copyright Infringement. Ultimately, this study makes three key contributions: (1) mapping the comprehensive knowledge structure of AI ethics, (2) assessing and refining the existing body of knowledge on AI ethics, and (3) enhancing public understanding of AI ethics to mitigate the risks associated with AI technologies.""","The increasing integration of Artificial Intelligence (AI) into various services has brought about a surge in ethical concerns. Recognizing these ethical dilemmas often proves difficult for the general public, and this lack of awareness further complicates efforts to address them. While past research has investigated public sentiment on AI ethics through surveys and platforms like Twitter, these methods primarily focus on identifying popular themes and opinions, neglecting the potential knowledge gaps underpinning these issues. Consequently, there is limited understanding of the comprehensive structure of AI ethical topics and their interrelations. Leveraging Wikipedia's collaborative nature and its established hierarchical framework, this study explores public understanding of AI ethics. Employing community detection techniques, the research identifies the hierarchical arrangement of AI ethical topics and extracts relevant entities such as specific terms, institutions, and individuals. The results highlight that top-level topics, most closely linked to AI ethics, primarily center around knowledge-based and ethical concerns, illustrating a transition from theoretical concepts like Information Theory to practical dilemmas like Internet Copyright Infringement. In essence, this study makes three key contributions: it provides a comprehensive representation of the AI ethical knowledge structure, offers a means to assess and enhance the existing knowledge base on AI ethics, and ultimately seeks to raise public awareness to mitigate potential risks associated with AI technologies."
85,Artistic representations of data can help bridge the US political divide over climate change,"Visual art has been used to revamp the portrayal of climate change with the aims of engaging emotions and expanding nonexperts’ psychological capacity to perceive its relevance. However, empirical evidence supporting the effectiveness of artistic representation of data as a tool for public communication is lacking. Using controlled experiments with two national samples of U.S. adults (total N = 671), here we found that artistic visualizations elicited stronger positive emotions than informationally equivalent data graphs but did not differ in their perceived credibility or effectiveness as visual aids for learning. When used to prompt individual reflection, artistic visualizations appeared to mitigate the political division in viewers’ perceived relevance of climate change that could otherwise be exacerbated by exposure to data graphs.","Art has been employed to reshape the depiction of climate change with the goal of stirring emotions and enhancing the general public’s ability to recognize its significance. Nevertheless, there is a scarcity of empirical data confirming the efficacy of artistic data representation as a method for public outreach. Through controlled experiments involving two national samples of U.S. adults (total N = 671), we discovered that artistic visualizations evoked stronger positive emotions than data graphs with similar information content, although they did not differ in terms of perceived credibility or educational usefulness. When used to encourage personal reflection, artistic visualizations seemed to reduce the political divide in viewers' perceived importance of climate change, a divide that might otherwise be worsened by data graphs.","The representation of climate change through visual art has been employed to reinvigorate its depiction, seeking to evoke emotional responses and enhance the cognitive ability of non-specialists to recognize its significance. Nevertheless, concrete proof substantiating the efficacy of artistic data visualization as a means of public outreach remains scarce. Our study, involving controlled experiments with two nationally representative samples of American adults (N = 671), reveals that artistic renderings triggered more intense positive emotional reactions compared to data graphs conveying equivalent information, although no differences were observed in terms of perceived trustworthiness or instructional value. Furthermore, when utilized to stimulate personal introspection, artistic visualizations appeared to reduce the partisan polarization in viewers' perceptions of climate change relevance that might otherwise be intensified by exposure to data graphs.","To make climate change more relatable and emotionally impactful, visual art has been employed. However, there is limited empirical support for the efficacy of artistic data representation as a public communication tool. Controlled experiments with two nationally representative samples of U.S. adults (N = 671) revealed that while artistic visualizations evoked stronger positive emotions compared to data graphs conveying the same information, they did not exhibit differences in perceived credibility or effectiveness as learning aids. Interestingly, prompting personal reflection with artistic visualizations appeared to lessen the political divide in perceived climate change relevance, a divide that data graph exposure might otherwise widen."
60,aiWATERS: an artificial intelligence framework for the water sector,"The presence of Artificial Intelligence (AI) and Machine Learning (ML) applications has led to its widespread adoption across diverse domains. AI is making its way into industry, beyond research and academia. Concurrently, the water sector is undergoing a digital transformation. Water utilities in the United States are at different stages in their journey of digital transformation, and the decision makers in water sector, who are non-expert stakeholders in AI applications, need to better understand this technology to make informed decisions. While AI has numerous benefits to offer, there are also many challenges related to data, model development, knowledge integration and ethical concerns that should be considered before implementing it for real world applications. Civil engineering is a licensed profession where critical decision making is involved. Therefore, trust in any decision support technology is critical for its acceptance in real-world applications. Therefore, this research proposes a framework called aiWATERS (Artificial Intelligence for the Water Sector) which can serve as a guide for the water utilities to successfully implement AI in their system. Based on this framework, we conduct pilot interviews and surveys with various small, medium, and large water utilities in the United States (US) to capture their current state of AI implementation and identify the challenges faced by them. The research findings reveal that most of the water utilities in the United States are at an early stage of implementing AI as they face concerns regarding the black box nature, trustworthiness, and sustainability of AI technology in their system. The aiWATERS framework is intended to help the utilities navigate through these issues in their journey of digital transformation.","The integration of Artificial Intelligence (AI) and Machine Learning (ML) applications has fostered their extensive use in various sectors. AI is branching out from research and academia to make significant inroads into industry. Simultaneously, the water sector is experiencing a digital revolution. Water utilities in the United States are progressing at different rates in their digital transformation journey. Decision-makers in the water sector, who typically lack expertise in AI, must gain a deeper understanding of this technology to make informed choices. While AI offers numerous advantages, it also presents challenges related to data, model development, knowledge integration, and ethical issues that must be considered prior to real-world deployment. Civil engineering, a regulated profession involving critical decision-making, necessitates a high level of trust in decision support technologies for their acceptance in practical applications. This study proposes the aiWATERS (Artificial Intelligence for the Water Sector) framework as a guide for water utilities to effectively implement AI in their systems. We conducted pilot interviews and surveys with various small, medium, and large water utilities across the US to assess their current AI implementation status and identify challenges. Findings indicate that most US water utilities are in the early stages of AI adoption, with concerns about the opaque nature, reliability, and sustainability of AI technology. The aiWATERS framework aims to help these utilities address these issues during their digital transformation journey.","The proliferation of Artificial Intelligence (AI) and Machine Learning (ML) solutions has triggered a far-reaching uptake across disparate industries, with AI transcending its traditional bastion in research and academia to permeate the industrial landscape. Concurrently, the water industry is undergoing a profound digital metamorphosis, with water utilities in the United States at varying stages of this transformation. However, non-technical stakeholders in the water sector, who are key decision-makers, require a deeper understanding of AI to make informed choices. While AI offers numerous advantages, its implementation is fraught with challenges related to data quality, model development, knowledge integration, and ethical considerations. In the realm of civil engineering, where critical decision-making is paramount, trust in decision-support technologies is crucial for their real-world adoption. This research proposes the aiWATERS framework, a guiding paradigm for water utilities to successfully integrate AI into their operations. Through pilot interviews and surveys with diverse water utilities in the United States, this study captures their current AI implementation status and identifies the hurdles they face. The findings reveal that most US water utilities are in the nascent stages of AI adoption, grappling with concerns about AI's opacity, reliability, and long-term viability. The aiWATERS framework aims to facilitate the utilities' navigation of these challenges as they embark on their digital transformation journey.","The burgeoning influence of Artificial Intelligence (AI) and Machine Learning (ML) has spurred their integration across a myriad of fields, extending beyond the confines of research and academia into practical industry applications. Simultaneously, the water sector is experiencing a significant digital shift, with US water utilities at varying points in this evolution. Decision-makers within the water sector, often lacking specialized AI knowledge, require a deeper understanding of this technology to make well-informed choices. Although AI offers substantial advantages, challenges pertaining to data, model creation, knowledge assimilation, and ethical considerations necessitate careful evaluation before real-world implementation. Given the critical decision-making inherent in the licensed profession of civil engineering, trust in any decision support tool is paramount for its real-world acceptance. To address this, the aiWATERS (Artificial Intelligence for the Water Sector) framework is proposed as guidance for water utilities seeking successful AI integration. Pilot interviews and surveys involving diverse US water utilities, ranging in size, were conducted within this framework to assess the current state of AI implementation and pinpoint existing challenges. The findings highlight that most US water utilities are in the nascent stages of AI adoption, grappling with concerns about AI's opacity, trustworthiness, and long-term viability within their systems. The aiWATERS framework aims to assist utilities in navigating these complexities as they progress through their digital transformation."
175,Green digital finance and technology diffusion,"Technology diffusion is essential for sustainable development. However, traditional financing faces structural problems in supporting technological innovation and diffusion. Fortunately, greening and digitalisation have become important directions for the financial development. It’s well worth examining whether green digital finance would promote the diffusion of technology among regions. Therefore, this study explores the causal relationship between green digital finance and technology diffusion using the panel data of 35,532 ‘citing city-cited city’ pair observations from 2002 to 2015. Considering different spatial spillover effects of administrative boundaries and geographic distances, it applies and extends the classic spatial Dubin model with a dual-weighted boundary and distance. The main findings of this study are as follows. First, green digital finance shows significant and considerable spatial stimulus effects, particularly in terms of spatial diffusion across cities. A slight cumulative phenomenon is revealed for different technology diffusion durations. Green digital credit, investment, and support have better stimulating effects on technology diffusion. Second, digital economy and market integration are effective mechanism pathways during this stimulating process of green digital finance on technology diffusion. Digitalisation innovation has a better mechanism effect than others. Third, considering the direction of technology diffusion, the diffusion effect from the east-region cities was better than that from the central-west-region cities.","The spread of technology is crucial for sustainable progress. Nonetheless, conventional funding methods struggle with inherent issues in backing technological innovation and dissemination. Thankfully, the trends of greening and digitalisation are becoming vital for financial growth. Investigating whether green digital finance enhances the regional spread of technology is vital. This research examines the link between green digital finance and technology dissemination, using panel data from 35,532 city pairs observed between 2002 and 2015. By accounting for various spatial spillover effects of administrative boundaries and geographic distances, it extends the traditional spatial Dubin model through dual-weighted boundary and distance measures. Key discoveries include the following: firstly, green digital finance exhibits significant and substantial effects on spatial diffusion, especially among cities, with a minor cumulative phenomenon observed over different technology dissemination periods. Green digital credit, investment, and support have more pronounced effects on technology spread. Secondly, the digital economy and market integration act as effective pathways in this process, with digital innovation having a superior mechanism effect. Lastly, in terms of technology spread direction, eastern cities show a better diffusion effect compared to central-western cities.","The dissemination of technology is a crucial component of sustainable development, but conventional financing methods are hindered by inherent limitations in fostering innovation and adoption. Fortunately, the convergence of environmental sustainability and digital advancements has emerged as a promising trajectory for financial growth. It is thus imperative to investigate whether the integration of green and digital finance can facilitate the widespread adoption of technology across regions. This research endeavors to elucidate the causal link between green digital finance and technology diffusion, leveraging a comprehensive panel dataset of 35,532 'citing city-cited city' pair observations spanning 2002-2015. By accounting for the disparate spatial spillover effects of administrative boundaries and geographic distances, this study adapts and extends the classic spatial Dubin model with a dual-weighted boundary and distance approach. The key findings of this study reveal that green digital finance exhibits substantial and significant spatial stimulus effects, particularly in terms of inter-city technology diffusion, with a moderate cumulative phenomenon observed across varying technology diffusion periods. Green digital credit, investment, and support demonstrate superior stimulating effects on technology diffusion. Furthermore, the digital economy and market integration emerge as effective mechanisms facilitating the stimulating impact of green digital finance on technology diffusion, with digital innovation exhibiting a more pronounced mechanism effect. Finally, the direction of technology diffusion indicates that cities in the eastern region exhibit a more pronounced diffusion effect compared to those in the central-western region.","The spread of technology is crucial for achieving lasting progress, but conventional funding methods struggle to adequately support innovation and its adoption. The rise of environmentally conscious practices and digital advancements in finance offers a promising avenue. This research investigates if green digital finance can effectively bridge the technology gap between regions. Utilizing data from over 35,000 city-to-city technology transfer instances between 2002 and 2015, the study employs a refined spatial model accounting for both administrative and geographical distances. The findings highlight the significant positive impact of green digital finance, particularly its role in facilitating technology spread across cities. This effect strengthens over time, with green digital credit, investment, and support showing the most pronounced influence. Furthermore, the study identifies a digital economy and market integration as key facilitators in this process, with digital innovation demonstrating the strongest mediating role. Finally, the analysis reveals that technology originating from eastern cities exhibits a stronger diffusion effect compared to that from central and western regions."
34,IoT-based eHealth using blockchain technology: a survey,"The eHealth sector has witnessed significant growth due to technological advancements, facilitating care delivery in patients' homes and moving away from traditional hospital settings. Blockchain and the Internet of Things (IoT) play pivotal roles in enhancing healthcare services, offering features such as remote patient monitoring, streamlined electronic medical record (EMR) management, drug traceability, and effective disease control, particularly during events like the COVID-19 pandemic. The growing utilization of IoT devices brings about security challenges, including concerns related to data integrity and device authentication. This paper proposes the integration of blockchain technology as a robust solution. Leveraging its decentralized and tamper-resistant features, blockchain establishes trust among diverse IoT devices, ensuring the integrity of IoT data. Additionally, smart contracts enhance device authentication, fortifying overall security by addressing vulnerabilities associated with centralization. Regarding the management of eHealth, this survey begins with an overview of the industry, highlighting IoT-related challenges in healthcare. It explores various IoT applications in eHealth and discusses how blockchain can effectively address obstacles in healthcare management through IoT. Notably, the paper provides insights into examining consensus algorithm parameters within blockchain systems, clarifying the methodology used to assess and optimize these critical components. The survey extends to a thorough review of existing research on integrating blockchain-based IoT in eHealth. Finally, it presents an overview of challenges and potential solutions for implementing blockchain-based IoT in the eHealth sector. This comprehensive survey aims to empower stakeholders by providing insights to enhance patient care in this dynamic and evolving field.","The eHealth industry has experienced considerable expansion driven by technological progress, enabling care provision within patients' homes rather than conventional hospital environments. Blockchain and the Internet of Things (IoT) are crucial in improving healthcare services, offering capabilities such as remote patient monitoring, efficient management of electronic medical records (EMRs), drug traceability, and effective disease control, especially during events like the COVID-19 pandemic. However, the increasing adoption of IoT devices brings about security issues, including concerns regarding data integrity and device authentication. This paper suggests incorporating blockchain technology as a robust solution. Utilizing its decentralized and tamper-proof characteristics, blockchain fosters trust among various IoT devices, ensuring the integrity of IoT data. Moreover, smart contracts improve device authentication, enhancing overall security by mitigating centralization vulnerabilities. This survey begins with an overview of the eHealth industry, highlighting IoT-related healthcare challenges. It examines several IoT applications in eHealth and discusses how blockchain can effectively address hurdles in healthcare management through IoT. The paper also offers insights into analyzing consensus algorithm parameters in blockchain systems, explaining the methodology for evaluating and optimizing these essential components. The survey includes a comprehensive review of current research on the integration of blockchain-based IoT in eHealth. Finally, it outlines the challenges and potential solutions for implementing blockchain-based IoT in the eHealth sector, aiming to provide stakeholders with insights to improve patient care in this evolving field.","The digital health industry has experienced remarkable expansion, driven by innovative technologies that enable remote patient care and shift the focus away from conventional hospital-based treatment. The convergence of blockchain and the Internet of Things (IoT) has revolutionized healthcare services, offering a range of benefits, including real-time patient monitoring, efficient electronic health record management, pharmaceutical supply chain tracking, and effective disease surveillance, particularly during crises like the COVID-19 pandemic. However, the increasing adoption of IoT devices has raised concerns about data security and authentication. This study proposes the integration of blockchain technology as a robust solution, leveraging its decentralized and tamper-evident architecture to establish trust among diverse IoT devices and ensure the integrity of IoT-generated data. Furthermore, smart contracts enhance device authentication, mitigating vulnerabilities associated with centralized systems. This comprehensive review provides an in-depth examination of the eHealth industry, highlighting IoT-related challenges and exploring the potential of blockchain technology to address these obstacles. The study delves into the assessment and optimization of consensus algorithm parameters, providing valuable insights into the methodology used. A thorough analysis of existing research on blockchain-based IoT integration in eHealth is also presented, along with an overview of the challenges and potential solutions for implementing this technology in the sector, ultimately aiming to inform stakeholders and improve patient outcomes in this rapidly evolving field.","Driven by technological progress, the eHealth domain has experienced substantial expansion, enabling healthcare delivery within patients' residences and shifting away from conventional hospital environments. Technologies like blockchain and the Internet of Things (IoT) are instrumental in improving healthcare services, offering capabilities such as remote patient observation, efficient electronic health record (EHR) handling, medication tracking, and effective disease management, especially during crises like the COVID-19 pandemic. However, the increasing use of IoT devices introduces security concerns, particularly regarding data reliability and device validation. This study proposes incorporating blockchain technology as a potent solution. Utilizing its decentralized and immutable characteristics, blockchain fosters trust among various IoT devices, guaranteeing the integrity of IoT data. Moreover, smart contracts enhance device verification, strengthening overall security by addressing vulnerabilities linked to centralized systems. This review commences with an eHealth industry overview, emphasizing IoT-related healthcare challenges. It examines various IoT applications within eHealth and analyzes how blockchain can effectively overcome obstacles in healthcare management via IoT. Significantly, the study provides insights into evaluating consensus algorithm parameters within blockchain systems, elucidating the methodology employed to assess and optimize these crucial elements. The review encompasses a comprehensive examination of existing research on integrating blockchain-based IoT in eHealth. Finally, it presents an overview of challenges and potential solutions for implementing blockchain-based IoT in the eHealth sector. This comprehensive review aims to equip stakeholders with insights to improve patient care in this rapidly evolving and dynamic field."
106,Erosion and Corrosion of Pipelines and Equipment on the Power Uprate of Power Units at Nuclear Power Plants (NPP),"AbstractThe development of the modern nuclear industry is characterized by the conversion of operating power units to operation at a thermal power level above the current rated power level to enhance generation of electricity and reduce its cost. This process is referred to as a power uprate (PU). This can be done by increasing the thermal power of a reactor units and improving the efficiency of the turbine unit. In Russia and abroad, the thermal power of NPP power units is usually increased by 2–20% above the nominal level. Such measures are most widely implemented in the United States and Europe. Increasing the thermal power of a power unit above the rated value may require retrofitting of turbines, condensers, electric generators, and other equipment. The power uprate changes the operating conditions of equipment and pipelines of the power units, including such working fluid conditions as temperature, pressure, and steam wetness. This, in turn, can affect the erosion-corrosion rate of the metal of the working surfaces in the process circuit and decrease the residual service life until the maximum allowable thinning of the walls of equipment and pipelines is attained. Computational studies of foreign specialists demonstrate that the erosion-corrosion rate of individual pipelines and groups of equipment after the power uprate can increase by more than 50%. Because of this, obtaining a license for the power uprate of an NPP power unit in the United States requires computational studies using software tools to identify pipelines and equipment where the erosion-corrosion rate can increase. Since 2009, the RAMEK software code has been used at Russian nuclear power plants to predict the erosion-corrosion rate of pipeline and equipment elements. It is noted that, in practice, the power uprate of a power unit can enhance not only the erosion-corrosion rate in elements with a complex geometry in the flow path but also the effect of cavitation and droplet impact erosion. The expediency is pointed out of computational and experimental studies to assess the effect of changes in the operating conditions on the rate and distribution of erosion-corrosion wear of pipeline elements and equipment.","The evolution of the contemporary nuclear sector is marked by increasing the thermal power of functioning power units beyond the established rated levels to boost electricity production and cut costs. This practice is known as a power uprate (PU). It involves augmenting the thermal output of reactor units and enhancing the turbine unit's efficiency. Typically, the thermal power of NPP units in Russia and internationally is increased by 2–20% above the nominal level, a strategy predominantly adopted in the United States and Europe. Elevating a power unit's thermal power above the rated capacity may necessitate upgrades to turbines, condensers, electric generators, and other components. This power uprate alters the operating parameters of the equipment and pipelines, including working fluid conditions such as temperature, pressure, and steam moisture. Consequently, it may influence the erosion-corrosion rate of the metal on the working surfaces within the operational circuit, potentially reducing the residual service life until the maximum permissible thinning of equipment and pipeline walls is reached. Computational research by foreign experts indicates that, following a power uprate, the erosion-corrosion rate for certain pipelines and equipment groups can surge by over 50%. Therefore, in the United States, securing a license for the power uprate of an NPP unit necessitates computational analyses with software tools to pinpoint pipelines and equipment subject to increased erosion-corrosion rates. Since 2009, Russian nuclear power plants have utilized the RAMEK software code to forecast erosion-corrosion rates for pipeline and equipment components. It is acknowledged that practically, a power uprate can not only enhance the erosion-corrosion rate in intricately shaped elements within the flow path but also intensify the effects of cavitation and droplet impact erosion. The importance of computational and experimental studies is emphasized to evaluate the impact of altered operating conditions on the rate and distribution of erosion-corrosion wear in pipeline elements and equipment.","The modern nuclear industry's evolution is marked by the upscaling of operational power units to surpass their current rated thermal capacity, thereby boosting electricity generation and reducing costs. This process, known as power uprating, can be achieved by augmenting the reactor's thermal output and optimizing the turbine's efficiency. Globally, including in Russia, nuclear power plants (NPPs) typically undergo a 2-20% thermal power increase above their nominal level, with the United States and Europe being at the forefront of this trend. However, such upgrades may necessitate the retrofitting of turbines, condensers, generators, and other equipment. The resulting altered operating conditions of power unit equipment and pipelines, including changes in temperature, pressure, and steam humidity, can accelerate the erosion-corrosion rate of metal surfaces in the process circuit, ultimately reducing their residual lifespan. Foreign experts' computational simulations have shown that the erosion-corrosion rate of specific pipelines and equipment can surge by over 50% following a power uprate. Consequently, obtaining a license for an NPP power uprate in the United States requires computational assessments using specialized software to identify vulnerable equipment and pipelines. Since 2009, Russia's nuclear power plants have employed the RAMEK software code to predict erosion-corrosion rates in pipeline and equipment elements. In practice, power uprating can also exacerbate cavitation and droplet impact erosion in elements with complex geometries, highlighting the need for comprehensive computational and experimental studies to evaluate the impact of altered operating conditions on erosion-corrosion wear patterns in pipeline elements and equipment.","Modern nuclear power generation increasingly focuses on augmenting electrical output and cost efficiency by boosting the thermal power of existing reactors, a process known as power uprating (PU). This involves enhancing reactor thermal capacity and refining turbine efficiency. Typically, thermal power increases range from 2% to 20% above nominal levels, a practice particularly prevalent in the US and Europe. Power uprating often necessitates modifications to turbines, condensers, generators, and other components. These adjustments alter operational parameters such as temperature, pressure, and steam wetness, consequently influencing the rate of erosion-corrosion in working surfaces within the system. This accelerated wear can potentially shorten the remaining lifespan of equipment and pipelines by reducing the time to reach maximum allowable wall thinning. International computational analyses indicate that erosion-corrosion rates in specific pipelines and equipment groups can escalate by over 50% following power uprating. Consequently, US licensing for power uprating mandates computational assessments using specialized software to pinpoint areas susceptible to heightened erosion-corrosion. Since 2009, Russian nuclear facilities have employed the RAMEK software for predicting erosion-corrosion rates in pipeline and equipment components. Notably, practical experience reveals that power uprating can not only accelerate erosion-corrosion in intricate flow path geometries but also intensify cavitation and droplet impact erosion. This underscores the significance of computational and experimental investigations to evaluate the influence of modified operating conditions on the rate and distribution of erosion-corrosion wear in pipelines and equipment."
135,TALE-based organellar genome editing and gene expression in plants,"Key messageTALE-based editors provide an alternative way to engineer the organellar genomes in plants. We update and discuss the most recent developments of TALE-based organellar genome editing in plants.AbstractGene editing tools have been widely used to modify the nuclear genomes of plants for various basic research and biotechnological applications. The clustered regularly interspaced short palindromic repeats (CRISPR)/Cas9 editing platform is the most commonly used technique because of its ease of use, fast speed, and low cost; however, it encounters difficulty when being delivered to plant organelles for gene editing. In contrast, protein-based editing technologies, such as transcription activator-like effector (TALE)-based tools, could be easily delivered, expressed, and targeted to organelles in plants via Agrobacteria-mediated nuclear transformation. Therefore, TALE-based editors provide an alternative way to engineer the organellar genomes in plants since the conventional chloroplast transformation method encounters technical challenges and is limited to certain species, and the direct transformation of mitochondria in higher plants is not yet possible. In this review, we update and discuss the most recent developments of TALE-based organellar genome editing in plants.","Primary insight: TALE-based editors offer a novel approach to modify plant organellar genomes. We review and examine the latest advancements in TALE-based genome editing for plant organelles. Overview: Gene editing technologies have been extensively utilized to alter plant nuclear genomes for fundamental research and various biotechnological purposes. Among these, CRISPR/Cas9 stands out for its simplicity, rapidity, and cost-effectiveness. However, its application faces challenges in delivering gene-editing tools to plant organelles. Alternatively, protein-centric editing mechanisms, such as those based on transcription activator-like effectors (TALEs), are more readily deliverable, expressible, and targetable to organelles within plants through Agrobacterium-mediated nuclear transformation. Consequently, TALE-based editors represent a viable option for engineering plant organellar genomes, especially since traditional chloroplast transformation methods confront technical obstacles and are restricted to specific species, and direct mitochondrial transformation in higher plants remains unachievable. This review highlights and deliberates the cutting-edge progress in TALE-based editing of plant organellar genomes.","Here is a paraphrased version of the text, exhibiting both syntactic and lexical diversity while preserving semantic similarity: ""Novel approach: TALE-based editing tools offer a viable alternative for modifying plant organellar genomes, circumventing the limitations of traditional methods. This review provides an update on the latest advancements in TALE-based organellar genome editing in plants. Genome modification techniques have revolutionized plant research and biotechnology, with the CRISPR/Cas9 system being the most popular due to its efficiency, speed, and affordability. However, its application in plant organelles is hindered by delivery challenges. In contrast, protein-based technologies, such as TALE-based systems, can be easily introduced, expressed, and targeted to plant organelles via Agrobacterium-mediated nuclear transformation. Consequently, TALE-based editors provide a promising solution for engineering plant organellar genomes, overcoming the technical hurdles and species limitations associated with conventional chloroplast transformation and the currently impossible direct transformation of mitochondria in higher plants.""","Plant organellar genomes can be modified using TALE-based editors, offering a new approach to genetic engineering. While CRISPR/Cas9 is widely employed for nuclear genome editing due to its user-friendliness, speed, and affordability, its application to organellar editing is hampered by delivery challenges. Conversely, protein-based technologies like TALE readily reach plant organelles via Agrobacterium-mediated nuclear transformation, allowing for efficient expression and targeting. This makes TALE-based editors a viable alternative, particularly as traditional chloroplast transformation methods face technical limitations and species specificity, and direct mitochondrial transformation in higher plants remains unattainable. This review provides an updated analysis of the latest advancements in TALE-mediated organellar genome editing in plants."
59,Role of Regulatory Sandboxes and MLOps for AI-Enabled Public Sector Services,"This paper discusses how innovations in public sector AI-based services must comply with the Artificial Intelligence Act (AI Act) regulatory frameworks while enabling experimentation and participation of diverse stakeholders throughout the Artificial Intelligence (AI) lifecycle. The paper examines the implications of the emerging regulation, AI regulatory sandboxes and Machine Learning Operations (MLOps) as tools that facilitate compliance while enabling co-learning and active participation of multiple stakeholders. We propose a framework that fosters experimentation with automation pipelines and continuous monitoring for the deployment of future public sector AI-based services in a regulatory-compliant and technically innovative manner. AI regulatory sandboxes can be beneficial as a space for contained experimentation that goes beyond regulatory considerations to specific experimentation with the implementation of ML frameworks. While the paper presents a framework based on emerging regulations, tools and practices pertaining to the responsible use of AI, this must be validated through pilot experimentation with public and private stakeholders and regulators in different areas of high-risk AI-based services.","This paper explores the necessity for innovations in AI-driven public sector services to align with the regulatory structures of the Artificial Intelligence Act (AI Act) while fostering experimentation and engagement from a variety of stakeholders throughout the AI lifecycle. The paper investigates the outcomes of the new regulation, AI regulatory sandboxes, and Machine Learning Operations (MLOps) as mechanisms that aid in compliance and promote co-learning and active involvement from diverse stakeholders. We introduce a framework designed to encourage experimentation with automation pipelines and continuous oversight for the implementation of future public sector AI services in a manner that is both regulatory-compliant and technically innovative. AI regulatory sandboxes serve as conducive environments for controlled experiments, extending beyond regulatory concerns to specific testing with ML frameworks. Although the paper proposes a framework based on current regulations, tools, and practices for the responsible application of AI, it necessitates validation through pilot experiments involving public and private stakeholders, as well as regulators, in various domains of high-risk AI services.","This study delves into the imperative of aligning cutting-edge public sector AI-driven services with the Artificial Intelligence Act's regulatory guidelines, while simultaneously encouraging collaborative exploration and stakeholder engagement throughout the AI development cycle. The analysis explores the far-reaching consequences of burgeoning regulations, AI regulatory sandpits, and MLOps as catalysts for compliance, co-creation, and active stakeholder involvement. We advocate for a paradigm that cultivates experimentation with automated workflows and continuous oversight, ensuring the deployment of future public sector AI-driven services in a manner that is both regulatory-compliant and technologically pioneering. AI regulatory sandpits offer a controlled environment for experimentation, transcending regulatory constraints to explore the practical application of ML frameworks. While this study presents a framework grounded in emerging regulations, tools, and best practices for responsible AI adoption, its validity hinges on pilot testing with diverse stakeholders, regulators, and high-risk AI-based service providers.","This study explores the challenge of balancing regulatory compliance with innovation in public sector AI services. It analyzes the role of the Artificial Intelligence Act, regulatory sandboxes, and Machine Learning Operations (MLOps) in fostering a collaborative approach to AI development that includes diverse stakeholders. The authors propose a framework that integrates automation pipelines and continuous monitoring to ensure both regulatory adherence and technical advancement in the deployment of public sector AI services. Recognizing the potential of regulatory sandboxes as controlled environments for experimentation, the paper emphasizes their value in exploring practical implementation of ML frameworks beyond mere regulatory considerations. While the framework draws on existing regulations, tools, and best practices for responsible AI, its efficacy must be evaluated through pilot programs involving public and private stakeholders, as well as regulatory bodies, across various high-risk AI service domains."
105,Alien Fish Species in Cooling Ponds of Nuclear Power Plants and State District Power Stations of Central Russia,"AbstractThe article presents an overview of alien fish species living in cooling ponds of nuclear power plants and state district power stations in Central Russia. A brief description of recipient reservoirs is given as a habitat for thermophilic fish species that are not characteristic of the native ichthyofauna of the region. The essays on the history of the appearance of these species, the peculiarities of biology, occurrence in catches, and fishery significance in six small reservoirs of special water management located within the borders of the Moscow, Smolensk, Ryazan, and Kursk oblasts of Russia are given. The main vectors of invasions of hydrobionts in cooling ponds for each alien species recorded as a result of ichthyological monitoring in this group of reservoirs are shown. The forecast of the dynamics of the number of alien fish species is given depending on changes in the thermal regime of reservoirs and the volume of stocking and the intensity of fishing in the cooling ponds of nuclear power plants and state district power plants of Central Russia.","The paper offers a comprehensive review of non-native fish species inhabiting cooling ponds at nuclear and state district power stations in Central Russia. It provides a succinct overview of the target reservoirs as habitats for thermophilic fish species uncommon to the native ichthyofauna in the area. The document narrates the history of these species introduction, their biological traits, their presence in catches, and their relevance to fisheries across six small, specially managed reservoirs within Moscow, Smolensk, Ryazan, and Kursk regions. It identifies the primary pathways through which these aquatic species have infiltrated cooling ponds and presents ichthyological monitoring data from the said reservoirs. Additionally, the forecast of the future population trends of these alien fish species is discussed, taking into account changes in the thermal environment of the reservoirs, stocking practices, and fishing pressure in the cooling ponds of nuclear and state district power stations in Central Russia.","This paper provides a comprehensive review of non-native fish populations thriving in the cooling systems of nuclear power plants and district power stations across Central Russia, highlighting the unique characteristics of the recipient water bodies that support these thermophilic species, which are foreign to the region's indigenous fish fauna. The study delves into the history of their introduction, biological distinctiveness, catch rates, and fisheries importance in six specially managed reservoirs spanning the Moscow, Smolensk, Ryazan, and Kursk regions. The primary pathways of hydrobiont invasions in these cooling systems are identified for each non-native species, based on ichthyological monitoring data. Furthermore, the paper predicts the future trajectory of alien fish species populations in response to shifts in reservoir thermal regimes, stocking volumes, and fishing intensities in Central Russia's nuclear and district power plants.","This study examines the presence of non-native fish species inhabiting cooling ponds at nuclear and district power plants in Central Russia. It details the characteristics of these reservoirs as habitats for warm-water fish atypical of the region's indigenous fish population. The paper investigates six specific reservoirs in the Moscow, Smolensk, Ryazan, and Kursk regions, exploring the history, biology, capture rates, and economic importance of these introduced species. Furthermore, it identifies the primary pathways of aquatic organism introduction into these cooling ponds, based on ichthyological monitoring data. Finally, the study predicts how fluctuations in water temperature, stocking practices, and fishing intensity might influence the future population dynamics of these non-native fish species within the power plant cooling ponds of Central Russia."
43,Effect of digital literacy on social entrepreneurial intentions and nascent behaviours among students and practitioners in mass communication,"Because journalism and mass communication have changed dramatically in the digital era, exploring alternative career paths for students and practitioners in mass communication may offer valuable insights for the future of the field. Considering the emphasis on decent work opportunities and responsible production practices outlined in the Sustainable Development Goals, this study evaluates the determinants of social entrepreneurial intentions and behaviours of students and practitioners of mass communication in Taiwan. A quantitative survey method is adopted. In total, 814 participants, consisting of 373 students and 441 practitioners, provided valid responses, which were analysed utilising structural equation modelling. The results in both samples indicate positive direct effects of perceived social support, perceived social awareness of peers, and digital literacy on social entrepreneurial intentions, and positive direct effects of digital literacy and social entrepreneurial intentions on social entrepreneurial behaviours. The study identifies digital literacy as a critical element for fostering social entrepreneurial intentions and behaviours. Moreover, educational implications and suggestions are also provided.","Due to significant transformations in journalism and mass communication in the digital age, investigating alternative career trajectories for students and professionals in this field may yield important insights for its future. With a focus on decent work opportunities and responsible production practices as highlighted by the Sustainable Development Goals, this study examines the factors influencing social entrepreneurial intentions and behaviours among Taiwanese mass communication students and practitioners. Using a quantitative survey approach, 814 respondents—comprising 373 students and 441 professionals—provided valid data, which were analysed via structural equation modelling. Both groups' results show that perceived social support, peer social awareness, and digital literacy positively impact social entrepreneurial intentions, and that digital literacy and social entrepreneurial intentions positively influence social entrepreneurial behaviours. The study underscores digital literacy as a vital factor in promoting social entrepreneurial intentions and actions. Additionally, educational implications and recommendations are discussed.","In light of the profound transformations that have reshaped the landscape of journalism and mass communication in the digital age, it is essential to investigate novel career trajectories for students and professionals in this field, which may yield valuable lessons for its future development. Given the importance assigned to decent work opportunities and responsible production practices in the Sustainable Development Goals, this research examines the factors influencing the social entrepreneurial aspirations and actions of mass communication students and practitioners in Taiwan. A quantitative survey approach is employed, yielding 814 valid responses from 373 students and 441 professionals, which are subsequently analysed using structural equation modelling. The findings from both samples reveal that perceived social support, peer awareness, and digital proficiency have a direct, positive impact on social entrepreneurial aspirations, while digital proficiency and social entrepreneurial aspirations, in turn, directly influence social entrepreneurial actions. The study highlights digital literacy as a pivotal factor in cultivating social entrepreneurial aspirations and actions, and also provides educational recommendations and implications.","The rapid evolution of journalism and mass communication in the digital age necessitates investigating alternative career options for students and professionals in this field. This study, grounded in the Sustainable Development Goals' emphasis on ethical employment and production practices, examines the factors influencing social entrepreneurial intentions and actions among Taiwanese mass communication students and practitioners. Employing a quantitative survey approach, data from 814 valid respondents (373 students and 441 practitioners) were analyzed using structural equation modelling. Findings reveal that perceived social support, peer social awareness, and digital literacy positively and directly impact social entrepreneurial intentions in both groups. Additionally, digital literacy and entrepreneurial intentions directly contribute to actual entrepreneurial behaviors. This research highlights the crucial role of digital literacy in fostering social entrepreneurship. The study concludes by offering educational implications and recommendations for future practice."
18,An immersive virtual reality learning environment with CFD simulations: Unveiling the Virtual Garage concept,"Virtual reality has become a significant asset to diversify the existing toolkit supporting engineering education and training. The cognitive and behavioral advantages of virtual reality (VR) can help lecturers reduce entry barriers to concepts that students struggle with. Computational fluid dynamics (CFD) simulations are imperative tools intensively utilized in the design and analysis of chemical engineering problems. Although CFD simulation tools can be directly applied in engineering education, they bring several challenges in the implementation and operation for both students and lecturers. In this study, we develop the “Virtual Garage” as a task-centered educational VR application with CFD simulations to tackle these challenges. The Virtual Garage is composed of a holistic immersive virtual reality experience to educate students with a real-life engineering problem solved by CFD simulation data. The prototype is tested by graduate students (n = 24) assessing usability, user experience, task load and simulator sickness via standardized questionnaires together with self-reported metrics and a semi-structured interview. Results show that the Virtual Garage is well-received by participants. We identify features that can further leverage the quality of the VR experience with CFD simulations. Implications are incorporated throughout the study to provide practical guidance for developers and practitioners.","Virtual reality has emerged as a pivotal tool to broaden the repertoire supporting engineering education and training. The cognitive and behavioral benefits of VR assist educators in lowering the barriers to understanding challenging concepts for students. Computational fluid dynamics (CFD) simulations are critical instruments extensively used in the design and analysis of chemical engineering issues. Despite their direct applicability in engineering education, CFD simulation tools present numerous challenges in their implementation and operation for both students and instructors. This research introduces the ""Virtual Garage,"" a task-focused educational VR application incorporating CFD simulations to address these challenges. The Virtual Garage offers an all-encompassing immersive VR experience, educating students through a real-world engineering problem solved with CFD simulation data. The prototype was tested with graduate students (n=24), who evaluated usability, user experience, task load, and simulator sickness using standardized questionnaires along with self-reported metrics and a semi-structured interview. Findings indicate that the Virtual Garage is positively received by participants. We pinpoint features that can further enhance the quality of the VR experience incorporating CFD simulations. Practical recommendations for developers and practitioners are provided throughout the study.","The incorporation of virtual reality into the engineering education and training paradigm has yielded a substantial enhancement, broadening the scope of available resources. By harnessing the cognitive and behavioral benefits of virtual reality, educators can facilitate a more seamless understanding of complex concepts that often pose difficulties for students. In the realm of chemical engineering, computational fluid dynamics simulations play a vital role in the design and analysis of problems, although their direct application in educational settings can be hindered by implementation and operational challenges. To address these obstacles, this study introduces the ""Virtual Garage,"" a task-oriented virtual reality platform that integrates CFD simulations to provide a comprehensive, immersive learning experience centered on real-world engineering problems. The efficacy of the Virtual Garage was evaluated through a pilot study involving 24 graduate students, who assessed its usability, user experience, task load, and simulator sickness via standardized questionnaires, self-reported metrics, and semi-structured interviews. The results indicate a positive reception among participants, highlighting areas for further improvement to optimize the virtual reality experience with CFD simulations. The study's findings offer practical insights for developers and practitioners seeking to integrate virtual reality into their educational approaches.","Immersive virtual reality (VR) technologies are increasingly recognized as valuable tools for enhancing engineering education. VR's ability to influence cognition and behavior can be leveraged by educators to simplify complex concepts that students often find challenging. Computational fluid dynamics (CFD) simulations, while crucial for analyzing and designing chemical engineering solutions, pose considerable operational and implementation difficulties for both educators and learners. To address these obstacles, we have created ""Virtual Garage,"" an educational VR application centered around a real-world engineering challenge that utilizes CFD simulation data. This immersive experience provides students with a comprehensive understanding of the problem and its resolution. Graduate students (n=24) evaluated a prototype of the Virtual Garage, assessing its usability, user experience, task load, and simulator sickness through standardized questionnaires, self-reported measures, and semi-structured interviews. The Virtual Garage was favorably received by participants, and the study identifies areas for improvement to enhance the quality of the VR experience with CFD simulations. Practical guidance for developers and practitioners is provided throughout the study, highlighting the implications of integrating VR and CFD simulations in engineering education."
6,Virtual team training with Mixed Reality and Virtual Reality – benefits and limitations illustrated on the example of two paramedic classes,"In the field of medical case simulations, strong technologization has been observed for years, for example through Extended Reality. This study examined the impact of Virtual and Mixed Reality (VR, resp. MR) on the team training of paramedic trainees. In the quasi-experimental cross-sectional controlled comparison study, participants were assigned to an experimental group (= EG, mixed reality with manikin) and a comparison group (= CG, virtual reality without manikin). After a virtual case simulation, “VR sickness”, sense of presence, motivation, and sociodemographic variables were elicited. Parametric tests were used for mean comparison and correlation analysis. A total of 20 participants were evaluated in the EG and 16 in the CG. In both groups, high intrinsic motivation (mean 5.32/7), an acceptable sense of spatial presence (mean 4.24/6), and advanced usability (mean 68.54/100), were observed. VR sickness symptoms were present (SSQ total score: 11.43). In both groups, correlation was found between Identified Regulation and Usability (EG: r = .74, p < .001, CG: r = .76, p < .001). The same pattern could be observed for Intrinsic Motivation and Usability (EG: r = .83, p < .001, CG: r = .70, p = .003). Spatial Presence and Usability were also correlated in both groups (EG: r = .71, p < .001, CG: r = .61, p = .01). A significant mean score difference (p = .021) was found for Amotivation between the EG (Mean = 1.92/6) and the CG (Mean = 1.56/6). VR and MR are potential learning methods if they are implemented in a planned manner by competent faculty. Our recommendation is to use setup checklists, but also a competence-oriented approach, considering fiction contract and structured debriefing. The use of wireless head-mounted displays and ""data gloves"" is also recommended.","The domain of medical case simulations has witnessed significant technological advancements over the years, particularly with the integration of Extended Reality. This research investigated how Virtual Reality (VR) and Mixed Reality (MR) influence team training for paramedic learners. The study adopted a quasi-experimental, cross-sectional, controlled comparison methodology, categorizing participants into an experimental group (EG) using mixed reality with a manikin, and a control group (CG) utilizing virtual reality without a manikin. Post-simulation, participants were assessed on ""VR sickness,"" presence, motivation, and demographic variables. Analysis through parametric tests was used to compare mean scores and identify correlations. Twenty participants were evaluated in the EG and sixteen in the CG. Findings demonstrated high intrinsic motivation (mean 5.32/7), acceptable spatial presence (mean 4.24/6), and good usability (mean 68.54/100) in both groups, despite reports of VR sickness (SSQ total score: 11.43). Positive correlations were seen between identified regulation and usability (EG: r = .74, p < .001; CG: r = .76, p < .001), intrinsic motivation and usability (EG: r = .83, p < .001; CG: r = .70, p = .003), and spatial presence and usability (EG: r = .71, p < .001; CG: r = .61, p = .01) across both groups. There was a notable mean score difference in amotivation between the EG (mean 1.92/6) and CG (mean 1.56/6, p = .021). VR and MR can be effective learning tools when thoughtfully implemented by knowledgeable instructors. We recommend the use of preparation checklists, a competence-focused approach, recognition of implicit agreements, structured debriefing, wireless head-mounted displays, and ""data gloves.""","In the realm of medical simulation training, a pronounced trend towards technological advancement has been evident for some time, exemplified by the integration of Extended Reality. This investigation explored the effects of Virtual and Mixed Reality (VR and MR, respectively) on the collaborative training of paramedic students. A quasi-experimental, cross-sectional, controlled study was conducted, wherein participants were divided into an experimental cohort (EC), utilizing MR with a manikin, and a control group (CG), employing VR without a manikin. Following a virtual simulation exercise, the incidence of ""VR sickness,"" sense of immersion, motivational levels, and demographic characteristics were assessed. Parametric statistical analyses were employed to compare means and examine correlations. The study evaluated 20 participants in the EC and 16 in the CG. Notably, both groups exhibited high levels of intrinsic motivation (mean score: 5.32/7), a satisfactory sense of spatial immersion (mean score: 4.24/6), and advanced usability (mean score: 68.54/100). Symptoms of VR sickness were present, with a total SSQ score of 11.43. Correlations were observed between identified regulation and usability, intrinsic motivation and usability, as well as spatial presence and usability in both groups. A significant difference in mean scores was found for amotivation between the EC (mean score: 1.92/6) and the CG (mean score: 1.56/6). The strategic implementation of VR and MR by competent instructors holds promise as a learning methodology. Recommendations include the use of setup checklists, a competence-oriented approach incorporating fiction contracts and structured debriefing, as well as the integration of wireless head-mounted displays and ""data gloves.""","The increasing integration of technology, notably Extended Reality, in medical simulation training has spurred investigations into its effectiveness. This study explored the influence of Virtual and Mixed Reality (VR/MR) on paramedic team training using a quasi-experimental design. Participants, divided into an MR group using manikins and a VR group without, completed a virtual case simulation. Assessments included VR sickness, sense of presence, motivation, and sociodemographic factors. Both groups exhibited high intrinsic motivation, moderate spatial presence, and good usability, but experienced VR sickness symptoms. Significant correlations emerged between self-regulation, intrinsic motivation, spatial presence, and usability within each group. Notably, the MR group showed lower amotivation levels. This suggests that VR/MR can be valuable learning tools when thoughtfully implemented by skilled instructors, emphasizing setup protocols, competency-based approaches, acknowledging the simulation context, and incorporating structured debriefings. The study also recommends the use of wireless VR headsets and haptic gloves."
186,Modelling Climate Finance Flows in Sub-Saharan Africa,"Climate finance stakeholders across Africa have long sought to understand the complex nature of the climate cash flow architecture. Distribution models are critical mathematical tools for generating the general characteristics of the cash flow that are used to inform policy decisions. In this paper, we undertake a comprehensive investigation of the climate funds flowing into sub-Saharan Africa (SSA) by suggesting candidate climate finance models that can be used by policy makers to design simulations that can aid in assessing climate risks, identify more efficient climate finance schemes, and obtain optimal control parameter settings under different scenarios. This is achieved by considering climate finance as a form of insurance. Different dimensions of the data are examined following four distinct groupings of the data set. This is to account for different views of risk by the various climate finance participants. The frequency and severity of the approved funds are analyzed with the aid of various mathematical distribution models and regression analyses. The dynamics of a given variable relative to varying scenarios are examined. The findings obtained confirm the presence of emerging risks induced by the nature of the flow. Central Africa for instance records the lowest theme-specific projects and mitigation finance accounts for more than half of all the climate funds while sectoral-wise, adaptation finance is majorly concentrated in the energy sector. The perpetuation of the observed inequalities across the themes, subregions and sector-specific climate-related projects portends grave consequences as these risks begin to accumulate over time. The Burr mixture model best fitted the approved projects’ cost distribution and the factors driving the frequency and severity of approved projects ranged from Central Africa to projects in the general environment sector. One of the policy recommendations emphasized was the need to adopt a risk-adjusted distribution model for climate finance allocation in SSA.","Stakeholders involved in climate finance across Africa have long strived to comprehend the intricate nature of the climate cash flow framework. Distribution models serve as essential mathematical instruments for generating the general characteristics of cash flow, which are crucial for policy-making. This paper conducts an exhaustive analysis of climate funds directed towards sub-Saharan Africa (SSA), proposing candidate climate finance models for policymakers. These models can be deployed to design simulations that assist in evaluating climate risks, identifying more efficient climate finance schemes, and determining optimal control parameters under varying scenarios. This is approached by treating climate finance as a form of insurance. Different data dimensions are reviewed through four distinct categorizations to account for varying risk perspectives among climate finance stakeholders. The frequency and severity of approved funds are assessed using various mathematical distribution models and regression analyses, examining the dynamics of a given variable under different scenarios. The findings reveal emerging risks attributable to the nature of the cash flow, with Central Africa exhibiting the fewest theme-specific projects and mitigation finance comprising over half of all climate funds. Sector-wise, adaptation finance is predominantly concentrated in the energy sector. The persistence of observed disparities across themes, subregions, and sector-specific climate-related projects signals serious consequences as these risks accumulate over time. The Burr mixture model was found to most accurately represent the cost distribution of approved projects, with drivers of project frequency and severity ranging from Central Africa to projects in the general environment sector. Among the policy recommendations, adopting a risk-adjusted distribution model for allocating climate finance in SSA was highlighted.","Across Africa, stakeholders in climate finance have persistently endeavored to decipher the intricate framework of climate-related financial flows. Mathematical distribution models serve as vital instruments for policymakers, as they provide a comprehensive understanding of cash flow patterns, thereby informing strategic decisions. This study undertakes an exhaustive examination of climate finance inflows into sub-Saharan Africa, proposing novel models that enable policymakers to simulate climate risk assessments, identify optimized financing schemes, and determine ideal control parameters under diverse scenarios. By conceptualizing climate finance as a form of risk management, this research explores multiple dimensions of the data, categorized into four distinct groups, to accommodate varied risk perceptions among stakeholders. Frequency and severity analyses of approved funds, facilitated by mathematical distribution models and regression techniques, reveal the dynamics of variables under different scenarios. The findings confirm the emergence of risks inherent to the flow of climate finance, with notable disparities observed across themes, subregions, and sector-specific projects. For instance, Central Africa exhibits the lowest number of theme-specific projects, while mitigation finance dominates the climate fund allocation, and adaptation finance is predominantly concentrated in the energy sector. The perpetuation of these inequalities poses significant long-term consequences. The Burr mixture model is identified as the best fit for the cost distribution of approved projects, with factors influencing project frequency and severity ranging from Central Africa to projects focused on the general environment sector. A key policy recommendation is the adoption of a risk-adjusted distribution model for climate finance allocation in sub-Saharan Africa.","African stakeholders in climate finance have consistently aimed to decipher the intricate mechanisms governing climate-related financial flows. Mathematical distribution models play a crucial role in delineating the general characteristics of these flows, which subsequently inform policy decisions. This study offers a detailed exploration of climate funds directed towards sub-Saharan Africa (SSA), proposing candidate models that policymakers can utilize for simulations. These simulations, conceptualizing climate finance as a form of insurance, can facilitate climate risk assessment, identify superior finance schemes, and determine optimal control parameters under varying scenarios. To account for diverse perspectives on risk among stakeholders, the data is categorized into four distinct groups and analyzed across different dimensions. Through mathematical distribution models and regression analyses, the frequency and magnitude of approved funds are scrutinized, alongside the dynamic behavior of variables in various scenarios. The results highlight emerging risks inherent in the nature of climate finance flows. For example, Central Africa exhibits the lowest number of theme-specific projects, mitigation finance constitutes over half of all climate funds, and adaptation finance is heavily concentrated in the energy sector. The persistence of observed inequalities across themes, subregions, and sector-specific climate projects poses significant future risks if allowed to accumulate. The Burr mixture model was found to best fit the cost distribution of approved projects, with factors influencing project frequency and severity ranging from Central Africa to initiatives within the general environment sector. One key policy recommendation underscores the need for a risk-adjusted distribution model for allocating climate finance in SSA."
49,Linear digital precoding technology in massive multiple input multiple output wireless communication system,"The rapid development of wireless communication in today's society is facing the development bottleneck of increasingly tense spectrum resources. As the key technology of 5G technology, Massive Multiple Input Multiple Output (MMIMO) technology has been widely studied and applied in recent years. It is expected to bring progress to wireless communication systems. The key characteristic of MMIMO is its large antenna array that results in a high dimension of the wireless system channel. This article examines the use of precoding technology in large-scale multi-antenna MIMO systems to address the issue of Common Channel Interference (CCI). The study starts by introducing the concept of linear digital precoding and provides mathematical demonstrations of three linear digital precoding schemes. The performance of these schemes is then simulated and analyzed using MATLAB, with the results showing a significant improvement in the bit error performance and combining rate of the MIMO system.","The swift advancement of wireless communication in contemporary society is encountering the challenge of increasingly scarce spectrum resources. MMIMO technology, a crucial component of 5G, has garnered significant attention and application in recent times. It promises to enhance wireless communication networks. MMIMO is characterized by its extensive antenna array, which leads to a high-dimensional wireless system channel. This paper explores the utilization of precoding technology within large-scale multi-antenna MIMO systems to solve the problem of Common Channel Interference (CCI). Beginning with an introduction to linear digital precoding, the study offers mathematical explanations of three linear digital precoding approaches. The performance of these approaches is subsequently simulated and evaluated using MATLAB, with findings indicating notable improvements in bit error rate and combining rate for the MIMO system.","The exponential growth of wireless communication in modern times is hindered by the escalating scarcity of spectral resources, prompting a pressing need for innovative solutions. As a pivotal component of 5G technology, the Massive Multiple Input Multiple Output (MMIMO) system has garnered significant attention and widespread adoption in recent years, promising to revolutionize wireless communication networks. A distinctive feature of MMIMO is its massive antenna array, which yields a high-dimensional wireless system channel. This paper delves into the application of precoding techniques in large-scale multi-antenna MIMO systems to mitigate the detrimental effects of Co-Channel Interference (CCI). The investigation commences with an exposition of linear digital precoding, followed by mathematical derivations of three linear digital precoding algorithms. The efficacy of these schemes is subsequently evaluated and analyzed using MATLAB simulations, revealing a substantial enhancement in the bit error rate and combining capacity of the MIMO system.","Contemporary society's swift advancement in wireless communication is encountering a significant obstacle due to the escalating scarcity of spectrum resources. Massive Multiple Input Multiple Output (MMIMO), a pivotal technology in 5G development, has garnered substantial research and implementation in recent times, holding the potential to revolutionize wireless communication systems. The defining feature of MMIMO lies in its expansive antenna array, resulting in a highly dimensional wireless system channel. This study delves into the application of precoding techniques within large-scale multi-antenna MIMO systems to mitigate Common Channel Interference (CCI). Commencing with an elucidation of linear digital precoding, the study presents mathematical proofs of three distinct linear digital precoding methodologies. Subsequently, the efficacy of these schemes is evaluated through MATLAB simulations, revealing a notable enhancement in both the bit error rate and combining rate of the MIMO system."
174,Does green finance policy promote green total factor productivity? Evidence from a quasi-natural experiment in the green finance pilot zone,"The establishment of Green Finance Reform and Innovation Pilot Zones represents a novel endeavor aimed at paving a “bottom-up” pathway for the advancement of green finance and, consequently, holds immense significance in fostering economic growth that is environmentally sustainable. Drawing on data spanning the years 2013–2019 from 278 prefecture-level cities across China, we delve into the influence of the Green Finance Reform and Innovation Pilot policy on urban Green Total Factor Productivity. We found that: Firstly, the Green Finance Reform and Innovation Pilot policy has demonstrated a discernible enhancement in Green Total Factor Productivity within the designated pilot regions. Secondly, there is heterogeneity in the promotion effect of this pilot policy on Green Total Factor Productivity. As the geographical advantage and economic development level decrease, this promotional effect gradually diminishes. Thirdly, it is evident that the Green Finance Reform and Innovation Pilot policy primarily elevates Green Total Factor Productivity in the pilot regions by improving resource allocation efficiency, fostering innovation in green technologies, and facilitating the upgrading of industrial structures. This research not only underscores the efficacy of green finance in the context of economic transition but also provides empirical substantiation for the strategic utilization of pilot zones to propel nationwide initiatives aimed at sustainable green development.Graphical abstract","The creation of Green Finance Reform and Innovation Pilot Zones signifies a groundbreaking effort to establish a grassroots approach to advancing green finance, which is highly important for promoting environmentally sustainable economic growth. Utilizing data from 278 prefecture-level cities in China between 2013 and 2019, this study explores the impact of the Green Finance Reform and Innovation Pilot policy on urban Green Total Factor Productivity. The findings are as follows: Firstly, the Green Finance Reform and Innovation Pilot policy has significantly boosted Green Total Factor Productivity in the designated pilot areas. Secondly, the policy’s effect on Green Total Factor Productivity varies, with a decrease in promotional impact corresponding to lower geographical advantages and levels of economic development. Thirdly, the Green Finance Reform and Innovation Pilot policy primarily enhances Green Total Factor Productivity in these regions by improving resource allocation efficiency, encouraging green technology innovation, and promoting industrial structure upgrades. This research highlights the effectiveness of green finance in economic transitions and provides empirical evidence supporting the strategic use of pilot zones to drive nationwide sustainable green development initiatives.","The creation of Green Finance Reform and Innovation Pilot Zones embodies a pioneering initiative designed to forge a grassroots approach to green finance development, thereby playing a pivotal role in catalyzing environmentally conscious economic expansion. By analyzing a comprehensive dataset covering 278 prefecture-level cities in China from 2013 to 2019, we investigate the impact of the Green Finance Reform and Innovation Pilot policy on urban green productivity. Our findings reveal that: Firstly, the policy has yielded a tangible improvement in green productivity within the designated pilot areas. Secondly, the policy's promotional effect on green productivity exhibits regional disparities, with the impact diminishing as geographical advantages and economic development levels decrease. Thirdly, the policy primarily boosts green productivity in pilot regions by optimizing resource allocation, driving innovation in eco-friendly technologies, and facilitating industrial upgrading. This study not only highlights the potency of green finance in facilitating economic transformation but also provides empirical evidence for the strategic deployment of pilot zones to drive nationwide initiatives towards sustainable green growth.","China's groundbreaking initiative to establish Green Finance Reform and Innovation Pilot Zones is a localized approach designed to propel the growth of environmentally responsible financial practices. Utilizing data from 278 Chinese cities between 2013 and 2019, this study explores the impact of this policy on urban Green Total Factor Productivity. The research reveals that the pilot program demonstrably boosts Green Total Factor Productivity in participating regions, although this effect varies based on geographic and economic factors, diminishing in areas with fewer advantages. Notably, the policy primarily enhances Green Total Factor Productivity by optimizing resource allocation, stimulating green technology innovation, and driving industrial upgrades. This investigation not only confirms the vital role of green finance in economic transformation but also provides concrete evidence supporting the strategic use of pilot zones to advance nationwide sustainable development goals."
179,Intelligent finance and change management implications,"Change management is the embodiment of an enterprise’s core competence. It provides competitive differentiation and effectively adapts to the ever-changing world. This paper explores the implications of intelligent finance on change management and provides insights into how organizations can effectively manage change to achieve the desired outcomes. The study examines the case of Ping An (Ping An Insurance (Group) Company of China, Ltd.), a leading insurance company in China that has successfully implemented intelligent finance and change management strategies. The paper begins with a literature review that provides an overview of the concept of intelligent finance, the relevance of change management in the context of intelligent finance, models, and frameworks for intelligent finance, and approaches to change management. The study then presents a case analysis of Ping An, including descriptive statistics, inferential statistics, regression analysis, and qualitative findings. The paper concludes with implications for practice and theory, contributions of the study, and recommendations for future research. Overall, this paper contributes to the growing literature on intelligent finance and change management and provides practical insights for organizations seeking to adopt intelligent finance.","Change management represents the foundational competence of a business, offering a distinct competitive edge and efficiently adjusting to a constantly evolving environment. This paper delves into how intelligent finance impacts change management and provides guidance on managing change effectively to achieve desired results. The research focuses on Ping An Insurance (Group) Company of China, Ltd., a prominent insurance firm that has adeptly applied intelligent finance and change management methodologies. The paper starts with a literature review, giving an overview of intelligent finance, its significance in relation to change management, and existing models and frameworks. It then moves on to a detailed case study of Ping An, including both descriptive and inferential statistics, regression analysis, and qualitative data. The study concludes by discussing the practical and theoretical implications, the contributions made by the research, and suggestions for future studies. In summary, this paper enriches the existing body of knowledge on intelligent finance and change management and offers actionable insights for businesses looking to implement intelligent finance.","""An organization's capacity for change management serves as the cornerstone of its competitive prowess, enabling it to thrive in a dynamic environment. This research delves into the interplay between intelligent finance and change management, offering actionable guidance for businesses to navigate transformation and achieve their objectives. A case study of Ping An, a Chinese insurance giant that has successfully harnessed intelligent finance and change management, is presented, featuring a comprehensive literature review, statistical analysis, and qualitative findings. The paper culminates in practical recommendations, theoretical implications, and avenues for future investigation, thereby enriching the existing body of knowledge on intelligent finance and change management and providing valuable lessons for organizations embarking on intelligent finance adoption.""","At the heart of an organization's success lies its ability to manage change, a key differentiator in today's dynamic landscape. This research delves into the intersection of intelligent finance and change management, illuminating how companies can navigate this transformation for optimal results. Focusing on Ping An, a pioneering Chinese insurance firm, the study analyzes its successful implementation of intelligent finance and associated change management tactics. Through a comprehensive literature review, the paper first establishes the foundations of intelligent finance, its relationship with change management, relevant models and frameworks, and various change management approaches. Subsequently, a detailed case study of Ping An is presented, employing descriptive and inferential statistics, regression analysis, and qualitative insights. The paper concludes by outlining practical and theoretical implications, highlighting the study's contributions, and suggesting avenues for further investigation. Ultimately, this work enriches the existing body of knowledge on intelligent finance and change management, offering valuable guidance for organizations looking to embrace intelligent finance."
147,Driver behavior while using Level 2 vehicle automation: a hybrid naturalistic study,"Vehicle automation is becoming more prevalent. Understanding how drivers use this technology and its safety implications is crucial. In a 6–8 week naturalistic study, we leveraged a hybrid naturalistic driving research design to evaluate driver behavior with Level 2 vehicle automation, incorporating unique naturalistic and experimental control conditions. Our investigation covered four main areas: automation usage, system warnings, driving demand, and driver arousal, as well as secondary task engagement. While on the interstate, drivers were advised to engage Level 2 automation whenever they deemed it safe, and they complied by using it over 70% of the time. Interestingly, the frequency of system warnings increased with prolonged use, suggesting an evolving relationship between drivers and the automation features. Our data also revealed that drivers were discerning in their use of automation, opting for manual control under high driving demand conditions. Contrary to common safety concerns, our data indicated no significant rise in driver fatigue or fidgeting when using automation, compared to a control condition. Additionally, observed patterns of engagement in secondary tasks like radio listening and text messaging challenge existing assumptions about automation leading to dangerous driver distraction. Overall, our findings provide new insights into the conditions under which drivers opt to use automation and reveal a nuanced behavioral profile that emerges when automation is in use.","The prevalence of vehicle automation is on the rise, making it important to understand driver interactions with this technology and its safety consequences. We conducted a 6-8 week naturalistic study using a hybrid research design to assess driver behavior with Level 2 automation, incorporating both naturalistic and controlled experimental settings. Our investigation focused on four key areas: the use of automation, system warnings, driving demands, and driver arousal, in addition to secondary task engagement. On the interstate, drivers were instructed to enable Level 2 automation whenever they felt it was safe, and they did so over 70% of the time. Notably, the number of system warnings increased with extended use, indicating an evolving relationship between drivers and the automation. Our data also showed that drivers selectively used manual control in high-demand driving situations. Contrary to prevalent safety concerns, our findings did not show a significant increase in driver fatigue or fidgeting when automation was used, compared to a control condition. Additionally, observed patterns of secondary task engagement, such as listening to the radio and text messaging, challenge existing assumptions about automation leading to heightened driver distraction. In summary, our study offers new insights into when drivers choose to use automation and reveals a complex behavioral pattern associated with its use.","The increasing adoption of autonomous vehicles necessitates a thorough examination of how drivers interact with this technology and its impact on road safety. A comprehensive 6-8 week study employing a mixed-methods approach to naturalistic driving research was conducted to assess driver behavior in Level 2 autonomous vehicles, featuring both naturalistic and controlled experimental conditions. The investigation explored four primary domains: automation adoption, warning system efficacy, driving complexity, and driver alertness, as well as secondary task involvement. When driving on highways, participants were encouraged to activate Level 2 autonomy at their discretion, resulting in over 70% usage. Notably, the frequency of warning system alerts increased with prolonged autonomy use, suggesting a dynamic interplay between drivers and autonomous features. The data revealed that drivers exercised discernment in autonomy use, preferring manual control in high-demand driving scenarios. Contrary to prevailing safety concerns, the findings indicated no significant increase in driver fatigue or restlessness when using autonomy compared to a controlled environment. Furthermore, observed patterns of secondary task engagement, such as listening to the radio or texting, challenged common assumptions about autonomy-induced driver distraction. Ultimately, the study's results offer novel insights into the circumstances under which drivers opt for autonomy and uncover a complex behavioral profile that emerges when autonomy is engaged.","The increasing prevalence of vehicle automation necessitates a thorough understanding of driver interaction with this technology and its associated safety implications. Utilizing a hybrid naturalistic driving research methodology, a 6-8 week study was conducted to examine driver behavior in the context of Level 2 vehicle automation, employing a blend of natural driving scenarios and controlled experimental conditions. The investigation focused on four key aspects: automation utilization, system alerts, driving task demands, and driver alertness, along with secondary task engagement. Drivers were instructed to activate Level 2 automation on interstate highways whenever they perceived it as safe, resulting in an adoption rate exceeding 70%. Notably, the frequency of system warnings exhibited an upward trend with extended automation usage, indicating a dynamic interplay between drivers and the automated features. The study revealed that drivers displayed judicious automation usage, choosing manual control in situations characterized by elevated driving demands. Contrary to prevailing safety concerns, the data did not demonstrate a significant increase in driver fatigue or restlessness during automation usage compared to a controlled setting. Furthermore, observed patterns of secondary task engagement, such as radio listening and text messaging, challenge existing hypotheses suggesting that automation fosters hazardous driver distraction. In conclusion, these findings offer valuable insights into the circumstances under which drivers choose to engage automation and unveil a complex behavioral profile that manifests during its utilization."
108,Anomaly detection for early ransomware and spyware warning in nuclear power plant systems based on FusionGuard,"Securing critical infrastructure, particularly nuclear power plants, against emerging cyber threats necessitates innovative cybersecurity approaches. This research introduces FusionGuard, a hybrid machine learning-based anomaly detection system designed for early warnings of ransomware and spyware intrusions within nuclear power plant systems. Meticulously tailored to the unique characteristics of nuclear power plant networks, FusionGuard leverages diverse datasets encompassing normal operational behavior and historical threat data. Through cutting-edge machine learning algorithms, the system dynamically adapts to the network's baseline behavior, effectively identifying deviations indicative of ransomware or spyware activities. Rigorous experimentation and validation using real-world data and simulated attack scenarios affirm FusionGuard's proficiency in detecting anomalous behavior with remarkable accuracy and minimal false positives. The research also explores the system's scalability and adaptability to evolving attack vectors, fortifying the cybersecurity posture of nuclear power plant systems in a dynamic threat landscape. In summary, FusionGuard promises to fortify the security of nuclear power plant systems against ransomware and spyware threats by capitalizing on machine learning and anomaly detection. Serving as a sentinel, the system issues timely alerts and enables proactive responses, contributing substantively to the ongoing discourse on protecting essential systems in high-stakes environments.","Protecting critical infrastructure, especially nuclear power facilities, from new cyber threats requires cutting-edge cybersecurity strategies. This study presents FusionGuard, a hybrid anomaly detection system based on machine learning, created to provide early warnings of ransomware and spyware intrusions in nuclear power plant networks. Custom-designed to suit the specific features of these networks, FusionGuard utilizes varied datasets that capture normal operations and historical threats. Employing advanced machine learning algorithms, the system dynamically conforms to the network's typical behavior, effectively detecting deviations that signal ransomware or spyware activities. Thorough testing and validation with real-world data and simulated attacks demonstrate FusionGuard's capability to identify anomalies with high precision and low false positive rates. The research also examines the system's scalability and adaptability to changing attack methods, strengthening the cybersecurity framework of nuclear power plants in an evolving threat environment. Essentially, FusionGuard aims to enhance the security of nuclear power plant systems against ransomware and spyware by leveraging machine learning and anomaly detection, providing timely alerts and facilitating proactive measures, thus significantly contributing to the protection of critical infrastructure in high-stakes settings.","""Protecting vital infrastructure, such as nuclear power plants, from burgeoning cyber threats demands pioneering cybersecurity strategies. This study presents FusionGuard, a novel, adaptive anomaly detection system that harnesses machine learning to provide early warnings of ransomware and spyware breaches within nuclear power plant networks. Carefully calibrated to the distinct attributes of these networks, FusionGuard aggregates diverse datasets capturing normal operational patterns and historical threat intelligence. By employing advanced machine learning techniques, the system continuously refines its understanding of the network's baseline behavior, thereby accurately pinpointing aberrations indicative of malicious activities. Exhaustive testing and validation using real-world data and simulated attack scenarios confirm FusionGuard's exceptional ability to detect anomalies with high precision and minimal false alarms. Furthermore, the research examines the system's capacity to scale and adapt to emerging attack vectors, thereby bolstering the cybersecurity resilience of nuclear power plant systems in an ever-evolving threat environment. Ultimately, FusionGuard offers a robust solution to safeguard nuclear power plant systems against ransomware and spyware threats by leveraging machine learning and anomaly detection, providing timely alerts and enabling proactive responses that significantly contribute to the ongoing discussion on securing critical systems in high-risk settings.""","The evolving cyber threat landscape demands novel cybersecurity solutions to protect vital infrastructure, especially nuclear facilities. This study presents FusionGuard, a sophisticated anomaly detection system employing hybrid machine learning techniques to provide early warnings of ransomware and spyware attacks targeting nuclear power plant systems. Specifically designed for the intricacies of these networks, FusionGuard utilizes diverse datasets, including normal operational data and historical threat information. Advanced machine learning algorithms allow the system to continuously adapt to the network's baseline, effectively identifying deviations suggestive of malicious activities. Rigorous testing and validation, using both real-world data and simulated attack scenarios, demonstrate FusionGuard's exceptional ability to detect anomalous behavior with high accuracy and minimal false alarms. The research further examines the system's scalability and capacity to adapt to evolving attack vectors, bolstering the cybersecurity posture of nuclear power plant systems within a dynamic threat environment. Ultimately, FusionGuard offers a robust defense against ransomware and spyware threats by harnessing the power of machine learning and anomaly detection. Acting as a vigilant watchdog, the system provides timely alerts, enabling proactive responses and significantly contributing to the ongoing dialogue on safeguarding critical systems in high-risk environments."
162,Recommender Systems in Cybersecurity,"With the growth of CyberTerrorism, enterprises worldwide have been struggling to stop intruders from obtaining private data. Despite the efforts made by Cybersecurity experts, the shortage of skillful security teams and the usage of intelligent attacks have slowed down the enhancement of defense mechanisms. Furthermore, the pandemic in 2020 forced organizations to work in remote environments with poor security, leading to increased cyberattacks. One possible solution for these problems is the implementation of Recommender Systems to assist Cybersecurity human operators. Our goal is to survey the application of Recommender Systems in Cybersecurity architectures. These decision-support tools deal with information overload through filtering and prioritization methods, allowing businesses to increase revenue, achieve better user satisfaction, and make faster and more efficient decisions in various domains (e-commerce, healthcare, finance, and other fields). Several reports demonstrate the potential of using these recommendation structures to enhance the detection and prevention of cyberattacks and aid Cybersecurity experts in treating client incidents. This survey discusses several studies where Recommender Systems are implemented in Cybersecurity with encouraging results. One promising direction explored by the community is using Recommender Systems as attack predictors and navigation assistance tools. As contributions, we show the recent efforts in this area and summarize them in a table. Furthermore, we provide an in-depth analysis of potential research lines. For example, the inclusion of Recommender Systems in security information event management systems and security orchestration, automation, and response applications could decrease their complexity and information overload.","As CyberTerrorism expands, businesses globally are grappling with thwarting unauthorized access to confidential information. Although Cybersecurity professionals have exerted considerable effort, a lack of skilled security personnel and the deployment of sophisticated attacks have hindered advancements in defense strategies. Additionally, the 2020 pandemic compelled organizations to operate remotely with inadequate security, resulting in a surge of cyberattacks. One potential remedy to these challenges is the adoption of Recommender Systems to support Cybersecurity personnel. Our objective is to review the use of Recommender Systems within Cybersecurity frameworks. These decision-support mechanisms manage information overload through filtering and prioritization techniques, enabling companies to boost revenue, enhance user satisfaction, and make quicker, more effective decisions across various sectors such as e-commerce, healthcare, and finance. Several studies highlight the promise of employing these recommendation systems to improve cyberattack detection and prevention, as well as to assist Cybersecurity experts in managing client incidents. This review examines various research where Recommender Systems have been successfully incorporated into Cybersecurity with positive outcomes. A notable direction investigated by the field is the application of Recommender Systems as tools for predicting attacks and facilitating navigation. Our contributions include presenting recent advances in this domain and summarizing them in a table. Additionally, we offer a comprehensive analysis of potential research directions. For instance, integrating Recommender Systems into security information and event management systems and security orchestration, automation, and response applications could reduce their complexity and information burden.","As the menace of CyberTerrorism escalates, global enterprises are grappling to thwart unauthorized access to sensitive information. Despite the endeavors of Cybersecurity specialists, the scarcity of adept security professionals and the employment of sophisticated attack vectors have hindered the advancement of defensive countermeasures. The 2020 pandemic further exacerbated the issue, as organizations were compelled to operate in remote settings with inadequate security infrastructure, thereby precipitating a surge in cyberattacks. A potential panacea for these challenges lies in the integration of Recommender Systems to augment the capabilities of human Cybersecurity operators. Our objective is to conduct a comprehensive review of the applications of Recommender Systems within Cybersecurity frameworks. These decision-support tools mitigate information overload by leveraging filtering and prioritization techniques, thereby enabling businesses to boost revenue, enhance user experience, and make swift and informed decisions across diverse domains, including e-commerce, healthcare, finance, and others. Various reports have demonstrated the efficacy of these recommendation structures in bolstering the detection and prevention of cyberattacks, as well as facilitating the incident response capabilities of Cybersecurity experts. This survey examines multiple studies that have successfully implemented Recommender Systems in Cybersecurity, yielding promising outcomes. A notable area of exploration involves the utilization of Recommender Systems as predictive tools for attack forecasting and navigational aids. Our contributions include a summary of recent advancements in this field, presented in a tabular format, as well as an in-depth examination of potential avenues for future research, such as the incorporation of Recommender Systems into security information event management systems and security orchestration, automation, and response applications to alleviate complexity and information overload.","The rise of cyber threats poses a significant challenge for organizations globally, as they strive to safeguard sensitive information. Despite the expertise of cybersecurity professionals, the scarcity of skilled security personnel and the increasing sophistication of cyberattacks have hindered the development of robust defense strategies. The shift to remote work environments during the 2020 pandemic, often characterized by inadequate security measures, further exacerbated the vulnerability to cyberattacks. A promising approach to address these challenges lies in the integration of recommender systems to support cybersecurity experts. This review explores the implementation of recommender systems within cybersecurity frameworks. These decision-support tools effectively manage information overload through filtering and prioritization techniques, empowering businesses across various sectors, including e-commerce, healthcare, and finance, to enhance revenue, improve user satisfaction, and expedite decision-making processes. Numerous studies highlight the potential of recommender systems in bolstering cyberattack detection and prevention efforts, providing valuable assistance to cybersecurity professionals in handling security incidents. This survey examines various research endeavors where recommender systems have been successfully applied in cybersecurity, demonstrating promising outcomes. Notably, the use of recommender systems for attack prediction and navigation assistance has emerged as a significant area of exploration within the research community. This review provides a comprehensive overview of recent advancements in this field, summarizing key findings in a table format. Additionally, it delves into potential avenues for future research, such as the integration of recommender systems into security information and event management systems and security orchestration, automation, and response applications, aiming to mitigate their complexity and address information overload challenges."
19,Virtual reality headsets for perimetry testing: a systematic review,"Standard automated perimetery is considered the gold standard for evaluating a patient’s visual field. However, it is costly and requires a fixed testing environment. In response, perimetric devices using virtual reality (VR) headsets have emerged as an alternative way to measure visual fields in patients. This systematic review aims to characterize both novel and established VR headsets in the literature and explore their potential applications within visual field testing. A search was conducted using MEDLINE, Embase, CINAHL, and the Core Collection (Web of Science) for articles published until January 2023. Subject headings and keywords related to virtual reality and visual field were used to identify studies specific to this topic. Records were first screened by title/abstract and then by full text using predefined criteria. Data was extracted accordingly. A total of 2404 records were identified from the databases. After deduplication and the two levels of screening, 64 studies describing 36 VR headset perimetry devices were selected for extraction. These devices encompassed various visual field measurement techniques, including static and kinetic perimetry, with some offering vision rehabilitation capabilities. This review reveals a growing consensus that VR headset perimetry devices perform comparably to, or even better than, standard automated perimetry. They are better tolerated by patients in terms of gaze fixation, more cost-effective, and generally more accessible for patients with limited mobility.","Standard automated perimetry is widely regarded as the benchmark for assessing a patient's visual field. Nonetheless, it is expensive and necessitates a stationary testing setting. Consequently, virtual reality (VR) headset-based perimetric devices have been developed as an alternative method for evaluating visual fields in patients. This systematic review seeks to describe both innovative and established VR headsets discussed in the literature and examine their potential uses in visual field assessment. A thorough search was performed using MEDLINE, Embase, CINAHL, and the Core Collection (Web of Science) for articles available up to January 2023. Subject headings and keywords related to virtual reality and visual fields were employed to locate relevant studies. Titles and abstracts were initially screened, followed by full texts, based on pre-established criteria. Data were subsequently extracted. Out of 2404 records retrieved from the databases, 64 studies detailing 36 VR headset perimetry devices were chosen after deduplication and two screening rounds. These devices used various visual field measurement methods, including static and kinetic perimetry, and some also featured vision rehabilitation functions. This review identifies a growing agreement that VR headset perimetry devices perform on par with or superior to standard automated perimetry. They are better accepted by patients in terms of gaze fixation, more cost-effective, and typically more accessible for patients with limited mobility.","The assessment of a patient's visual field is typically conducted using conventional automated perimetry, regarded as the benchmark for this purpose. However, this method is hindered by its high cost and requirement for a fixed testing setting. As a result, innovative perimetric tools utilizing virtual reality (VR) technology have emerged as a viable alternative for measuring visual fields. This systematic review seeks to provide a comprehensive overview of both cutting-edge and established VR headsets in the existing literature, exploring their potential uses in visual field assessment. A thorough search of prominent databases, including MEDLINE, Embase, CINAHL, and the Core Collection (Web of Science), was conducted to identify relevant articles published up to January 2023. Studies specific to this topic were identified using relevant subject headings and keywords related to virtual reality and visual field. Following a rigorous screening process, 64 studies describing 36 VR headset perimetry devices were selected for analysis. These devices employ diverse visual field measurement techniques, including static and kinetic perimetry, with some offering additional vision rehabilitation capabilities. The findings of this review suggest a growing agreement that VR headset perimetry devices perform equally well as, or even surpass, standard automated perimetry in terms of accuracy, while offering advantages such as improved patient tolerance, cost-effectiveness, and increased accessibility for patients with mobility limitations.","While standard automated perimetry remains the benchmark for assessing patients' visual fields, its high cost and reliance on a stationary setting have spurred the development of virtual reality (VR) headset-based perimetric devices as a viable alternative. This systematic review sought to analyze both established and novel VR headsets described in scientific literature, investigating their potential for visual field assessment. A comprehensive search encompassing MEDLINE, Embase, CINAHL, and Web of Science's Core Collection yielded 2404 records published up to January 2023, using keywords and subject headings pertaining to both virtual reality and visual field testing. After removing duplicates and conducting a two-stage screening process (title/abstract and full text) based on predetermined criteria, 64 studies detailing 36 distinct VR headset perimetry devices were ultimately selected for data extraction. These devices utilized a range of visual field measurement techniques, including static and kinetic perimetry, with some even incorporating vision rehabilitation functionalities. This review highlights a growing body of evidence suggesting that VR headset perimetry devices demonstrate performance comparable to, if not exceeding, standard automated perimetry. Furthermore, they offer improved patient tolerance regarding gaze fixation, greater cost-effectiveness, and enhanced accessibility for individuals with mobility limitations."
149,The impact of automation and optimization on customer experience: a consumer perspective,"The adoption of digital technologies has significantly transformed businesses and society as a whole. The automation of tasks is leading to changes in organizational structures and strategies. Due to technological growth, users are able to identify the benefits and risks that technology can entail in the purchasing process. Specifically, robotic process automation (RPA) can improve efficiency and agility in a company, which in turn can positively impact consumer satisfaction and engagement. However, automation can also negatively affect the consumer experience and service quality if not applied correctly. Therefore, this research focuses on analyzing the impact of automation technologies on purchasing processes and consumer satisfaction. For this purpose, a survey was developed by means of the Likert 5-point scale, which allowed for obtaining 215 valid responses from consumers in the Community of Madrid. The data were processed through the SPSS tool, which enabled the analysis of the data and the proposed model. Consequently, the results show that potential RPA-based automation and optimization of processes can be of great utility for businesses to better address investment for improving consumer satisfaction. In addition, it should be highlighted that this research contributes in an original way to the area of information and communication technologies by allowing for the development of proactive technological implementation plans that consider end-user satisfaction.","The integration of digital technologies has profoundly altered both businesses and society. Task automation is prompting modifications in organizational structures and strategies. As technology advances, users can discern the advantages and risks technology may bring in the purchasing process. Specifically, robotic process automation (RPA) has the potential to enhance a company's efficiency and adaptability, which can positively influence customer satisfaction and engagement. However, if not implemented correctly, automation may adversely affect the consumer experience and service quality. This study aims to examine the effects of automation technologies on purchasing processes and consumer satisfaction. To this end, a survey utilizing a Likert 5-point scale was conducted, yielding 215 valid responses from consumers in the Community of Madrid. The data were analyzed using the SPSS tool, facilitating the evaluation of the data and the proposed model. The findings indicate that RPA-based process automation and optimization can significantly benefit businesses in improving consumer satisfaction. Furthermore, this research makes a unique contribution to the field of information and communication technologies by enabling the formulation of proactive technology implementation plans that prioritize end-user satisfaction.","The integration of digital solutions has brought about a profound paradigm shift in the business landscape and society at large, prompting a reevaluation of organizational frameworks and strategies. As technological advancements continue to accelerate, consumers are increasingly aware of the benefits and drawbacks associated with technology in the purchasing process. Notably, the strategic deployment of robotic process automation (RPA) can yield significant gains in operational efficiency and adaptability, ultimately enhancing customer satisfaction and loyalty. However, if misapplied, automation can have detrimental effects on the customer experience and service quality. This study seeks to investigate the influence of automation technologies on purchasing processes and customer satisfaction, relying on a 5-point Likert scale survey that garnered 215 valid responses from consumers in the Community of Madrid. The data analysis, facilitated by the SPSS tool, revealed that the judicious implementation of RPA-based automation and process optimization can be a valuable asset for businesses seeking to optimize investments in customer satisfaction. Furthermore, this research makes a novel contribution to the field of information and communication technologies by enabling the development of forward-thinking technological implementation plans that prioritize end-user satisfaction.","The integration of digital tools has revolutionized both commerce and society at large. This shift towards automated processes necessitates adaptations in organizational frameworks and plans. Technological advancements empower users to discern both the advantages and pitfalls of technology in their purchasing decisions. Robotic process automation (RPA), for instance, can enhance a company's efficiency and responsiveness, ultimately boosting customer contentment and engagement. However, if implemented incorrectly, automation can also negatively impact the consumer experience and service delivery. This study, therefore, investigates the influence of automation technologies on purchasing processes and customer satisfaction. A five-point Likert scale survey was administered, generating 215 valid responses from consumers in the Community of Madrid. Data analysis was facilitated by SPSS, enabling the examination of the data and the proposed model. The findings indicate that harnessing the potential of RPA-driven automation and process optimization can be highly beneficial for businesses seeking to optimize investments aimed at enhancing customer satisfaction. Furthermore, this research makes a novel contribution to the field of information and communication technologies by facilitating the creation of proactive technological implementation strategies that prioritize end-user satisfaction."
70,A thorough review of the existing concentrated solar power technologies and various performance enhancing techniques,"Solar thermal power plants today are the most viable alternative to replace conventional thermal power plants to successfully combat climate change and global warming. In this paper, the reasons behind this imminent and inevitable transition and the advantages of solar thermal energy over other renewable sources including solar PV have been discussed. The current literature on different types of solar thermal power plants and their performance optimisation techniques is quite scattered. Efforts have been made in this paper to bring the scattered information together in one thorough review so that it helps researchers across the spectrum undertaking studies on concentrated solar power technologies. This paper has established a brief background of these technologies. Furthermore, it has put forth a comprehensive review of different concentrated solar power technologies implemented throughout the world. The review in its latter part has highlighted the current trends of various hybrid, performance enhancing techniques being employed with these technologies. A brief review of the importance of economic analysis of these technologies has also been done. The future scope and course of action adopted to keep this technology growing are also discussed.","Modern solar thermal power plants represent a highly feasible alternative to conventional thermal power stations, playing a critical role in combating climate change and global warming. This paper delves into the underlying reasons for this imminent and unavoidable shift, highlighting the benefits of solar thermal energy compared to other renewable energy sources, including solar photovoltaics (PV). The current body of literature on various solar thermal power plant types and their optimization strategies is fragmented. This paper consolidates this dispersed information into a comprehensive review, aiming to assist researchers focused on concentrated solar power (CSP) technologies. It provides an overview of these technologies, alongside a detailed examination of different CSP technologies deployed worldwide. The latter section of the review showcases current trends in hybrid techniques and performance enhancement methods utilized with these technologies. Additionally, a succinct review of the economic evaluation importance for these technologies is presented. The discussion also touches on future perspectives and strategies to ensure the continued advancement of this technology.","Here is a paraphrased version of the original text, exhibiting both syntactic and lexical diversity while preserving semantic similarity: ""Conventional thermal power plants are on the cusp of being supplanted by solar thermal power plants, a development crucial for mitigating the pressing issues of climate change and global warming. This study delves into the rationale behind this impending paradigm shift and the benefits of solar thermal energy relative to other renewable sources, including solar photovoltaic systems. Despite the abundance of literature on various solar thermal power plant designs and optimization strategies, a comprehensive synthesis of this knowledge has been lacking. This paper seeks to bridge this gap by providing an exhaustive review of concentrated solar power technologies, encompassing their fundamental principles, global implementations, and cutting-edge hybrid approaches aimed at enhancing performance. Additionally, the importance of economic viability assessments for these technologies is underscored, and the future trajectory and necessary steps for their continued growth are outlined.""","The urgent need to mitigate climate change necessitates a shift from traditional thermal power plants to sustainable alternatives, with solar thermal power emerging as a frontrunner. This article delves into the rationale driving this transition and illuminates the superiorities of solar thermal energy compared to other renewable energy sources, including photovoltaic solar. Existing research on diverse solar thermal power plant types and their optimization strategies is currently fragmented. This paper endeavors to consolidate this dispersed knowledge, providing a comprehensive review aimed at assisting researchers exploring concentrated solar power technologies. After establishing a foundational understanding of these technologies, the paper presents an extensive overview of diverse concentrated solar power technologies deployed globally. It further examines contemporary trends in hybrid and performance-enhancing techniques integrated with these technologies. The economic implications of these technologies are also briefly analyzed. Finally, the paper explores future directions and outlines steps required to foster the continued growth of this vital technology."
55,"Invasiveness, Intrusiveness and Influence: three new metrics to measure communication between political digital echo chambers","The contemporary debate about the place and role of political digital echo chambers (DPECs) in political communication relies more on assumptions, guess work, and speculations rather than empirical conclusions. Such shortcomings reflect the lack of empirical tools to measure the communication between echo chambers and the outside world. We try to overcome this deficiency by construing three graph-level metrics: Invasiveness, Intrusiveness and Influence, which try to capture the information dominance of a DPEC over another one, its strength of information source, and the penetration capability of one DPEC’s message into another DPEC’s space. We tested our metrics with simulated and real network data, and they seem to respond according to their design and our expectations. Test results on real network data showed that our metrics would be very useful in measuring the comparative strength of political mobilization in face of opposing forces that use the same social networks for political countermobilization.","The current discussion on the significance and function of political digital echo chambers (DPECs) in political communication is primarily grounded in assumptions, guesswork, and speculations rather than empirical evidence. This deficit highlights the absence of empirical methods to assess the interaction between these echo chambers and external environments. To address this gap, we have developed three graph-level metrics: Invasiveness, Intrusiveness, and Influence. These metrics aim to measure the information dominance of one DPEC over another, its role as an information source, and its ability to infiltrate another DPEC’s space. We validated our metrics using both simulated and real network data, and they performed in line with their intended purpose and our predictions. Results from real network data indicated that our metrics are highly valuable for assessing the relative strength of political mobilization in the face of adversarial forces utilizing the same social networks for political counter-mobilization.","The ongoing discussion surrounding the significance and function of political digital echo chambers (DPECs) in the realm of political discourse is largely founded upon unsubstantiated claims, conjecture, and speculation, rather than being grounded in empirical evidence. This limitation stems from the absence of reliable methodologies for assessing the exchange of information between echo chambers and the external environment. To address this knowledge gap, we propose the development of three novel graph-level metrics – namely, Infiltration, Informational Authority, and Message Permeability – designed to quantify the informational supremacy of one DPEC over another, the potency of its information sources, and the capacity of its messages to permeate the boundaries of rival DPECs. Our metrics were validated through simulations and real-world network data, yielding results that align with their intended purpose and our expectations. Furthermore, the application of these metrics to real-world network data revealed their potential to effectively gauge the relative efficacy of political mobilization efforts in the face of countermobilization strategies employed by opposing forces within the same social media platforms.","Current discussions surrounding the influence and function of political digital echo chambers (DPECs) in political discourse often rely on conjecture and speculation rather than concrete evidence. This deficiency stems from a lack of effective empirical tools for analyzing interactions between echo chambers and the external world. To address this gap, we propose three novel graph-based metrics – Invasiveness, Intrusiveness, and Influence – designed to assess the informational dominance of one DPEC over another, its strength as an information source, and the extent to which its messaging penetrates other echo chamber environments. Our evaluation of these metrics using both simulated and real-world network data indicates that they perform as intended and align with our expectations. Real-world network data analysis suggests that these metrics could be highly valuable for gauging the relative effectiveness of political mobilization efforts against opposing forces leveraging the same social networks for counter-mobilization."
200,The Cost and Benefit of Regional Cultural Diversity on the Income of Rural Workers: Evidence from China,"This paper studies the impact of regional cultural diversity of working cities on the income of rural workers. Using a nationally representative micro-dataset in China, and adopting dialect diversity at the city level as the indicator of regional cultural diversity, we find that regional cultural diversity has a highly significant hump-shaped effect on rural workers' incomes from the wage employment and self-employment, reflecting a trade-off between the beneficial and the detrimental effects of regional cultural diversity on rural workers' incomes. The results remain robust to the inclusion of a set of covariates, alternative estimation technique, alternative measure of regional cultural diversity, and a placebo test. We also find that the detrimental effect of diversity is weaker for high-educated rural workers, and the impact of regional cultural diversity remains stable in the short term. To address potential endogeneity, we employ an instrumental variable approach by using the river length of cities to instrument the dialect diversity, and the instrumented results confirm the conclusions. In addition, we provide empirical evidence that the cost and benefit of regional cultural diversity on the income of rural workers can be attributed to reduced social capital and increased creativity.","This study examines how the cultural diversity of urban areas affects the earnings of rural laborers. Drawing on a comprehensive micro-dataset from China and using city-level dialect diversity as a measure of regional cultural diversity, we discover that this diversity has a statistically significant hump-shaped influence on the incomes of rural workers involved in both wage employment and self-employment. This indicates a balance between the positive and negative impacts of cultural diversity on their earnings. The findings are robust even when accounting for various covariates, alternative estimation methods, different measures of cultural diversity, and a placebo test. Additionally, we observe that the adverse effect of diversity is less pronounced for rural workers with higher education, and the impact of cultural diversity remains consistent in the short term. To mitigate potential endogeneity, we apply an instrumental variable approach using city river length as an instrument for dialect diversity, with instrumented results corroborating our conclusions. Our empirical evidence also suggests that the costs and benefits of regional cultural diversity on rural workers' incomes can be linked to decreased social capital and enhanced creativity.","This research examines how the cultural heterogeneity of urban centers in a region affects the earnings of rural laborers, utilizing a comprehensive micro-dataset from China and measuring cultural diversity through dialectal variation at the city level. The findings reveal a significant, nonlinear relationship between regional cultural diversity and rural workers' income from both wage and self-employment, suggesting a balance between the positive and negative consequences of cultural diversity on rural incomes. The results withstand the inclusion of control variables, alternative estimation methods, and alternative cultural diversity metrics, as well as a placebo test. Furthermore, the analysis shows that the adverse effects of diversity are mitigated for highly educated rural workers, and the impact of cultural diversity remains consistent in the short term. To address potential biases, an instrumental variable approach is employed, using city river lengths to proxy dialect diversity, and the results confirm the conclusions. The study also provides empirical evidence that the costs and benefits of regional cultural diversity on rural workers' income can be attributed to decreased social cohesion and enhanced innovation.","This study investigates how the cultural variety within cities where rural workers seek employment affects their earnings. Utilizing a comprehensive Chinese micro-dataset and employing dialect diversity as a proxy for cultural richness, the research reveals a notable non-linear relationship between cultural diversity and rural worker income, regardless of employment type. This suggests a balance between the advantages and disadvantages of such diversity. Rigorous testing with various statistical methods, alternative diversity measures, and placebo tests validate these findings. Further analysis indicates that well-educated rural workers are less susceptible to the negative impacts of diversity, and the observed effects persist over time. To mitigate potential biases, the study uses an instrumental variable approach, leveraging river length as a proxy for dialect diversity, with consistent results. Finally, empirical evidence is presented that links the economic implications of regional cultural diversity to its influence on social capital and creative output."
129,Mitochondrial gene editing,"Mutations in mitochondrial DNA (mtDNA) are responsible for several severe diseases that have no available cures. The multicopy nature of the mitochondrial genome means that mutations often exist in a state known as heteroplasmy, where both mutant and wild-type mtDNA are present in the same cell. The wild-type mtDNA can functionally compensate for the mutant mtDNA until a mutation threshold is reached, beyond which disease symptoms begin to manifest. Despite the interest mitochondrial genetics has generated, the double mitochondrial membrane proved to be a formidable barrier to genetic manipulation. However, in the past two decades, scientists have discovered that mtDNA could be modified by importing gene editing proteins to target specific DNA sequences. Mitochondria-targeted nucleases specifically cleave and eliminate mutant mtDNA in heteroplasmic cells and in animal models. More recently, base editors have been adapted to modify mtDNA via precise C>T or A>G transitions. Therefore, tools to modify mtDNA are, finally, a reality with the promise to revolutionize the mitochondrial genetics field. This Primer delves into mitochondrial gene editing, providing details on the selection of mitochondrial gene editing tools, best practices for designing experiments, relevant types of analyses and specific applications and limitations pertaining to the different technologies and the field.","Mutations in mitochondrial DNA (mtDNA) cause numerous severe diseases for which there are currently no cures. Due to the mitochondrial genome's multicopy characteristic, these mutations typically occur in a condition termed heteroplasmy, where both mutant and normal mtDNA coexist within a single cell. Normal mtDNA can offset the function of mutant mtDNA until a certain mutation threshold is surpassed, leading to the onset of disease symptoms. Although mitochondrial genetics has captivated significant interest, the dual mitochondrial membrane posed a significant challenge to genetic modification. Nevertheless, over the past twenty years, researchers have identified methods to alter mtDNA by importing gene-editing proteins that target particular DNA sequences. Mitochondria-targeted nucleases can specifically cut and remove mutant mtDNA in cells with heteroplasmy and in animal models. More recently, base editors have been adapted to make exact C>T or A>G changes in mtDNA. Consequently, tools for mtDNA modification have become a reality, holding significant potential to transform the field of mitochondrial genetics. This Primer explores mitochondrial gene editing, detailing the selection of gene-editing tools for mitochondria, optimal practices for planning experiments, types of analyses to conduct, and specific uses and limitations of the various technologies in the field.","""Alterations to the mitochondrial DNA (mtDNA) genome are the root cause of numerous debilitating diseases that currently lack effective treatments. The presence of multiple copies of the mitochondrial genome leads to a phenomenon known as heteroplasmy, where cells harbor both normal and mutated mtDNA. While normal mtDNA can functionally counterbalance the effects of mutated mtDNA until a critical threshold is exceeded, beyond which disease symptoms emerge. Despite the significant attention devoted to mitochondrial genetics, the double membrane surrounding mitochondria has long posed a significant obstacle to genetic modification. However, breakthroughs in the past twenty years have enabled the manipulation of mtDNA through the introduction of gene editing proteins that target specific DNA sequences. Mitochondria-targeted nucleases have been shown to selectively eliminate mutated mtDNA in cells and animal models exhibiting heteroplasmy. Furthermore, base editors have been adapted to induce precise C>T or A>G transitions in mtDNA. As a result, the long-awaited tools for modifying mtDNA have finally become a reality, holding immense promise for transforming the field of mitochondrial genetics. This Primer provides an in-depth exploration of mitochondrial gene editing, covering the selection of optimal gene editing tools, experimental design best practices, relevant analytical approaches, and the applications, limitations, and challenges associated with these technologies and the field as a whole.""","Alterations in the genetic material of mitochondria are linked to numerous serious illnesses for which there are currently no effective treatments. The presence of multiple copies of the mitochondrial genome often results in a situation called heteroplasmy, where both normal and altered mitochondrial DNA coexist within a single cell. The normal DNA can compensate for the dysfunctional DNA until a certain level of mutation is reached, at which point disease symptoms emerge. Despite significant scientific interest in mitochondrial genetics, the double membrane surrounding mitochondria posed a substantial obstacle to genetic modification. Nevertheless, in recent decades, researchers have found ways to alter mitochondrial DNA by delivering gene editing proteins that target specific DNA sequences. Specifically, mitochondria-targeted enzymes can cut and remove mutated mitochondrial DNA in cells containing both normal and mutated DNA, as well as in animal models. More recently, base editors have been modified to precisely change specific DNA bases within mitochondria. As a result, tools for modifying mitochondrial DNA are now available, holding the potential to transform the field of mitochondrial genetics. This comprehensive overview explores mitochondrial gene editing, providing information on choosing appropriate tools, best practices for experimental design, relevant analysis methods, and the specific applications and limitations associated with various technologies in this field."
40,The Mediating Effect of Self-Efficacy in the Relationship Between Parental Communication and Digital Addiction,"In recent years, the excessive use of digital technologies has led to the emergence of digital addiction at earlier ages. However, studies explaining the causes of digital addiction are limited. The purpose of the current study was to investigate the links between maternal communication, paternal communication, and digital addiction, and whether these relationships were mediated by self-efficacy. It was also tested whether these links were moderated by gender. The data were collected from 601 middle school students aged 11–14 (61.7% girls, Mage = 12.23 SD = 1.03). The participants completed the Digital Addiction Scale for Children, Adolescent Family Process Measure, General Self-Efficacy Scale, and Demographic Questionnaire. Study hypotheses were tested by structural equation modeling. Findings showed that paternal communication was directly associated with self-efficacy, and also indirectly with digital addiction, mediated by self-efficacy. However, maternal communication was only directly associated with digital addiction. Moderation tests by gender were significant. The indirect effect of maternal communication on digital addiction was significant only for girls. The results suggest that the development of programs that will support parents' communication with their children and increase children's self-efficacy may be beneficial in preventing digital addiction. The findings may also help mental health professionals to understand the effects of parent–child communication on children's digital addiction.","In recent years, the overuse of digital technologies has resulted in digital addiction emerging at younger ages. However, there are limited studies explaining the causes of digital addiction. This study aimed to explore the connections between communication with mothers and fathers and digital addiction, and whether self-efficacy mediated these relationships. Additionally, the study examined if gender moderated these links. Data were gathered from 601 middle school students aged 11–14 (61.7% girls, average age 12.23, SD 1.03). Participants completed the Digital Addiction Scale for Children, Adolescent Family Process Measure, General Self-Efficacy Scale, and a Demographic Questionnaire. Hypotheses were tested using structural equation modeling. Results indicated that paternal communication was directly linked to self-efficacy and indirectly to digital addiction via self-efficacy. Conversely, maternal communication was only directly linked to digital addiction. Gender moderation tests were significant; the indirect effect of maternal communication on digital addiction was significant only for girls. These results imply that developing programs to enhance parent-child communication and boost children's self-efficacy could help prevent digital addiction. The findings could also assist mental health professionals in understanding the impact of parent-child communication on children’s digital addiction.","Over the past few years, the rampant utilization of digital technologies has precipitated the onset of digital dependency at increasingly younger ages. Despite this, research elucidating the underlying causes of digital dependency remains scarce. The present investigation aimed to explore the correlations between maternal and paternal communication patterns, digital dependency, and the potential mediating role of self-efficacy, as well as the moderating influence of gender. A sample of 601 middle school students (61.7% female, mean age 12.23, SD 1.03) completed a battery of assessments, including the Digital Addiction Scale for Children, Adolescent Family Process Measure, General Self-Efficacy Scale, and Demographic Questionnaire. Structural equation modeling was employed to test the study's hypotheses. The results revealed that paternal communication exhibited a direct association with self-efficacy, which, in turn, was indirectly linked to digital dependency. In contrast, maternal communication was found to have a direct, yet distinct, relationship with digital dependency. Notably, gender moderation tests yielded significant results, with the indirect effect of maternal communication on digital dependency emerging as significant only among female participants. These findings suggest that initiatives aimed at enhancing parental communication and fostering self-efficacy in children may be instrumental in mitigating digital dependency, while also providing valuable insights for mental health professionals seeking to understand the complex interplay between parent-child communication and digital dependency in young people.","The increasing prevalence of digital addiction in younger populations due to widespread digital technology use has prompted research into its underlying causes, yet such studies remain scarce. This study aimed to examine the connections between maternal and paternal communication styles and digital addiction, exploring the mediating role of self-efficacy and potential gender-based moderation effects. Data from 601 middle school students (aged 11-14, with a majority being girls) were collected using various scales measuring digital addiction, family dynamics, self-efficacy, and demographics. Structural equation modeling revealed that paternal communication directly influenced self-efficacy, which, in turn, indirectly affected digital addiction. Conversely, maternal communication only showed a direct link to digital addiction. Notably, gender played a moderating role, with the indirect influence of maternal communication on digital addiction being significant only among girls. These findings underscore the potential value of interventions aimed at enhancing parental communication skills and bolstering children's self-efficacy as preventive measures against digital addiction. Additionally, these insights can contribute to a deeper understanding of the impact parent-child communication has on digital addiction within the mental health field."
29,A scalable blockchain storage scheme for VANET,"Intelligent transportation systems (ITSs) make it possible for applications such as autonomous driving, active safety systems and smart cities. As the infrastructure of ITSs, vehicular ad-hoc network (VANET) plays a key role in ensuring traffic safety while improving driving experience and comfort. However, VANET faces many challenges as the network is exposed to the public and involves sensitive information such as vehicle control commands and driving records. Although blockchain technology can provide such features as decentralization, immutability, non-reliance on trust and traceability, most of the existing blockchain systems impose high storage and computing capacity requirements for participating nodes while terminals nodes in VANET, such as road-side units (RSUs), on-board units (OBUs) and sensors, usually have very limited storage and computing capacity. To solve this problem, this paper proposes a blockchain system that provides scalable storage capacity for VANETs. The proposed scheme uses network sharding and multi-consensus strategy to improve the topology and consensus process of the blockchain. Aimed at lowering the storage requirement on the RSU nodes that participate in the blockchain, a collaborative storage mechanism and a dynamic copy number strategy for the blockchain ledger is designed. Experimental results show that compared to existing schemes, the proposed scalable blockchain storage scheme can significantly lower the storage requirement for the blockchain nodes, thus making it possible for the nodes with limited storage capacity in VANET to participate in the maintenance of the blockchain to prevent the blockchain from drifting towards centralization, and support maximal sharing of road traffic information while ensuring the security, privacy and trustworthiness of information.","Intelligent transportation systems (ITSs) enable technologies like autonomous driving, active safety systems, and smart cities. Vehicular ad-hoc network (VANET), as the backbone of ITSs, is crucial for traffic safety and enhancing driving comfort and experience. However, VANET encounters several challenges due to its public exposure and the sensitive nature of the information it handles, such as vehicle control commands and driving logs. While blockchain technology offers benefits like decentralization, immutability, trustlessness, and traceability, most current blockchain implementations demand substantial storage and computational power from participating nodes. This poses a problem for VANET's terminal nodes, such as road-side units (RSUs), on-board units (OBUs), and sensors, which typically have limited storage and computational capabilities. To address this issue, this paper presents a blockchain system designed to provide scalable storage capacity for VANETs. The proposed approach utilizes network sharding and a multi-consensus strategy to enhance the blockchain's topology and consensus process. A collaborative storage mechanism combined with a dynamic copy number strategy for the blockchain ledger is implemented to reduce the storage requirements on RSU nodes. Experimental results indicate that, compared to existing solutions, the proposed scalable blockchain storage scheme drastically reduces the storage demands on blockchain nodes, enabling those with limited storage capacity in VANET to participate in blockchain maintenance. This prevents the blockchain from becoming centralized and supports the extensive sharing of road traffic information while ensuring the security, privacy, and trustworthiness of the data.","The advent of intelligent transportation systems (ITSs) has paved the way for innovative applications like autonomous vehicles, advanced safety features, and smart urban infrastructure. At the heart of ITSs lies vehicular ad-hoc networks (VANETs), which play a crucial role in enhancing traffic safety, driving comfort, and overall experience. However, the open nature of VANETs and the sensitivity of transmitted data, including vehicle control commands and driving records, pose significant security risks. While blockchain technology offers a promising solution with its decentralized, immutable, and transparent characteristics, most existing blockchain systems are resource-intensive, making them incompatible with the limited storage and computing capacities of VANET terminals, such as road-side units, on-board units, and sensors. To address this challenge, this paper presents a novel blockchain system designed specifically for VANETs, featuring a scalable storage architecture that leverages network sharding and multi-consensus strategies to optimize topology and consensus processes. By introducing a collaborative storage mechanism and dynamic ledger replication strategy, the proposed system significantly reduces storage requirements for participating nodes, enabling even resource-constrained VANET terminals to contribute to blockchain maintenance and ensuring the secure, private, and trustworthy sharing of road traffic information.","Vehicular ad-hoc networks (VANETs), which serve as the foundation for smart transportation systems enabling advancements like self-driving cars, safety features, and interconnected cities, face significant security and privacy challenges due to their public nature and the sensitive data they handle. Blockchain technology, with its inherent decentralization, immutability, and traceability, offers a potential solution. However, conventional blockchain systems demand substantial storage and processing power, which is often beyond the capabilities of resource-constrained VANET devices like roadside units (RSUs), onboard units (OBUs), and sensors. To address this, this study introduces a novel blockchain framework designed for scalable storage within VANETs. This framework leverages network partitioning and a multi-consensus approach to optimize the blockchain's structure and consensus mechanism. To minimize storage demands on participating RSU nodes, a collaborative storage system and a dynamic replication strategy for the blockchain ledger are employed. Evaluations demonstrate that this scalable storage solution significantly reduces storage requirements for blockchain nodes compared to existing methods, enabling resource-limited VANET devices to participate in blockchain maintenance, preventing centralization, and promoting secure and trustworthy sharing of traffic data."
45,Intelligent patrol inspection of low code enabled electric power communication equipment based on digital transformation,"In order to meet the needs of digital development of power system and ensure the safety of power communication equipment, an intelligent patrol inspection method for power communication equipment with low code enabling based on digital transformation is proposed. Build an intelligent patrol platform for low code enabled power communication equipment based on digital transformation. The data acquisition module of the infrastructure layer scans the RFID tags on the communication equipment through the patrol personnel’s handheld devices to obtain the specific information of the current equipment. The UDT data transmission protocol of the data transmission module transmits the equipment information collected by the patrol personnel to the patrol application layer; The fault diagnosis module of patrol application layer uses convolutional neural network to analyze patrol data and complete fault detection of power communication equipment; The integrated component layer uses the application components developed by visual tools to enable the platform with low code through the low threshold platform development technology; The front-end service layer, supported by the technology of the integrated component layer, implements the simplification and configurable processing of front-end display results such as patrol results display, and views and manages the patrol results of the platform through the user terminal layer. The experiment shows that the method is accurate in data acquisition, fast in data transmission and high in fault diagnosis accuracy. It is a highly practical intelligent patrol method for power communication equipment.","To address the digital development needs of power systems and ensure the safety of power communication equipment, a low code-enabled intelligent patrol inspection method, rooted in digital transformation, is suggested. An intelligent patrol platform for low code-enabled power communication equipment, leveraging digital transformation, is constructed. The infrastructure layer’s data acquisition module uses handheld devices carried by patrol personnel to scan RFID tags on communication equipment, gathering detailed information about the current equipment. The data transmission module's UDT data protocol conveys the collected equipment information to the patrol application layer. The patrol application layer’s fault diagnosis module deploys a convolutional neural network to analyze patrol data and conduct fault detection on power communication equipment. The integrated component layer employs visual tool-developed application components to empower the platform with low code via low-threshold platform development technology. Supported by the integrated component layer's technology, the front-end service layer facilitates the simplification and customization of display outputs such as patrol results. The user terminal layer allows viewing and management of the platform’s patrol results. Experiments demonstrate the method’s precision in data capture, speed in data transmission, and high fault diagnosis accuracy, establishing it as a highly effective intelligent patrol method for power communication equipment.","To address the demands of power system digitalization and ensure the reliability of power communication infrastructure, a novel, low-code-enabled intelligent inspection approach is introduced, leveraging digital transformation to create a smart patrol platform. This platform's infrastructure layer is equipped with a data acquisition module that utilizes handheld devices to scan RFID tags on communication equipment, gathering specific information on current assets. The collected data is then transmitted to the patrol application layer via the UDT protocol, where a convolutional neural network-based fault diagnosis module analyzes the data to detect potential issues. The integrated component layer employs visual tools to develop application components, enabling low-code platform development with a low threshold. The front-end service layer, supported by the integrated component layer, simplifies and configures the display of patrol results, such as inspection outcomes, which can be viewed and managed through the user terminal layer. Experimental results demonstrate the method's accuracy in data collection, speed in data transmission, and high fault diagnosis precision, making it a highly practical intelligent patrol solution for power communication equipment.","To address the digital evolution of power systems and bolster the security of communication infrastructure, this study introduces a novel, intelligent patrol inspection method for power communication equipment. This method leverages low-code development techniques, facilitating digital transformation. The proposed system comprises a custom-built, intelligent patrol platform empowered by low-code tools. At its foundation, a data acquisition module utilizes handheld devices to scan RFID tags affixed to communication equipment, gathering comprehensive equipment details. The collected information is then relayed to the patrol application layer through the robust UDT data transmission protocol. Within this application layer, a fault diagnosis module, powered by convolutional neural networks, meticulously analyzes the patrol data to identify potential faults in the communication equipment. An integrated component layer, utilizing visually-developed application components and low-threshold platform development technology, enables low-code functionality within the platform. This empowers the front-end service layer to streamline and customize the presentation of patrol outcomes, including display and management of results through the user terminal layer. Experimental validation confirms the efficacy of this method, demonstrating precision in data acquisition, rapid data transmission, and high accuracy in fault diagnosis. The result is a highly practical and intelligent patrol system optimized for power communication equipment."
104,Small Power Nuclear Plants: Technical Level and Prospects for Commercialization (Review),"AbstractThe active development of the Arctic and the Northern Sea Route determines the importance of the rapid development of energy-supply systems for remote regions. A key component of isolated power systems are low-power energy sources. The high cost of fossil fuels in remote regions, coupled with tightening environmental regulations, brings to the fore the challenge of implementing carbon-neutral energy generation technologies. Promising power plants, the performance of which is little dependent on weather conditions, and whose operation is not associated with the generation of greenhouse gas emissions, are low-power nuclear power plants. Currently, some countries are developing and implementing new types of reactor plants whose electrical power does not exceed 300 MW: according to the IAEA, there are more than 70 different projects. Modularity, versatility (in addition to power generation, many projects also provide for the production of thermal energy and hydrogen), increased compactness, and lower capital costs for construction compared to traditional high-power power units make it promising to create low-power reactor plants. This review presents an analysis of the current state of the problems in the design and implementation of such power plants. The technical level of domestic and foreign projects of small modular reactors (SMR) was assessed. Promising areas for the use of thermal energy from small modular installations have been identified, taking into account current trends in energy, including low-carbon and nuclear-hydrogen areas. Possible circuit solutions for the production of electricity based on advanced cycles, including the use of nontraditional working fluids, have been studied. The potential for commercialization of low-power nuclear power plant projects has been considered; the question of successful business implementation of power plants of this type remains open.","Abstract The ongoing expansion of the Arctic and the Northern Sea Route underscores the necessity for swift advancements in energy supply systems for secluded areas. Essential to these isolated power systems are small-scale energy sources. The steep expense of fossil fuels in remote locations, alongside stricter environmental policies, highlights the urgency of adopting carbon-neutral energy technologies. Low-power nuclear power plants, which are less influenced by weather variability and do not produce greenhouse gas emissions, are particularly promising. Currently, various countries are developing new reactor designs with electrical outputs of up to 300 MW: according to the IAEA, there are over 70 distinct projects underway. Features such as modularity, versatility (including capabilities for generating thermal energy and hydrogen), enhanced compactness, and reduced capital expenditure compared to conventional high-capacity power units make these low-power reactors attractive options. This review assesses the current challenges in designing and deploying these reactors. It examines the technical maturity of both domestic and international small modular reactor (SMR) projects. The review also identifies promising applications for the thermal energy produced by small modular units, considering present energy trends such as low-carbon and nuclear-hydrogen sectors. Moreover, it explores potential circuit configurations for electricity generation using advanced cycles and unconventional working fluids. The feasibility of commercializing small-scale nuclear power plant projects is discussed, though the issue of their successful business implementation remains unresolved.","The accelerated exploitation of the Arctic region and the Northern Sea Route necessitates the swift establishment of reliable energy infrastructure in remote areas, with a focus on decentralized power systems that incorporate low-capacity energy sources. The prohibitive cost of fossil fuels in these regions, exacerbated by increasingly stringent environmental protocols, underscores the imperative to adopt carbon-neutral energy generation technologies. A promising solution lies in the development of compact, low-power nuclear reactors that are resilient to weather fluctuations and emit no greenhouse gases. Over 70 international projects are currently underway to design and deploy reactors with capacities below 300 MW, leveraging advantages such as modularity, versatility, and reduced construction costs. This review examines the current challenges and opportunities in designing and implementing these small modular reactors, assessing their technical feasibility and potential applications in thermal energy and hydrogen production. It also explores innovative circuit solutions for electricity generation, including the use of non-traditional working fluids, and evaluates the commercial viability of these projects, highlighting the need for successful business models to ensure their widespread adoption.","The rapid development of the Arctic and Northern Sea Route necessitates efficient energy solutions for isolated regions. While fossil fuels remain costly and environmentally damaging in these areas, low-power nuclear plants offer a promising carbon-neutral alternative. Numerous countries are currently exploring innovative reactor designs with power outputs below 300 MW, characterized by modularity, adaptability for heat and hydrogen production, compact size, and reduced construction costs compared to traditional large-scale reactors. This review investigates the current landscape of designing and implementing such power plants, evaluating the technological advancement of domestic and international small modular reactor (SMR) projects. Promising applications for SMR-generated heat are identified, aligning with low-carbon and nuclear-hydrogen energy trends. The study also examines potential electricity generation circuits based on advanced cycles, including those utilizing unconventional working fluids. Finally, the commercial viability of low-power nuclear plants is considered, although the question of their successful market implementation remains unresolved."
125,Embedding responsibility in intelligent systems: from AI ethics to responsible AI ecosystems,"Intelligent systems that are capable of making autonomous decisions based on input from their environment have great potential to do good, but they also raise significant social and ethical concerns. The discourse on ethics and artificial intelligence (AI) has covered these concerns in depth and developed an array of possible ways of addressing them. This article argues that a shortcoming of this discourse is that it concentrates on specific issues and their mitigation but neglects the nature of intelligent systems as socio-technical systems of systems that are often described as ecosystems. Building on the discussion of ethics and AI, the article suggests that it would be beneficial to come to an understanding of what would constitute responsible AI ecosystems. By introducing the concept of meta-responsibility or higher-level responsibility, the article proposes characteristics that an ecosystem would have to fulfil, in order to be considered a responsible ecosystem. This perspective is theoretically interesting because it extends the current AI ethics discourse. It furthermore offers a novel perspective for researchers and developers of intelligent system and helps them reflect on the way they relate to ethical issues.","Autonomous intelligent systems, capable of making decisions based on their environmental inputs, possess significant potential for positive impact but also pose substantial social and ethical challenges. The ethical discourse surrounding artificial intelligence (AI) has extensively explored these challenges and suggested various mitigation strategies. This article critiques this discourse, arguing that it tends to focus on specific issues and solutions while overlooking the broader context of intelligent systems as complex socio-technical ecosystems. Expanding upon the discussion of AI ethics, the article advocates for a better understanding of what defines a responsible AI ecosystem. By introducing the concept of meta-responsibility or a higher-level responsibility, the article delineates qualities that such an ecosystem must exhibit to be considered responsible. This viewpoint is theoretically intriguing as it broadens the current discussion on AI ethics and provides a fresh perspective for researchers and developers of intelligent systems, aiding their contemplation of ethical considerations.","Autonomous intelligent systems, which can make decisions based on environmental inputs, hold immense promise for positive impact, yet they also spark substantial social and ethical apprehensions. While the ongoing debate surrounding AI ethics has exhaustively explored these concerns and proposed various solutions, a notable oversight is its focus on isolated issues and mitigation strategies, neglecting the intrinsic nature of intelligent systems as complex, interconnected socio-technical ecosystems. This article contends that a crucial step forward would be to define the parameters of responsible AI ecosystems, introducing the concept of meta-responsibility to outline the essential characteristics of such ecosystems. This novel perspective not only enriches the theoretical landscape of AI ethics but also provides a valuable framework for researchers and developers of intelligent systems to re-examine their approach to ethical considerations.","While smart systems capable of independent decision-making based on environmental input hold immense promise for positive impact, they also spark considerable societal and ethical apprehensions. Extensive discussions surrounding ethics in artificial intelligence (AI) have delved into these anxieties and offered various approaches to tackle them. However, this discourse often focuses narrowly on specific problems and their solutions, overlooking the inherent nature of intelligent systems as complex socio-technical networks often likened to ecosystems. Expanding upon existing ethical considerations in AI, this article posits that understanding what constitutes responsible AI ecosystems is crucial. By introducing the notion of meta-responsibility, it outlines the essential attributes for such ecosystems, thereby enriching the ongoing dialogue on AI ethics. This novel perspective encourages researchers and developers to critically examine their role in addressing ethical implications within intelligent systems."
169,Cybersecurity considerations for radiology departments involved with artificial intelligence,"AbstractRadiology artificial intelligence (AI) projects involve the integration of integrating numerous medical devices, wireless technologies, data warehouses, and social networks. While cybersecurity threats are not new to healthcare, their prevalence has increased with the rise of AI research for applications in radiology, making them one of the major healthcare risks of 2021. Radiologists have extensive experience with the interpretation of medical imaging data but radiologists may not have the required level of awareness or training related to AI-specific cybersecurity concerns. Healthcare providers and device manufacturers can learn from other industry sector industries that have already taken steps to improve their cybersecurity systems. This review aims to introduce cybersecurity concepts as it relates to medical imaging and to provide background information on general and healthcare-specific cybersecurity challenges. We discuss approaches to enhancing the level and effectiveness of security through detection and prevention techniques, as well as ways that technology can improve security while mitigating risks. We first review general cybersecurity concepts and regulatory issues before examining these topics in the context of radiology AI, with a specific focus on data, training, data, training, implementation, and auditability. Finally, we suggest potential risk mitigation strategies. By reading this review, healthcare providers, researchers, and device developers can gain a better understanding of the potential risks associated with radiology AI projects, as well as strategies to improve cybersecurity and reduce potential associated risks.Clinical Relevance StatementThis review can aid radiologists’ and related professionals’ understanding of the potential cybersecurity risks associated with radiology AI projects, as well as strategies to improve security.Key Points• Embarking on a radiology artificial intelligence (AI) project is complex and not without risk especially as cybersecurity threats have certainly become more abundant in the healthcare industry.• Fortunately healthcare providers and device manufacturers have the advantage of being able to take inspiration from other industry sectors who are leading the way in the field.• Herein we provide an introduction to cybersecurity as it pertains to radiology, a background to both general and healthcare-specific cybersecurity challenges; we outline general approaches to improving security through both detection and preventative techniques, and instances where technology can increase security while mitigating risks.Graphical Abstract","AbstractArtificial intelligence (AI) initiatives in radiology necessitate the integration of various medical devices, wireless technologies, data repositories, and social networks. Although cybersecurity threats in healthcare are longstanding, their frequency has surged with the ascension of AI applications in radiology, positioning them as one of the primary healthcare risks in 2021. Radiologists, while adept at interpreting medical imaging data, may lack sufficient awareness or training regarding the unique cybersecurity issues posed by AI. Healthcare professionals and device manufacturers can benefit from the cybersecurity advancements made in other industries to bolster their own security measures. This review introduces essential cybersecurity concepts specific to medical imaging and provides foundational knowledge on both general and healthcare-related cybersecurity challenges. We explore strategies for enhancing security through detection and prevention methods, and discuss how technology can augment security while reducing risk. Initially, we cover broad cybersecurity principles and regulatory aspects, then delve into their implications within radiology AI, emphasizing data handling, training, implementation, and auditability. Finally, we propose risk mitigation techniques. This review aims to enhance the understanding of healthcare providers, researchers, and device developers regarding the cybersecurity risks affiliated with radiology AI projects and suggests strategies for improving security and minimizing associated risks. Clinical Relevance StatementThis review is designed to help radiologists and associated professionals comprehend the potential cybersecurity threats linked to radiology AI projects and the strategies to enhance security. Key Points • Initiating a radiology AI project is intricate and fraught with risks, particularly as cybersecurity threats have proliferated in the healthcare sector. • Fortunately, healthcare professionals and device manufacturers can leverage lessons learned from other industries that are at the forefront of cybersecurity. • This document offers an overview of cybersecurity as it applies to radiology, a discussion on general and healthcare-specific cybersecurity challenges, and outlines methods to enhance security through both detection and prevention, along with how technology can safeguard against risks.","Here is a paraphrased version of the text, exhibiting both syntactic and lexical diversity while preserving semantic similarity: ""The development of artificial intelligence (AI) projects in radiology necessitates the convergence of diverse medical devices, wireless technologies, data repositories, and social media platforms. As AI research in radiology continues to expand, the healthcare sector is confronted with an escalating threat landscape, with cybersecurity breaches emerging as a major risk in 2021. While radiologists possess extensive expertise in interpreting medical imaging data, they may lack the requisite knowledge and training to address AI-specific cybersecurity concerns. Healthcare providers and device manufacturers can draw valuable lessons from other industries that have successfully enhanced their cybersecurity infrastructure. This review seeks to elucidate cybersecurity principles in the context of medical imaging, providing an overview of general and healthcare-specific cybersecurity challenges. We explore strategies for bolstering security through detection and prevention mechanisms, as well as technological innovations that can mitigate risks while enhancing security. Following a review of general cybersecurity concepts and regulatory issues, we examine these topics in the context of radiology AI, focusing on data, training, implementation, and auditability. Finally, we propose potential risk mitigation strategies. By engaging with this review, healthcare providers, researchers, and device developers can gain a deeper understanding of the potential risks associated with radiology AI projects, as well as strategies to improve cybersecurity and reduce associated risks.""","The implementation of artificial intelligence (AI) in radiology necessitates the interconnection of diverse medical technologies, encompassing devices, wireless networks, data repositories, and social platforms. This surge in AI-driven radiology research has amplified cybersecurity vulnerabilities within the healthcare domain, elevating them to a critical concern in recent years. While radiologists possess vast expertise in medical image interpretation, they may lack sufficient awareness and training regarding the unique cybersecurity risks associated with AI. Fortunately, healthcare stakeholders can draw valuable insights from other sectors that have proactively fortified their cybersecurity infrastructure. This analysis aims to elucidate cybersecurity principles within the realm of medical imaging, providing a foundational understanding of broader cybersecurity concerns and those specific to healthcare. We explore methodologies to bolster security effectiveness through detection and prevention mechanisms, while also examining how technology can enhance security while mitigating risks. Our exploration begins with a review of fundamental cybersecurity concepts and regulatory frameworks, subsequently delving into their implications for radiology AI, emphasizing data security, training, implementation, and auditability. Lastly, we propose potential risk mitigation strategies. Through this comprehensive review, healthcare practitioners, researchers, and device manufacturers can cultivate a deeper comprehension of the potential hazards inherent in radiology AI projects, along with strategies to fortify cybersecurity and minimize associated risks."
113,Basic social justice orientations—measuring order-related justice in the European Social Survey Round 9,"Individuals hold normative ideas about the just distribution of goods and burdens within a social aggregate. These normative ideas guide the evaluation of existing inequalities and refer to four basic principles: (1) Equality stands for an equal distribution of rewards and burdens. While the principle of (2) need takes individual contributions into account, (3) equity suggests a distribution based on merit. The (4) entitlement principle suggests that ascribed (e.g., gender) and achieved status characteristics (e.g., occupational prestige) should determine the distribution of goods and burdens. Past research has argued that preferences for these principles vary with social position as well as the social structure of a society. The Basic Social Justice Orientations (BSJO) scale was developed to assess agreement with the four justice principles but so far has only been fielded in Germany. Round 9 of the European Social Survey (ESS R9 with data collected in 2018/2019) is the first time; four items of the BSJO scale (1 item per justice principle) were included in a cross-national survey program, offering the unique opportunity to study both within and between country variation. To facilitate substantive research on preference for equality, equity, need, and entitlement, this report provides evidence on measurement quality in 29 European countries from ESS R9. Analyzing response distributions, non-response, reliability, and associations with related variables, we find supportive evidence that the four items of the BSJO scale included in ESS R9 produce low non-response rates, estimate agreement with the four distributive principles reliably, and follow expected correlations with related concepts. Researchers should, however, remember that the BSJO scale, as implemented in the ESS R9, only provides manifest indicators, which therefore may not cover the full spectrum of the underlying distributive principles but focus on specific elements of it.","Individuals maintain normative beliefs about the fair allocation of resources and responsibilities within a society. These beliefs influence the assessment of current inequalities and are based on four fundamental principles: (1) Equality, which advocates for an even distribution of benefits and responsibilities; (2) Need, which considers individual contributions; (3) Equity, which proposes distribution according to merit; and (4) Entitlement, which argues that both ascribed (e.g., gender) and achieved characteristics (e.g., job status) should influence the distribution. Previous research indicates that preferences for these principles vary depending on social position and societal structure. The Basic Social Justice Orientations (BSJO) scale was designed to measure agreement with the four principles, but it has only been used in Germany until recently. For the first time, four items from the BSJO scale—one for each principle—were included in an international survey with Round 9 of the European Social Survey (ESS R9, data collected in 2018/2019), providing a unique chance to examine variations within and between countries. This report supplies evidence on the measurement quality in 29 European countries from ESS R9, aiding substantive research on preferences for equality, equity, need, and entitlement. Analysis of response distributions, non-response rates, reliability, and correlations with related variables shows that the four BSJO items in ESS R9 exhibit low non-response rates, reliable agreement estimation with the four principles, and expected correlations. Researchers should note, however, that the BSJO scale in ESS R9 offers manifest indicators, which may not fully capture the entire spectrum of distributive principles but focus on their specific aspects.","People harbor conceptions of fairness regarding the allocation of resources and responsibilities within a collective entity, which influence their assessment of existing disparities and are rooted in four fundamental tenets. Firstly, the principle of egalitarianism advocates for uniform distribution of benefits and burdens. In contrast, the need principle considers individual inputs, while the merit-based principle of equity proposes allocation based on deservingness. Lastly, the entitlement principle suggests that distribution should be determined by inherent characteristics, such as gender, and acquired status, like occupational prestige. Previous studies have shown that preferences for these principles vary depending on social standing and societal structure. The Basic Social Justice Orientations scale was designed to measure adherence to these principles, but its application has been limited to Germany. The inclusion of four scale items, each representing a principle, in the 9th round of the European Social Survey provides a unique opportunity to examine both intra- and inter-country differences. This report presents evidence on the measurement quality of these items in 29 European countries, analyzing response patterns, non-response rates, reliability, and correlations with related variables. The findings indicate that the scale items exhibit low non-response rates, reliably estimate agreement with the principles, and display expected correlations with related concepts. However, researchers should be aware that the scale, as implemented, only provides surface-level indicators, which may not capture the full range of the underlying principles but rather focus on specific aspects.","People possess beliefs about how goods and responsibilities should be fairly allocated within a society. These beliefs, which inform judgments about existing disparities, are rooted in four fundamental principles: (1) Equality advocates for an even distribution of benefits and burdens. (2) Need considers individual contributions, while (3) equity proposes distribution based on merit. Lastly, (4) entitlement posits that inherent traits (like gender) and earned statuses (such as job prestige) should dictate the allocation of resources and obligations. Previous studies suggest that preferences for these principles are influenced by both individual social standing and the broader societal structure. A tool called the Basic Social Justice Orientations (BSJO) scale was created to gauge agreement with these four justice principles, but its use had been limited to Germany. Round 9 of the European Social Survey (ESS R9, with data gathered in 2018/2019) marked the first inclusion of four BSJO scale items (one per principle) in a multinational survey, providing a unique chance to examine variations within and across nations. This report contributes to substantive research on preferences for equality, equity, need, and entitlement by evaluating the measurement quality in 29 European countries using ESS R9 data. By analyzing response patterns, non-response rates, reliability, and relationships with relevant variables, we find encouraging evidence that the four BSJO items in ESS R9 yield low non-response, reliably gauge agreement with the four distributive principles, and exhibit anticipated correlations with related concepts. However, researchers should bear in mind that the BSJO scale, as implemented in ESS R9, offers only overt indicators, which might not encompass the full range of the underlying distributive principles but instead concentrate on specific facets."
13,"Lightness constancy in reality, in virtual reality, and on flat-panel displays","Virtual reality (VR) displays are being used in an increasingly wide range of applications. However, previous work shows that viewers often perceive scene properties very differently in real and virtual environments and so realistic perception of virtual stimuli should always be a carefully tested conclusion, not an assumption. One important property for realistic scene perception is surface color. To evaluate how well virtual platforms support realistic perception of achromatic surface color, we assessed lightness constancy in a physical apparatus with real lights and surfaces, in a commercial VR headset, and on a traditional flat-panel display. We found that lightness constancy was good in all three environments, though significantly better in the real environment than on the flat-panel display. We also found that variability across observers was significantly greater in VR and on the flat-panel display than in the physical environment. We conclude that these discrepancies should be taken into account in applications where realistic perception is critical but also that in many cases VR can be used as a flexible alternative to flat-panel displays and a reasonable proxy for real environments.","Virtual reality (VR) displays are being employed across a growing array of applications. Research indicates, however, that there are often significant differences in how viewers perceive scenes in real versus virtual settings. Therefore, concluding that virtual stimuli offer realistic perception requires careful testing rather than assumptions. One key aspect for realistic scene perception is surface color. To assess how well VR platforms provide realistic perception of achromatic surface color, we measured lightness constancy using a physical setup with real lights and surfaces, a commercial VR headset, and a traditional flat-panel display. The results showed that lightness constancy was effective in all three settings, although it was notably superior in the real environment compared to the flat-panel display. Additionally, we observed greater variability among observers in both VR and the flat-panel display than in the physical environment. These findings suggest that such discrepancies should be considered in applications where accurate perception is crucial, but also highlight that VR can often serve as a versatile alternative to flat-panel displays and a reasonable stand-in for real environments.","The scope of virtual reality (VR) technology is expanding rapidly, but research highlights that individuals tend to perceive environmental attributes distinctly in virtual and real-world settings, emphasizing the need to verify, rather than assume, the accuracy of virtual stimuli. A crucial aspect of realistic scene interpretation is surface color, particularly achromatic hues. To investigate the efficacy of virtual platforms in replicating realistic achromatic surface color perception, we conducted an experiment involving a physical setup with real lighting and surfaces, a commercial VR headset, and a traditional flat-screen display. Our results showed that lightness constancy was satisfactory across all three environments, although it was significantly superior in the real-world setting compared to the flat-screen display. Notably, we also observed greater inter-observer variability in VR and flat-screen display environments relative to the physical setup. These findings underscore the importance of considering these discrepancies in applications where accurate perception is paramount, while also suggesting that VR can serve as a versatile alternative to flat-screen displays and a reasonable surrogate for real-world environments in many cases.","The expanding use of VR displays necessitates careful consideration of their ability to accurately depict reality. Research indicates discrepancies in scene perception between real and virtual environments, particularly concerning surface color. To investigate this, a study comparing lightness constancy across a physical setup, a commercial VR headset, and a standard flat-panel display revealed robust performance in all conditions, albeit superior in the physical realm compared to the flat-panel. Notably, inter-observer variability was heightened in both VR and flat-panel environments. These findings highlight the need for rigorous testing of perceptual realism in VR applications, yet also suggest its potential as a versatile substitute for flat-panel displays and a feasible approximation of real-world settings."
117,Focusing on the “Social” in Social Justice Research,"I describe what I see as a very strong connection between fairness perceptions and reactions that show engagement with social entities and social relationships. A brief review of studies on perceived fairness and the fair process effects illustrates how very social is the reaction to fair or unfair treatment—that is, how perceptions of fairness have a strong impact on how people view their inclusion and safety in the social group or relationship in which the treatment occurs. I suggest that this much-observed connection between fairness and group engagement raises some interesting questions about how perceived fairness affects some traditional group process phenomena. I discuss research questions that arise with respect to fairness and social identity process, fairness and obedience to authority, and fairness and conformity. Investigations of these questions, I argue, would give the field a stronger foundation in the basic social psychology of fairness and it would re-invigorate the groups aspect of social psychology as a discipline.","I explore the significant link between perceptions of fairness and actions reflecting engagement with social entities and relationships. A brief review of research on perceived fairness and its effects on fair processes highlights the social nature of responses to fair or unfair treatment—demonstrating that perceptions of fairness greatly influence individuals' sense of inclusion and safety within their social groups or relationships. I propose that this well-documented relationship between fairness and group involvement prompts intriguing questions about the impact of perceived fairness on traditional group process phenomena. I examine research questions concerning fairness and social identity processes, fairness and obedience to authority, and fairness and conformity. Studying these questions, I argue, would strengthen the field's foundation in the basic social psychology of fairness and revitalize the focus on group dynamics in social psychology as a discipline.","I highlight the existence of a profound correlation between individuals' perceptions of fairness and their responses that demonstrate investment in social networks and interpersonal bonds. A concise examination of research on perceived fairness and the fair process effects reveals the deeply social nature of reactions to fair or unfair treatment, namely, how fairness perceptions significantly influence individuals' sense of belonging and security within the social context in which the treatment occurs. This well-documented link between fairness and group involvement prompts intriguing inquiries about the impact of perceived fairness on classic group dynamics phenomena. I explore potential research avenues related to fairness and social identity formation, fairness and deference to authority, and fairness and compliance with norms, arguing that investigating these questions would provide a more solid grounding in the fundamental social psychology of fairness and revitalize the group dynamics aspect of social psychology as a field of study.","My analysis reveals a powerful link between perceived fairness and individuals' engagement with social entities and relationships. Existing research on fairness perceptions and the ""fair process effect"" highlights the inherently social nature of responses to fair or unfair treatment, demonstrating how such perceptions profoundly influence individuals' feelings of belonging and security within the group or relationship where the treatment transpires. This well-documented connection between fairness and group engagement prompts intriguing questions regarding the impact of perceived fairness on established group dynamics. I propose research directions examining the interplay between fairness and social identity formation, obedience to authority, and conformity. Exploring these areas, I contend, would not only provide a deeper understanding of the fundamental social psychology underlying fairness but also revitalize the study of groups within the discipline of social psychology."
83,Humans versus AI: whether and why we prefer human-created compared to AI-created artwork,"With the recent proliferation of advanced artificial intelligence (AI) models capable of mimicking human artworks, AI creations might soon replace products of human creativity, although skeptics argue that this outcome is unlikely. One possible reason this may be unlikely is that, independent of the physical properties of art, we place great value on the imbuement of the human experience in art. An interesting question, then, is whether and why people might prefer human-compared to AI-created artworks. To explore these questions, we manipulated the purported creator of pieces of art by randomly assigning a “Human-created” or “AI-created” label to paintings actually created by AI, and then assessed participants’ judgements of the artworks across four rating criteria (Liking, Beauty, Profundity, and Worth). Study 1 found increased positive judgements for human- compared to AI-labelled art across all criteria. Study 2 aimed to replicate and extend Study 1 with additional ratings (Emotion, Story, Meaningful, Effort, and Time to create) intended to elucidate why people more-positively appraise Human-labelled artworks. The main findings from Study 1 were replicated, with narrativity (Story) and perceived effort behind artworks (Effort) moderating the label effects (“Human-created” vs. “AI-created”), but only for the sensory-level judgements (Liking, Beauty). Positive personal attitudes toward AI moderated label effects for more-communicative judgements (Profundity, Worth). These studies demonstrate that people tend to be negatively biased against AI-created artworks relative to purportedly human-created artwork, and suggest that knowledge of human engagement in the artistic process contributes positively to appraisals of art.","In light of the burgeoning development of sophisticated artificial intelligence (AI) models that can replicate human-created art, AI-generated artworks might emerge as substitutes for human creativity. However, critics dispute the likelihood of this scenario, suggesting that the intrinsic value we place on the human experience within art plays a crucial role, irrespective of the art's physical attributes. This raises a compelling issue regarding whether and why individuals may favor human-produced over AI-generated art. To investigate, researchers conducted an experiment where they randomly assigned either a “Human-created” or “AI-created” label to AI-generated paintings and then evaluated participants' perceptions of the artworks based on four criteria: Liking, Beauty, Profundity, and Worth. The first study revealed that artwork labeled as human-made received more favorable evaluations across all criteria compared to those labeled as AI-made. The second study sought not only to replicate but also to expand on these results by including additional ratings (Emotion, Story, Meaningfulness, Effort, and Time to create) to understand why human-labeled art received higher appraisals. The replication confirmed the initial findings, showing that labels influenced sensory-level judgments (Liking, Beauty), with narrativity (Story) and perceived effort (Effort) acting as moderators. For more communicative judgments (Profundity, Worth), personal positive attitudes towards AI moderated the label effects. Overall, these studies indicate a prevalent bias against AI-generated art compared to human-labeled art, emphasizing that the perceived human involvement in the artistic process enhances the appraisal of artworks.","The rapid emergence of sophisticated artificial intelligence (AI) models that can replicate human artistic masterpieces has sparked debate about whether AI-generated creations will eventually supplant human innovations, although some doubters contend that this scenario is improbable. A potential explanation for this skepticism lies in the fact that, beyond the physical attributes of art, we assign significant importance to the infusion of human sentiment and experience in artistic expressions. This raises an intriguing question: do people exhibit a preference for human-generated versus AI-created artworks, and if so, why? To investigate this, we conducted an experiment where we randomly attributed ""Human-created"" or ""AI-created"" labels to AI-generated paintings and evaluated participants' assessments of the artworks across four dimensions (Aesthetic Appeal, Visual Attractiveness, Depth, and Monetary Value). The results of Study 1 revealed that human-labelled art received more favorable evaluations across all dimensions. Study 2 aimed to replicate and expand upon Study 1 by incorporating additional ratings (Emotional Resonance, Narrative, Significance, Creative Effort, and Time Invested) to elucidate the underlying reasons for the preference towards human-labelled artworks. The primary findings of Study 1 were replicated, with narrative richness and perceived creative effort influencing the label effects (""Human-created"" vs. ""AI-created"") solely for the sensory-level evaluations (Aesthetic Appeal, Visual Attractiveness). Furthermore, positive attitudes towards AI moderated label effects for more abstract judgements (Depth, Monetary Value). These studies demonstrate that people tend to harbor a negative bias against AI-generated artworks compared to those perceived as human-created, suggesting that awareness of human involvement in the creative process positively impacts art appreciation.","The rapid increase in sophisticated artificial intelligence (AI) models capable of generating art mimicking human styles raises concerns about the potential displacement of human creativity, despite skepticism regarding this possibility. One argument against this displacement emphasizes the inherent value we assign to art's reflection of human experiences, irrespective of its physical characteristics. This leads to a compelling inquiry into the potential preference for human-created art over AI-generated art and the underlying reasons for this preference. To investigate this, a series of studies examined participants' judgments of artwork across various criteria (e.g., liking, beauty, profundity, worth), manipulating the perceived creator by randomly labeling AI-generated paintings as either ""Human-created"" or ""AI-created."" Results consistently revealed a preference for ""Human-created"" labeled artwork, suggesting a bias against AI-generated art. Further exploration revealed that this bias was influenced by perceptions of the narrative quality and effort invested in the artwork, primarily for sensory-level judgments, while positive attitudes toward AI mitigated the bias for judgments related to the artwork's communicative aspects. These findings underscore the significant role of perceived human involvement in shaping art appreciation, highlighting a tendency to favor artwork attributed to human creators."
187,Advancing green finance: a review of climate change and decarbonization,"This paper comprehensively reviews the interconnections between climate change, decarbonization, and green finance. The urgency of addressing climate change and its catastrophic consequences needs to focus on green finance as a vital tool in the global struggle against environmental damage. Green finance involves supplying investments, loans, or capital to support environmentally friendly activities, facilitating the transition to a more sustainable future. This review explores the theoretical frame of reference for green finance, including its impacts on climate change, decarbonization of economies, carbon-stranded assets, risk management, renewable energy, and sustainable economic growth. Additionally, it examines regional focuses in Asia, such as the importance of green finance in China and the beliefs and challenges of green finance in Bangladesh. The review also discusses future directions and recommendations for advancing green finance. The review examines the current research in green finance and how it can address environmental challenges and promote sustainable development. More research needs to be conducted in mainstream economics and finance journals to bridge the knowledge gap and foster broader scholarly engagement in green finance. Researchers, policymakers, investors, and stakeholders will receive help from the study's reliable and robust insights into combating climate change and promoting sustainable development.","This article thoroughly examines the links between climate change, decarbonization, and green finance. The critical necessity of mitigating climate change and its severe consequences highlights green finance as an essential instrument in the global fight against environmental harm. Green finance entails providing investments, loans, or capital to back eco-friendly initiatives, thus enabling the shift toward a more sustainable future. This examination delves into the theoretical framework surrounding green finance, addressing its effects on climate change, economic decarbonization, carbon-stranded assets, risk management, renewable energy, and sustainable economic growth. Furthermore, it analyzes regional focuses within Asia, such as the significance of green finance in China and the perspectives and obstacles faced in Bangladesh. The review also outlines future pathways and suggestions for the progression of green finance. It scrutinizes current research on how green finance can overcome environmental challenges and encourage sustainable development. There is a need for more studies published in prominent economics and finance journals to close the knowledge gap and boost wider academic interest in green finance. The study's substantial and dependable insights offer valuable support to researchers, policymakers, investors, and stakeholders in the quest to tackle climate change and endorse sustainable development.","This study undertakes an exhaustive examination of the intricate relationships between climate change mitigation, decarbonization strategies, and the emergence of sustainable financial systems. The imperative to tackle the devastating repercussions of climate change necessitates a concerted emphasis on green finance as a crucial instrument in the global quest to mitigate ecological degradation. By channeling investments, credit facilities, and capital towards eco-friendly initiatives, green finance facilitates a paradigm shift towards a more environmentally conscious future. This comprehensive review delves into the conceptual underpinnings of green finance, encompassing its influence on climate change mitigation, low-carbon economic transitions, stranded asset risks, risk management frameworks, renewable energy integration, and sustainable economic expansion. Furthermore, it scrutinizes regional hotspots in Asia, including the pivotal role of green finance in China's development and the perceptions, obstacles, and opportunities surrounding green finance in Bangladesh. The review also charts future trajectories and policy prescriptions for green finance advancement. By synthesizing existing research in green finance, this study provides actionable guidance on addressing environmental imperatives and promoting sustainable development, underscoring the need for further inquiry in mainstream economic and financial journals to bridge knowledge gaps and stimulate interdisciplinary dialogue among scholars, policymakers, investors, and stakeholders.","This comprehensive analysis explores the intricate relationship between climate change mitigation, economic decarbonization, and the burgeoning field of green finance. Highlighting the critical need to address the potentially devastating impacts of climate change, the paper positions green finance as a crucial instrument in the global fight against environmental degradation. Green finance, which encompasses the provision of funding, credit, and investment for environmentally sustainable initiatives, plays a pivotal role in facilitating the shift towards a more sustainable future. This review delves into the theoretical underpinnings of green finance, examining its influence on climate change mitigation, economic decarbonization, the issue of carbon-stranded assets, risk management strategies, renewable energy development, and sustainable economic growth. Furthermore, the analysis provides regional insights, specifically focusing on the significance of green finance in China and exploring the perspectives and obstacles surrounding green finance in Bangladesh. Future trajectories and recommendations for propelling the advancement of green finance are also discussed. The review critically assesses the existing research landscape in green finance, emphasizing its potential to tackle environmental challenges and foster sustainable development. The authors advocate for increased research within mainstream economics and finance publications to bridge existing knowledge gaps and encourage wider academic engagement in the field of green finance. This study's reliable and comprehensive insights aim to provide valuable guidance to researchers, policymakers, investors, and stakeholders in their collective efforts to combat climate change and promote sustainable development."
196,"Cultural diversity, social integration, and migrant entrepreneurship—evidence from the China migrants dynamic survey","Cultural diversity in host regions presents challenges for migrant entrepreneurs, such as resource competition and social conflicts. However, whether and how cultural diversity in the host regions affects migrants’ participation in entrepreneurship is not yet well understood. This study investigates the relationship by utilizing the China Migrants Dynamic Survey (CMDS) database and introducing food diversity as a novel proxy for cultural diversity. The logit regression analysis reveals that cultural diversity in host regions can reduce migrant engagement in entrepreneurial activities, especially in “self-employed” entrepreneurship. The results are consistent with several robustness tests. Additionally, a mechanism analysis based on a multi-path mediation model reveals that social integration, including factors such as social interaction, social exclusion, and social identification, plays a mediating role in this relationship. However, robust institutional environments can moderate the negative impact of cultural diversity on migrant entrepreneurship. This study provides a fresh non-market perspective for understanding and promoting cross-cultural entrepreneurial behavior.","Migrant entrepreneurs face obstacles in culturally diverse host regions, including competition for resources and social conflicts. Despite this, the effect of cultural diversity on migrant entrepreneurship remains unclear. This research explores the connection using the China Migrants Dynamic Survey (CMDS) database, employing food diversity as an innovative measure for cultural diversity. Findings from a logistic regression analysis suggest that cultural diversity can decrease migrant entrepreneurial activity, particularly in self-employment. These results are backed by several robustness checks. Furthermore, a mechanism analysis with a multi-path mediation model indicates that social integration—comprising social interaction, exclusion, and identification—mediates this relationship. However, strong institutional frameworks can mitigate the adverse effects of cultural diversity on migrant entrepreneurship. This study offers a novel non-market perspective on fostering cross-cultural entrepreneurial behavior.","The presence of diverse cultural backgrounds in host regions poses obstacles for entrepreneurs who are migrants, including rivalry for resources and social unrest. Despite this, the extent to which cultural diversity in these regions influences migrants' inclination to engage in entrepreneurial pursuits remains unclear. This research explores this connection by leveraging the China Migrants Dynamic Survey database and innovatively employing food diversity as a proxy for cultural diversity. The logit regression analysis uncovers that cultural diversity in host regions can deter migrants from participating in entrepreneurial ventures, particularly in self-employment. These findings are corroborated by multiple robustness tests. Furthermore, a mechanism analysis based on a multi-path mediation model reveals that social integration, encompassing factors such as social interaction, social exclusion, and social identification, serves as an intermediary in this relationship. However, a robust institutional framework can mitigate the adverse effects of cultural diversity on migrant entrepreneurship. This study offers a novel, non-market-oriented perspective on understanding and fostering entrepreneurial behavior across cultural boundaries.","The influence of culturally diverse host communities on migrant entrepreneurs remains underexplored, although difficulties like resource scarcity and social discord are recognized. This research examines this relationship using the China Migrants Dynamic Survey (CMDS) database, employing food diversity as a unique indicator of cultural variety. Logit regression analysis shows that cultural diversity in host communities can hinder migrant entrepreneurial activity, particularly in self-employment. These findings are confirmed through various robustness checks. Further, a multi-path mediation model reveals that social integration, encompassing elements like social interaction, exclusion, and identification, mediates this relationship. Notably, strong institutional frameworks can mitigate the adverse impact of cultural diversity on migrant entrepreneurship. This study offers a novel, non-market viewpoint for comprehending and fostering cross-cultural entrepreneurial endeavors."
143,Automation of Production Processes: Problems and Solutions,"AbstractAttention focuses on the automation of production processes at Russian enterprises within state programs for digitization, in an era of sanctions pressure. The current problems are analyzed. Software for the automation of production is investigated. A sequence of steps for the automation of production processes is proposed.","This abstract discusses the emphasis on automating production processes at Russian companies as part of national digitization initiatives amid sanctions. It examines existing challenges, reviews automation software, and outlines a step-by-step process for automating production workflows.","""Amidst the constraints of economic sanctions, AbstractAttention prioritizes the streamlining of manufacturing operations within Russian state-backed digital transformation initiatives, diagnosing existing challenges and scrutinizing specialized software solutions, ultimately outlining a phased approach to optimize production workflows.""","Under the constraints of international sanctions, this study examines the challenges and software solutions for automating production processes in Russian companies, within the framework of national digitalization initiatives. A phased approach to production automation implementation is presented."
128,AI ethics: from principles to practice,"Much of the current work on AI ethics has lost its connection to the real-world impact by making AI ethics operable. There exist significant limitations of hyper-focusing on the identification of abstract ethical principles, lacking effective collaboration among stakeholders, and lacking the communication of ethical principles to real-world applications. This position paper presents challenges in making AI ethics operable and highlights key obstacles to AI ethics impact. A preliminary practice example is provided to initiate practical implementations of AI ethics. We aim to inspire discussions on making AI ethics operable and focus on its impact on real-world applications.",A significant portion of contemporary research in AI ethics has become disconnected from tangible impacts by attempting to operationalize ethical standards. There are notable drawbacks in excessively focusing on abstract ethical concepts without fostering effective collaboration among stakeholders or effectively translating these principles into practical applications. This position paper outlines the challenges in operationalizing AI ethics and emphasizes major barriers to its real-world influence. An initial practical example is included to kick-start the implementation of AI ethics in practice. Our goal is to spark discussions on making AI ethics actionable and emphasizing its relevance to real-world scenarios.,"""A significant disconnect has emerged between the theoretical pursuits of AI ethics and their tangible consequences, largely due to an overemphasis on codifying abstract moral guidelines. This narrow approach neglects the crucial need for interdisciplinary cooperation and the translation of ethical frameworks into practical, real-world solutions. This paper identifies the hurdles hindering the effective integration of AI ethics and underscores the barriers to achieving meaningful, real-world outcomes. A pilot case study is offered to catalyze the development of actionable AI ethics strategies, with the ultimate goal of sparking a discourse on rendering AI ethics operational and impactful in everyday applications.""","The quest for operationalizing AI ethics has inadvertently distanced the field from its real-world implications. An excessive emphasis on abstract principles, coupled with insufficient collaboration among involved parties and inadequate translation of these principles into tangible applications, represents a substantial impediment. This paper delineates the hurdles encountered in bridging the gap between AI ethics theory and practice, underscoring the key barriers to its effective implementation. A preliminary practical illustration is offered to catalyze the tangible realization of AI ethics principles. Ultimately, this work seeks to stimulate discourse surrounding the operationalization of AI ethics and its tangible impact on real-world scenarios."
14,Exploratory analysis of the effectiveness of virtual reality in cardiovascular rehabilitation,"Virtual reality therapy has been shown to be effective in coping with psychological disorders accompanied by cardiovascular disease. Age appears to be a factor that can affect the effectiveness of psychological therapy in a virtual environment. Therefore, the aim of the study was to explore whether there are age-related differences in the effectiveness of reducing levels of depression and anxiety during a virtual reality psychological intervention implemented for rehabilitation. The study included 25 younger (< 65 years) and 25 older (65 +) patients with cardiovascular disease who participated in virtual reality therapy to cope with anxiety and depression. The Hospital Anxiety and Depression Scale was used to assess anxiety and depressive disorders before and after intervention. Significant reductions in anxiety and depression scores after intervention were observed in both age-matched groups, and no significant differences were found between the younger and older participants. Further evaluation of patient age as a predictor of the effectiveness of psychological intervention in virtual reality did not show a significant effect of age on effectiveness in reducing anxiety and depressive disorders. The results obtained suggest that older patients benefit similarly to younger patients from psychological intervention in a virtual environment. Furthermore, age does not appear to be considered a predictor of effectiveness in reducing the level of anxiety and depression in patients with cardiovascular disease using virtual reality therapy.","Research indicates that virtual reality therapy can effectively help manage psychological disorders in individuals with cardiovascular disease. The effectiveness of such therapy in a virtual setting may be influenced by age. Consequently, this study aimed to assess whether there are age-related disparities in reducing depression and anxiety through virtual reality psychological interventions utilized for rehabilitation. The study involved 25 patients under 65 and 25 patients aged 65 and over, all diagnosed with cardiovascular disease, who underwent virtual reality therapy for anxiety and depression. Anxiety and depression levels were evaluated before and after the intervention using the Hospital Anxiety and Depression Scale. Notable declines in anxiety and depression scores post-intervention were observed in both age groups, with no significant differences between younger and older participants. Further analysis did not indicate that age impacts the effectiveness of virtual reality therapy in reducing anxiety and depressive disorders. The findings suggest that older patients benefit equally to their younger counterparts from virtual reality psychological interventions, and age should not be considered a predictor of the therapy's effectiveness in alleviating anxiety and depression in cardiovascular disease patients.","Research has demonstrated the efficacy of virtual reality therapy in managing psychological disorders comorbid with cardiovascular disease. However, the impact of age on the therapeutic outcomes in virtual settings remains unclear. This study aimed to investigate whether age influences the effectiveness of virtual reality interventions in alleviating depression and anxiety in patients with cardiovascular disease. A total of 50 patients, divided into two age groups (<65 and 65+), participated in virtual reality therapy and were assessed using the Hospital Anxiety and Depression Scale before and after the intervention. The results revealed significant decreases in anxiety and depression scores across both age groups, with no notable differences between younger and older participants. Moreover, age was found not to be a significant predictor of the intervention's effectiveness in reducing anxiety and depressive symptoms. These findings suggest that older patients can derive similar benefits from virtual reality-based psychological interventions as their younger counterparts, and age does not appear to be a determining factor in the therapy's success.","Research suggests that virtual reality therapy can effectively address psychological distress in individuals with cardiovascular disease, but age may influence its impact. This study examined age-related differences in the effectiveness of VR therapy for anxiety and depression in 25 younger and 25 older cardiovascular patients. Pre- and post-intervention assessments using the Hospital Anxiety and Depression Scale revealed significant symptom reduction in both age groups, with no statistically significant difference between them. Further analysis confirmed that age did not significantly predict the efficacy of VR therapy in alleviating anxiety and depression. These findings indicate that older patients experience similar benefits to younger patients from virtual reality-based psychological interventions, suggesting that age should not be a primary factor in considering the suitability of this therapeutic approach for cardiovascular patients struggling with anxiety and depression."
134,Gene editing tool kit in millets: present status and future directions,"Millets, the sixth most-grown group of crops in the drylands, support the livelihood of many small-holder farmers in the region. Being one of the most nutritious groups of crops, their production has been increasing since the last decade to meet the demands of the world’s ever-increasing population. Since its discovery, CRISPR/Cas-mediated gene editing technology has revolutionized trait improvement in numerous crops by enabling targeted insertions and deletions at specific gene sequences. With advancements like base editing and prime editing, which offer precise modifications at the nucleotide level, this technology holds great promise for enhancing millets by targeting genes responsible for key traits. The updated sequence information in the public domain makes it possible to modify certain genic regions using the CRISPR/Cas-mediated gene editing technology to develop millet crops with improved agronomical properties. The review explores each component of the editing toolbox in millets, including the gRNA designing tools, types of Cas nucleases, and promoters to be considered for enhanced and efficient gene editing in millets. We have discussed fundamental information available to successfully employ CRISPR/Cas-mediated gene editing in millets, such as the availability of genomic information and plant transformation methods. Finally, we have highlighted the limitations of employing this novel technology in millet crops by providing future directions and immediate candidate genes that could be targeted to improve various traits in millet crops.Graphical abstract","Millets, the sixth largest group of crops cultivated in arid regions, are crucial for the livelihood of numerous small-scale farmers in these areas. Known for their high nutritional value, millet cultivation has surged over the past decade to address the needs of the world's rapidly growing population. The advent of CRISPR/Cas gene editing has transformed the enhancement of traits in multiple crops by allowing precise insertions and deletions at specific gene sequences. With advanced techniques such as base editing and prime editing that enable exact nucleotide modifications, this technology shows significant promise in improving millets by targeting genes linked to important characteristics. The updated sequence data available publicly facilitates the modification of specific gene regions using CRISPR/Cas technology to develop millet varieties with superior agronomic traits. The review delves into each aspect of the gene editing toolkit for millets, including gRNA design tools, various types of Cas nucleases, and the promoters required for more effective and efficient gene editing. Fundamental information essential for the successful application of CRISPR/Cas gene editing in millets is discussed, such as genomic data availability and plant transformation techniques. Additionally, the review highlights the challenges of implementing this innovative technology in millets and outlines future directions and potential gene targets for enhancing various millet traits.","In arid regions, the sixth most widely cultivated crop category, millets, serves as a vital source of income for numerous small-scale farmers. As one of the most nutrient-dense crop groups, their cultivation has experienced a surge in recent years to cater to the escalating global population. The groundbreaking CRISPR/Cas-mediated gene editing technology has transformed crop trait enhancement by facilitating precise insertions and deletions at specific genetic loci. With the emergence of base editing and prime editing, which enable exact modifications at the nucleotide level, this technology holds immense potential for upgrading millets by targeting genes responsible for critical characteristics. The publicly available updated genomic sequence data enables the modification of specific genic regions using CRISPR/Cas-mediated gene editing to develop millet varieties with enhanced agronomic features. This review delves into each component of the gene editing toolkit in millets, encompassing gRNA design tools, types of Cas nucleases, and promoters for optimized gene editing. We have outlined the fundamental knowledge required to successfully implement CRISPR/Cas-mediated gene editing in millets, including genomic data availability and plant transformation methods. Lastly, we have underscored the limitations of employing this innovative technology in millet crops, providing future research directions and immediate candidate genes that could be targeted to enhance various traits in millet crops.","As a significant food source in arid regions, millets are cultivated extensively, sustaining a large population of small-scale farmers. Recognized for their nutritional value, millet production has seen a surge in recent years to cater to the demands of a growing global population. The advent of CRISPR/Cas gene editing has fundamentally altered crop improvement strategies, allowing for precise genetic modifications. Further innovations like base and prime editing, enabling nucleotide-level changes, hold immense potential for enhancing desirable characteristics in millets by manipulating genes associated with key traits. Utilizing publicly available genomic data, researchers can now leverage CRISPR/Cas technology to modify specific genetic regions, ultimately cultivating millets with superior agricultural attributes. This comprehensive review delves into the various aspects of the CRISPR/Cas editing toolkit for millets, examining guide RNA design tools, Cas nucleases, and promoters for optimized gene editing. Additionally, it provides essential information for the effective implementation of CRISPR/Cas in millets, including available genomic resources and plant transformation techniques. Finally, the review addresses the challenges associated with applying this cutting-edge technology to millet crops, suggesting future research directions and pinpointing promising candidate genes for enhancing various millet traits."
181,Entrepreneurial finance and regional ecosystem emergence,"We present a novel framework for studying the evolving role of entrepreneurial finance over the stages of emergence for a regional entrepreneurial ecosystem. Drawing on entrepreneurial ecosystems, entrepreneurial finance, and territorial servitization, we explore how three different finance sources impact firm survival and how they relate to each other during ecosystem emergence. We analyze entrepreneurial firms in one industry and region over 36 years. We find that firm survival is differentially affected by funder type based on the stage of ecosystem emergence. Finance sources also have different interrelations depending on the stage of emergence. Based on our results, we abductively articulate a framework for stage-dependent ecosystem emergence microfoundations. This rectifies contradictory results that examine single sources of finance and use cross-sectional data. Had we not measured the emergence process, the results would have led to markedly different theoretical implications and practical takeaways for entrepreneurial finance and ecosystem emergence.","We introduce an innovative framework for examining how entrepreneurial finance evolves with the stages of development in a regional entrepreneurial ecosystem. By integrating concepts from entrepreneurial ecosystems, finance, and territorial services, we investigate the impact of three distinct financial sources on firm survival and their interrelations during the ecosystem's formation. Our study spans 36 years and focuses on entrepreneurial firms within a specific industry and region. Our findings suggest that the influence of different financial sources on firm survival varies with the ecosystem's emergence stage. Additionally, the relationships among these financial sources change depending on the stage of development. Using our findings, we propose a framework outlining the microfoundations for stage-dependent ecosystem emergence. This approach resolves conflicting results from studies that consider single finance sources and rely on cross-sectional data. Without accounting for the emergence process, our theoretical and practical implications for entrepreneurial finance and ecosystem development would have been significantly different.","Here is a paraphrased version of the original text, exhibiting both syntactic and lexical diversity while preserving semantic similarity: ""This study introduces an innovative paradigm for examining the dynamic influence of entrepreneurial finance across the developmental phases of a regional entrepreneurial ecosystem. By integrating insights from entrepreneurial ecosystems, entrepreneurial finance, and territorial servitization, we investigate how three distinct funding sources affect firm viability and interact with one another during ecosystem evolution. Our analysis focuses on entrepreneurial ventures within a single industry and region over a 36-year period. The findings reveal that firm survival is disproportionately impacted by the type of funder, contingent upon the ecosystem's stage of development. Furthermore, the interdependencies between finance sources vary according to the emergence stage. Based on our results, we inductively develop a framework for stage-contingent ecosystem emergence microfoundations, thereby resolving inconsistencies in previous research that examined individual finance sources using cross-sectional data. The omission of emergence process measurement would have led to substantially different theoretical and practical implications for entrepreneurial finance and ecosystem development.""","This study introduces a new approach to understanding the dynamic relationship between entrepreneurial finance and the development of regional entrepreneurial ecosystems. By integrating theories of entrepreneurial ecosystems, entrepreneurial finance, and territorial servitization, we investigate the influence of three distinct funding sources on firm survival and their interplay across various ecosystem development stages. Our longitudinal analysis of entrepreneurial firms within a specific industry and region over 36 years reveals that the impact of funding type on firm survival varies according to the ecosystem's stage of emergence. Moreover, the relationships between these funding sources also differ depending on the stage of development. These findings inform the development of a framework that illuminates the stage-specific microfoundations of ecosystem emergence, addressing inconsistencies found in previous studies that focused on single funding sources and cross-sectional data. Examining the emergence process longitudinally was crucial, as neglecting it would have resulted in significantly different theoretical interpretations and practical implications for both entrepreneurial finance and ecosystem development."
46,A Model for Digital Education Management Information System Using Wireless Communication and BP Neural Networks,"With the popularity of wireless communication and different mobile intelligent terminals, digital education management information systems are becoming increasingly popular among students. The problem of modernized teaching has significantly influenced and altered teaching quality evaluation methodologies and models. However, the findings revealed that the existing intelligent teaching quality diagnostic still needs to improve design and execution. As a result, creating an education management information system is critical, and establishing a digital education management information system has become a critical undertaking. To address these problems, this research paper presents a model for a digital education management information system (DEMIS) that utilizes wireless communication and BP neural networks (BPNNs) to manage digital education in schools. The paper highlights the significance of wireless communication and DEMIS in modern education. The study employs a grey prediction model and a BP neural network model to predict the trend of education development. The DEMIS is set up using MATLAB simulation with management constraint parameters, and the sample data originate from 85 instructors’ digital education assessment ratings. The experiment is carried out in two parts: using the grey prediction model and the BPNNs, which verify the suggested method’s efficacy in managing digital education in schools. The experimental findings suggest that, by combining wireless communication and BPNNs, the system can handle the campus network’s information in a unified manner and give decision assistance to managers, which is very practicable. Therefore, the suggested DEMIS may effectively enhance students’ learning experiences and education quality.","As wireless communication and various mobile smart devices gain traction, digital education management information systems are becoming more prevalent among students. Modern teaching's evolution has notably impacted teaching quality assessment techniques and frameworks. Nonetheless, research indicates that current intelligent teaching quality diagnostics require further refinement in design and implementation. Consequently, developing an education management information system is pivotal, making the establishment of a digital education management information system a crucial endeavor. This study introduces a digital education management information system (DEMIS) model leveraging wireless communication and BP neural networks (BPNNs) to oversee digital education in schools. It underscores the importance of wireless communication and DEMIS in contemporary education. The investigation employs both a grey prediction model and a BP neural network model to forecast education development trends. DEMIS is implemented through MATLAB simulation with specific management constraint parameters, using sample data from the digital education assessments of 85 instructors. The experiment is divided into two phases: utilizing the grey prediction model and the BPNNs, which confirm the proposed method's effectiveness in administrating digital education in schools. Results indicate that integrating wireless communication with BPNNs allows the system to manage campus network information cohesively and provides decision support to administrators, proving to be highly feasible. Consequently, the proposed DEMIS can significantly improve students' learning experiences and the quality of education.","The proliferation of wireless communication and diverse mobile intelligent devices has led to a surge in the adoption of digital education management information systems among students, significantly impacting the assessment methodologies and frameworks of modernized teaching. However, the current intelligent teaching quality diagnostic tools require enhancements in design and implementation. Consequently, the development of an education management information system has become imperative, and establishing a digital education management information system is a pressing necessity. This study proposes a novel digital education management information system (DEMIS) model that leverages wireless communication and back-propagation neural networks to optimize digital education in educational institutions. The significance of wireless communication and DEMIS in contemporary education is underscored. The research employs a grey prediction model and a back-propagation neural network model to forecast educational development trends. The DEMIS is established using MATLAB simulation with management constraint parameters, utilizing sample data from 85 instructors' digital education assessment ratings. The experiment is conducted in two phases, utilizing the grey prediction model and back-propagation neural networks, which validate the proposed method's effectiveness in managing digital education in educational institutions. The experimental results indicate that the system can integrate campus network information and provide decision support to administrators in a unified manner, making it highly practical. Thus, the proposed DEMIS has the potential to significantly enhance students' learning experiences and education quality.","The rise of mobile technology has led to increased adoption of digital education management systems in education. This shift towards modern teaching practices has significantly impacted how teaching quality is evaluated. However, current methods for intelligent teaching quality diagnosis require improvement. Thus, building robust digital education management information systems is crucial. This study proposes a digital education management information system (DEMIS) model that leverages wireless communication and BP neural networks for effective school management. The paper emphasizes the importance of wireless communication and DEMIS in contemporary education. Using grey prediction and BP neural network models, the study forecasts educational development trends. MATLAB simulations, incorporating management constraints and assessment data from 85 instructors, were used to implement DEMIS. The efficacy of the proposed method was validated through experiments using grey prediction and BPNNs, demonstrating its capability in managing digital education. The results highlight that integrating wireless communication and BPNNs allows the system to manage campus network information effectively and offer valuable decision support for administrators. This suggests that the proposed DEMIS can significantly improve student learning experiences and overall education quality."
116,Learning to Lead for Equity and Social Justice through Critical Reflection and Autobiography,"In this paper, the authors utilize critical reflection and autobiographical narratives as a pedagogical tool for aspiring school leaders to examine beliefs and assumptions on equity and social justice in an educational leadership preparation program. Comparative themes related to their developing social justice and equity orientations included: (a)Understanding heritage and lived experience in relation to inequity and injustice, (b) Positionality awareness as a meaning making tool for leadership development, and (c) Praxis: Aspiration for action in one’s leadership role. Incorporating pedagogical tools that engage in difficult conversations with the self and the world has the potential to provided avenues for students to become active interpreters of their past histories, current aspirations and heighten their capacity to be active agents of justice.","In this study, the authors use critical reflection and personal narratives as teaching strategies for future school leaders to scrutinize their beliefs and assumptions about equity and social justice within an educational leadership training program. Comparative themes concerning their evolving perspectives on social justice and equity included: (a) Understanding heritage and personal experiences in relation to inequity and injustice, (b) Recognizing positionality as a tool for making sense of leadership development, and (c) Praxis: Aspiration for action in leadership. Employing pedagogical tools that engage in challenging self-reflection and societal dialogue can provide pathways for students to actively interpret their past experiences, future goals, and enhance their ability to act as agents of justice.","This study leverages autobiographical storytelling and critical introspection as an instructional strategy within an educational leadership preparation program, empowering prospective school administrators to scrutinize their beliefs and presuppositions regarding equity and social justice. The emerging themes, which illuminate the evolution of their social justice and equity perspectives, encompass: (a) recognizing the interplay between personal heritage, lived experience, and systemic injustices, (b) cultivating self-awareness of one's positionality as a catalyst for informed leadership, and (c) translating vision into action through deliberate praxis. By integrating pedagogical approaches that facilitate candid self-reflection and engagement with the world, this method has the potential to enable students to reinterpret their past, reframe their current aspirations, and amplify their capacity to drive positive change as agents of justice.","This study explores the use of critical reflection and personal narratives as a pedagogical approach to foster the development of social justice and equity perspectives in aspiring school leaders. Through these reflective practices, participants engaged in a process of examining their own beliefs and assumptions about equity and social justice within the context of an educational leadership program. Three key themes emerged from their evolving understanding: (1) recognizing the interconnectedness of personal heritage, lived experiences, and systemic inequities, (2) developing awareness of their own positionality as a lens for understanding leadership, and (3) translating these insights into actionable plans for promoting justice in their leadership roles. By integrating pedagogical tools that encourage introspection and critical dialogue, the program aims to empower students to critically analyze their past experiences, define their aspirations for social change, and ultimately strengthen their ability to be active advocates for justice."
37,Decentralization System Using Smart Blockchain with Secure Hash,"Blockchain decentralization is a reference to decision making and transferring of data control. It is derived as a distributed network from a centralized network, either in an individual, organization, or group. The decentralized system reduces the user’s trust level, which has to be shown by one another. Instead, they confine the functionality of the system network to be degraded. This determines the ability to control the decentralized system using smart contracts. In this paper, we propose a blockchain decentralization system on the smart contracts produced by the blocks. This is called smart blockchain. Smart blockchain or Smart chain is the new generation of blockchain network that allows one or more than a smart contract. The decentralized system of smart contracts here collects the data accurately and flawlessly. The decentralized nature of smart blockchain ensures that there is no single point of control or failure, making it resistant to tampering, hacking, and other malicious activities. Simultaneously this decentralization method helps store and register the data in new blocks. The complete process of a transaction is automatic, without any human interaction. The blockchain used in networks like Ethereum, etc., can be entirely replaced by the smart blockchain or smart chain. The deployment and registration done in smart contracts are overcome with the smart blockchain. We have worked on the four processes in this paper. They are new_block, new_transaction, last_block, and hash. Using the flask framework, the requests are communicated and satisfied on the HTTP Protocol. The further assessment of this decentralized system enabling the smart blockchain can bring a difference in application research, such as Smart Homes using IoT Technology. Smart blockchain technology evaluates security, complexity, privacy, integrity, and implementation protocols. Additionally, we experimentally show the protocols’ outperformance towards multiple blocks of a blockchain.","Blockchain decentralization refers to the distribution of decision-making and control over data. It evolves from a centralized structure, whether it involves an individual, organization, or group, into a distributed network. In a decentralized system, user trust does not have to be placed in one another; rather, the functionality of the network is limited. This allows for the control of the decentralized system via smart contracts. This paper introduces a blockchain decentralization approach based on smart contracts linked to blocks, known as a smart blockchain. Smart blockchain, or Smart chain, represents the next evolution of blockchain technology, enabling the use of multiple smart contracts. This decentralized system of smart contracts ensures precise and flawless data collection. The decentralized characteristic of smart blockchain eliminates single points of control or failure, enhancing resilience against tampering, hacking, and other malicious activities. Simultaneously, this decentralization strategy supports the storage and registration of data in new blocks. Transactions are processed automatically without human intervention. The smart blockchain, or Smart chain, can fully substitute blockchain networks like Ethereum. We address four processes in this study: new_block, new_transaction, last_block, and hash. Using the Flask framework, requests are communicated and fulfilled over HTTP Protocol. Further exploration of this decentralized system facilitating the smart blockchain can impact application research areas, including Smart Homes using IoT Technology. Smart blockchain technology assesses security, complexity, privacy, integrity, and implementation protocols. Additionally, our experiments demonstrate the superior performance of the protocols across multiple blockchain blocks.","Here is a paraphrased version of the original text, exhibiting both syntactic and lexical diversity while preserving semantic similarity: ""The concept of blockchain decentralization revolves around the redistribution of authority and data governance. This paradigm shift transforms a centralized network into a distributed one, where decision-making power is relinquished from a single entity, be it an individual, organization, or group. By doing so, the decentralized system minimizes the need for mutual trust among users, instead relying on the inherent functionality of the network to prevent degradation. This, in turn, enables the utilization of smart contracts to regulate and control the decentralized system. This paper proposes a novel blockchain decentralization framework, dubbed 'smart blockchain' or 'smart chain,' which leverages smart contracts generated by blocks to facilitate accurate and seamless data collection. The decentralized nature of smart blockchain ensures that there is no single point of vulnerability, rendering it impervious to tampering, hacking, and other malicious activities. Concurrently, this decentralization approach enables the secure storage and registration of data in new blocks, with transactions being executed automatically, sans human intervention. The smart blockchain technology has the potential to supplant traditional blockchain networks, such as Ethereum, and overcome the limitations of smart contract deployment and registration. Our research focuses on four key processes: new_block, new_transaction, last_block, and hash, which are facilitated through the Flask framework and communicated via HTTP Protocol. The assessment of this decentralized system, enabled by smart blockchain, is poised to revolutionize application research, including IoT-powered Smart Homes. Furthermore, our evaluation of smart blockchain technology examines its security, complexity, privacy, integrity, and implementation protocols, demonstrating its superior performance across multiple blockchain blocks.""","Distributed decision-making and data control are central to blockchain decentralization. This concept arises from the shift of a centralized network, controlled by a single entity, organization, or collective, to a distributed one. This distribution diminishes the need for user trust, as reliance on a central authority is minimized. However, this decentralization can potentially limit system functionality. Control over this decentralized system is achieved through smart contracts. This work introduces a novel blockchain decentralization system built on smart contracts generated by blocks, termed ""smart blockchain"" or ""smart chain."" This next-generation blockchain network accommodates one or more smart contracts, facilitating precise and reliable data collection through its decentralized structure. The absence of a single point of control or vulnerability makes smart blockchain resilient to tampering, hacking, and other malicious acts. This decentralized system also enables data storage and registration in new blocks, with the entire transaction process automated, eliminating human intervention. Networks currently utilizing blockchain, such as Ethereum, could potentially be replaced with smart blockchain or smart chain. The deployment and registration challenges associated with smart contracts are addressed within the smart blockchain framework. This study focuses on four key processes: new_block, new_transaction, last_block, and hash, using the Flask framework for communication and fulfillment of requests via HTTP protocol. Further exploration of this smart blockchain-enabled decentralized system holds significant potential for advancements in various application domains, including IoT-based Smart Homes. Smart blockchain technology offers advantages in security, complexity management, privacy, data integrity, and implementation protocols, with experimental results demonstrating its superior performance across multiple blockchain blocks."
164,Spreading cybersecurity awareness via gamification: zero-day game,"This research project aims to increase the employees' cybersecurity awareness by proposing a training program that consists of pre-game and post-game phishing attacks, pre-game and post-game surveys, and an interactive zero-day game with embedded threat scenarios. Furthermore, we propose five flowcharts that simulate common cyberattacks. The zero-day game consists of five levels. The first level trains employees in using complex passwords. Two levels cover the social engineering attacks: one level trains employees about suspicious links and online survey tricks, while the other level trains players about the organization's security policies. The fourth level educates employees about malware spread via repackaged fake applications. The last level educates employees about phishing email attacks. Furthermore, we developed a chatbot inside the game to aid users in playing the different game levels. Additionally, two surveys and two phishing attacks are carried out to assess the employees' knowledge level of cybersecurity threats. The zero-day cybersecurity awareness program is applied to twenty-three employees. Experimental results indicate that the zero-day cybersecurity awareness program is more engaging and effective than traditional awareness programs. Results also indicate that the proposed program accurately measures and improves the employees' cybersecurity awareness level.","This study seeks to enhance employee cybersecurity awareness by implementing a training initiative that includes pre- and post-game phishing simulations, surveys, and an interactive zero-day game featuring embedded threat scenarios. Additionally, we introduce five flowcharts to illustrate typical cyberattacks. The zero-day game comprises five stages: the initial stage teaches complex password usage; two stages focus on social engineering, instructing employees on identifying suspicious links, online survey scams, and understanding the organization's security policies; the fourth stage addresses malware dissemination through repackaged fake apps; and the final stage covers phishing email training. We also integrated a chatbot within the game to support users through the different stages. Moreover, two surveys and phishing attempts are conducted to evaluate the employees' cybersecurity threat awareness. This zero-day program was applied to twenty-three employees, with experimental data showing it to be more engaging and effective compared to traditional training methods. The findings also show that the program effectively measures and enhances the employees' cybersecurity awareness.","This study seeks to enhance workforce cybersecurity literacy by introducing a comprehensive training initiative, comprising pre- and post-intervention phishing simulations, surveys, and an immersive zero-day gaming experience featuring embedded threat scenarios. Additionally, five flowcharts are proposed to mimic prevalent cyber threats. The zero-day game is structured into five tiers, with the inaugural level focusing on complex password management, followed by two levels addressing social engineering tactics, including suspicious links and organizational security protocols. The fourth tier educates participants about malware dissemination through repackaged fake applications, while the final tier concentrates on phishing email attacks. A built-in chatbot is integrated into the game to facilitate user navigation. To evaluate employee knowledge of cybersecurity threats, two surveys and two phishing simulations are conducted. The zero-day cybersecurity awareness program is piloted among twenty-three employees, with experimental findings suggesting that this approach is more captivating and effective than traditional awareness initiatives, accurately assessing and improving employee cybersecurity awareness.","This study endeavors to enhance employee cybersecurity awareness through a novel training program. This program utilizes a multi-faceted approach, incorporating simulated phishing attacks both before and after training, pre- and post-training surveys to gauge knowledge, and an interactive, multi-level ""zero-day"" game designed to immerse employees in realistic threat scenarios. Five distinct flowcharts are also introduced to illustrate common cyberattack methods. The zero-day game is structured across five levels, each addressing specific cybersecurity skills. The initial level focuses on building robust password practices. Two levels are devoted to social engineering, with one emphasizing suspicious link and online survey recognition, and the other highlighting organizational security policies. The fourth level tackles malware dissemination through deceptive applications, while the final level centers on recognizing and avoiding phishing emails. An in-game chatbot provides support and guidance to players navigating the various levels. To evaluate the program's impact on cybersecurity knowledge, two phishing exercises and surveys are conducted. This zero-day awareness program was implemented with a group of 23 employees. Findings suggest this program fosters greater engagement and effectiveness compared to conventional awareness training, accurately measuring and enhancing employee cybersecurity awareness."
190,Direct and Indirect Opportunities to Learn Astronomy Within the Chilean Science Curriculum,"Astronomy has great potential to attract children toward science and improve their scientific literacy. However, it has a relatively small presence within the school curricula worldwide. In Chile, home of the world’s largest telescopes, astronomy is even more relevant in science education, but the presence of astronomical content within the national curriculum was not studied yet. In this context, this article characterizes the opportunities to learn astronomy within the Chilean science curriculum from grades 1 to 12. A mixed descriptive design with documental analysis is conducted to determine which science learning objectives (LOs) are directly or indirectly related to astronomy and classify them according to astronomical topics, cognitive processes, and Big Ideas in Astronomy. The proportion of LOs related to astronomy within the whole science curriculum and the categories of interest are calculated and compared over the school grades. The results show that the LOs directly related to astronomy are present only in a third of the grades and include a small variety of astronomical topics. Meanwhile, LOs indirectly related to astronomy appear in all grades and include more topics and higher cognitive processes. We discuss the implications of the outcomes and raise some possibilities to promote astronomy literacy through interdisciplinary work.","Astronomy holds significant promise for engaging children in science and enhancing their scientific understanding, yet it occupies a limited role in global school curricula. In Chile, renowned for its large telescopes, astronomy's importance in science education is heightened, though the extent of its inclusion in the national curriculum has not been thoroughly examined. This study investigates the presence of astronomical education in Chilean science curricula from grades 1 to 12. Utilizing a mixed descriptive model and document analysis, the study identifies and classifies science learning objectives (LOs) related to astronomy based on topics, cognitive processes, and overarching astronomical concepts. The analysis determines the proportion of astronomy-related LOs within the entire science curriculum, comparing them across different grade levels. Findings indicate that directly astronomy-related LOs are found in only a third of the grades and cover a limited range of topics. Conversely, indirectly related LOs are present in all grades, encompassing a broader array of topics and more complex cognitive processes. The paper discusses the implications of these findings and suggests ways to enhance astronomy literacy through cross-disciplinary approaches.","The field of astronomy possesses immense capacity to captivate young minds and foster a deeper understanding of scientific concepts, yet its representation in educational curricula globally remains limited. In Chile, a nation renowned for its colossal telescopes, the significance of astronomy in science education is amplified, although the extent of its incorporation into the national curriculum has not been explored. This study delves into the opportunities for astronomy education within Chile's science curriculum, spanning grades 1-12, employing a mixed descriptive design and document analysis to identify science learning objectives linked to astronomy, categorized by astronomical themes, cognitive processes, and fundamental astronomy concepts. The proportion of astronomy-related objectives within the broader science curriculum and their distribution across grade levels are calculated and compared. The findings reveal that direct astronomy-related objectives are confined to a third of the grades, encompassing a narrow range of topics, whereas indirect connections to astronomy are present throughout all grades, incorporating a broader spectrum of topics and higher-order cognitive skills. The implications of these outcomes are discussed, and potential avenues for promoting astronomy literacy through interdisciplinary approaches are proposed.","While astronomy holds immense promise for engaging children in science and fostering scientific understanding, its integration into school curricula globally remains limited. This is particularly noteworthy in Chile, a nation housing some of the world's most advanced telescopes, where astronomy's educational significance is paramount. Despite this, the extent to which astronomical content features in the Chilean national curriculum remains unexplored. This study, therefore, endeavors to identify opportunities for learning astronomy within the Chilean science curriculum from primary to secondary education (grades 1 to 12). Employing a mixed descriptive methodology with document analysis, the research identifies science learning objectives that have direct or indirect links to astronomy. These objectives are then categorized based on astronomical themes, cognitive processes involved, and fundamental astronomical concepts. The study quantifies and compares the proportion of astronomy-related learning objectives within the broader science curriculum across different grade levels, focusing on specific categories of interest. Findings reveal that learning objectives directly addressing astronomy are found only in a limited number of grades and encompass a restricted range of astronomical topics. Conversely, learning objectives indirectly related to astronomy are present throughout all grades and encompass a wider array of topics and demand more sophisticated cognitive processes. The implications of these results are discussed, along with potential avenues for enhancing astronomy literacy through interdisciplinary approaches."
156,Are transport networks in low-income countries prepared for climate change? Barriers to preparing for climate change in Africa and South Asia,"Climate change, through extreme weather events and slow onset climatic changes, disrupts the operation of transport networks, and those in low-income countries (LICs) across Africa and South Asia are particularly vulnerable to climate change. This paper explores the barriers that LICs face across Africa and South Asia regarding preparedness of transport infrastructure to climate change, with the intent of addressing the knowledge gaps and consequential needs of LICs to support the delivery of more climate-resilient transport. Literature on climate change adaptation and transport resilience among LICs is reviewed to identify the broad challenges and barriers regarding climate change adaptation. Semi-structured interviews with 13 transport stakeholders across Africa and South Asia were also conducted to understand the challenges specific to the transport sector in the regions. Several barriers were identified, including a lack of data and knowledge on climate change impacts, design and prioritisation of remedial actions, budgeting and planning for climate change, and identifying and engaging with stakeholders. Findings from this paper and the wider research project it comprises inform policy guidance that calls for greater national and international coordination to develop practical, relevant and usable data, tools, advice and support for some of the most at-risk transport networks to climate change in the world.","This study investigates the obstacles that low-income countries (LICs) in Africa and South Asia encounter in preparing their transport infrastructure for climate change impacts. Climate change, through extreme weather and gradual climatic shifts, disrupts transport systems, especially in LICs in these regions. The paper aims to fill knowledge gaps and address the needs of LICs to facilitate the implementation of more climate-resilient transport solutions. By reviewing literature on climate change adaptation and transport resilience in LICs and conducting semi-structured interviews with 13 transport stakeholders from Africa and South Asia, the research identifies several barriers. These include insufficient data and knowledge on climate change impacts, challenges in designing and prioritizing responses, budgetary and planning obstacles, and difficulties in stakeholder identification and engagement. The findings inform policy recommendations that emphasize the need for enhanced national and international cooperation to develop useful data, tools, advice, and support for the world's most climate-vulnerable transport networks.","The devastating impacts of climate change, manifested through catastrophic weather events and gradual climatic shifts, severely impede the functioning of transportation systems, with those in low-income countries across Africa and South Asia being disproportionately susceptible to its effects. This study delves into the obstacles faced by these nations in preparing their transportation infrastructure to withstand the challenges posed by climate change, aiming to bridge the knowledge gaps and address the pressing needs of these countries to develop more climate-resilient transportation systems. A comprehensive review of existing literature on climate change adaptation and transportation resilience in low-income countries reveals the overarching challenges and barriers hindering climate change adaptation efforts. Additionally, in-depth interviews with 13 transportation stakeholders across Africa and South Asia provide insight into the sector-specific challenges in these regions. The findings highlight several key barriers, including inadequate data and knowledge on climate change impacts, ineffective design and prioritization of remedial measures, insufficient budgeting and planning for climate change, and difficulties in identifying and engaging relevant stakeholders. The results of this study and the broader research project inform policy recommendations that emphasize the need for enhanced national and international collaboration to develop practical, relevant, and accessible data, tools, guidance, and support for the world's most climate-vulnerable transportation networks.","The effects of climate change, ranging from sudden extreme weather to gradual shifts in climate patterns, significantly impact transportation systems. Developing nations in Africa and South Asia are especially susceptible to these disruptions. This study investigates the obstacles these countries face in preparing their transportation infrastructure for climate change, aiming to fill knowledge gaps and address their specific needs for building more resilient transportation systems. Existing research on climate adaptation and transport resilience in these regions is analyzed to pinpoint overarching challenges and barriers. To gain a deeper understanding of the transport sector's unique challenges, in-depth interviews were conducted with thirteen transport stakeholders across both continents. The study revealed several key obstacles, including insufficient data and knowledge about the impact of climate change, difficulties in designing and prioritizing remedial actions, inadequate budgeting and planning for climate change, and challenges in identifying and engaging with relevant stakeholders. The findings of this research, part of a larger project, contribute to policy recommendations advocating for increased national and international collaboration. This collaborative effort aims to develop practical, relevant, and user-friendly data, tools, guidance, and support for some of the world's most vulnerable transportation networks facing the impacts of climate change."
121,A semi-automated software model to support AI ethics compliance assessment of an AI system guided by ethical principles of AI,"Compliance with principles and guidelines for ethical AI has a significant impact on companies engaged in the development of artificial intelligence (AI) systems. Specifically, ethics is a broad concept that continuously evolves over time and across cultural and geographical boundaries. International organisations (IOs), individual states, and private groups, all have an interest in defining the concept of ethics of AI. IOs, as well as regional and national bodies, have issued many decisions on AI ethics. Developing a system that complies with the ethical framework poses a complex challenge for companies, and the consequences of not complying with ethical principles can have severe consequences, making compliance with these requirements a key issue for companies. Furthermore, there is a shortage of technical tools to ensure that such AI systems comply with ethical criteria. The scarcity of ethics compliance checking tools for AI, and the current focus on defining ethical guidelines for AI development, has led us to undertake a proposal consisting in a semi-automated software model to verify the ethical compliance of an AI system’s code. To implement this model, we focus on the following important aspects: (1) a literature review to identify existing ethical compliance systems, (2) a review of principles and guidelines for ethical AI to determine the international and European views regarding AI ethics, and (3) the identification of commonly accepted principles and sub-principles of AI. These elements served to inform (4) our proposal for the design of a semi-automated software for verifying the ethical compliance of AI systems both at design-time (ethics-by-design perspective) and afterwards on the resulting software.","Adhering to ethical AI principles and guidelines profoundly affects companies involved in creating artificial intelligence (AI) systems. Ethics, a broad and ever-changing concept, varies across cultures and regions. International organizations (IOs), governments, and private entities all strive to define AI ethics. Numerous decisions on AI ethics have been issued by IOs, regional, and national bodies. Crafting a compliant system within this ethical framework presents a considerable challenge for companies, with significant repercussions for non-compliance, making adherence to these standards critically important. Additionally, there is a lack of technical tools to ensure AI systems meet ethical standards. The deficiency of such compliance-checking tools, coupled with the ongoing effort to define ethical guidelines for AI development, has prompted us to propose a semi-automated software model to verify the ethical adherence of AI system code. To develop this model, we consider: (1) a literature review to identify current ethical compliance systems, (2) an analysis of ethical AI principles and guidelines to capture international and European perspectives on AI ethics, and (3) the identification of widely accepted AI principles and sub-principles. These considerations informed (4) our design proposal for a semi-automated software to verify ethical compliance of AI systems during design (ethics-by-design approach) and post-development.","The adherence to fundamental tenets and directives governing the ethical development of artificial intelligence (AI) has a profound influence on organizations involved in creating AI systems. Ethics, a multifaceted and dynamic concept, undergoes continuous transformation across cultural, geographical, and temporal boundaries. A diverse range of stakeholders, including international organizations, nation-states, and private entities, strive to delineate the ethical paradigm for AI. Consequently, numerous regulatory bodies have issued decrees and guidelines pertaining to AI ethics. The development of AI systems that conform to ethical frameworks poses a formidable challenge for companies, and the repercussions of non-compliance can be severe, rendering adherence to these requirements a paramount concern. Moreover, a dearth of technical instruments exists to ensure AI systems conform to ethical standards. The scarcity of ethics auditing tools for AI, coupled with the prevailing emphasis on establishing ethical guidelines for AI development, has prompted us to propose a semi-automated software model for verifying the ethical compliance of AI system code. To actualize this model, we prioritize the following crucial aspects: (1) a comprehensive examination of existing ethical compliance systems through literature review, (2) an analysis of international and European perspectives on AI ethics, (3) the identification of universally accepted AI principles and sub-principles, and (4) the design of a semi-automated software for verifying AI systems' ethical compliance during both the design phase and post-development, adopting an ethics-by-design approach.","Adhering to ethical AI principles and guidelines is crucial for companies developing AI systems. However, the evolving nature of ethics, with variations across cultures and regions, complicates this process. Various stakeholders, including international organizations, national governments, and private entities, are actively shaping the ethical landscape of AI. Numerous pronouncements on AI ethics have emerged from these groups, creating a complex compliance challenge for companies. Failure to comply can lead to serious repercussions, highlighting the criticality of ethical adherence. Furthermore, a scarcity of technical tools for verifying ethical compliance in AI systems exacerbates this challenge. This lack of verification tools, coupled with the ongoing focus on defining ethical guidelines, motivated our development of a semi-automated software model for assessing the ethical compliance of AI system code. This model is grounded in a comprehensive review of existing ethical compliance systems, an analysis of international and European perspectives on AI ethics, and the identification of widely accepted ethical principles and sub-principles. These foundational elements informed the design of our proposed software, which aims to verify ethical compliance in AI systems both during the design phase (ethics-by-design) and post-development."
100,"Robots, labor markets, and universal basic income","Automation is a big concern in modern societies in view of its widespread impact on many socioeconomic issues including income, jobs, and productivity. While previous studies have concentrated on determining the effects on jobs and salaries, our aim is to understand how automation affects productivity, and how some policies, such as taxes on robots or universal basic income, moderate or aggravate those effects. To this end, we have designed an experiment where workers make productive effort decisions, and managers can choose between workers and robots to do these tasks. In our baseline treatment, we measure the effort made by workers who may be replaced by robots, and also elicit firm replacement decisions. Subsequently, we carry out treatments in which workers have a universal basic income of about a fifth of the workers’ median wages, or where there is a tax levy on firms who replace workers by robots. We complete the picture of the impact of automation by looking into the coexistence of workers and robots with part-time jobs. We find that the threat of a robot substitution does not affect the amount of effort exerted by workers. Also, neither universal basic income nor a tax on robots decrease workers’ effort. We observe that the robot substitution tax reduces the probability of worker substitution. Finally, workers that benefit from managerial decisions to not substitute them by more productive robots do not increase their effort level. Our conclusions shed light on the interplay of policy and workers behavior under pervasive automation.","The extensive influence of automation on various socioeconomic factors, including income, employment, and productivity, raises significant concerns in contemporary societies. While earlier research has primarily focused on evaluating the effects on job markets and wages, this study seeks to explore automation's impact on productivity and the role of specific policies, such as robot taxes or universal basic income, in moderating these outcomes. To achieve this, an experiment was structured where workers decide on their productivity levels, while managers select between employing humans or robots for tasks. In the initial setup, the effort levels of employees at risk of being replaced by robots and the firms' decisions to replace them are assessed. Following this, additional scenarios are tested in which employees either receive a universal basic income equating to approximately 20% of the median wages or firms incur taxes for substituting workers with robots. The study also examines the dynamic of workers and robots coexisting in part-time roles to complete the analysis. Findings indicate that the potential threat of being replaced by robots does not alter the workers' effort levels. Furthermore, neither universal basic income nor a robot tax impacts worker effort. However, the imposition of a robot substitution tax diminishes the likelihood of workers being replaced. Lastly, employees who are retained over robots by managerial choice do not necessarily increase their effort. The conclusions shed light on how policy interventions can influence worker behavior amidst prevalent automation.","""In contemporary societies, the far-reaching consequences of automation on various socioeconomic aspects, including income, employment, and productivity, have sparked significant concern. Whereas prior research has primarily focused on the implications of automation on job security and remuneration, our investigation seeks to elucidate the impact of automation on productivity and how policy interventions, such as robot taxation or universal basic income, influence this relationship. To achieve this, we designed an experiment in which workers make decisions about their productive effort, while managers choose between human labor and robotic alternatives. Our baseline scenario assesses the effort exerted by workers facing potential robot replacement, as well as firms' substitution decisions. Subsequent treatments introduce a universal basic income equivalent to one-fifth of the median wage or a tax on firms that replace workers with robots. We also examine the coexistence of workers and robots in part-time employment arrangements. Our findings indicate that the threat of robot substitution does not influence workers' effort levels. Furthermore, neither universal basic income nor robot taxation leads to decreased worker effort. However, the robot substitution tax reduces the likelihood of worker replacement. Notably, workers who benefit from managers' decisions to retain them over more productive robots do not exhibit increased effort. Our conclusions provide valuable insights into the complex interplay between policy and worker behavior in an era of pervasive automation.""","The pervasive nature of automation in contemporary societies has sparked significant concern due to its impact on various socioeconomic facets, including income, employment, and productivity. While prior research has primarily focused on the effects of automation on jobs and wages, this study seeks to understand how automation influences productivity and how policies such as robot taxes or universal basic income might mitigate or exacerbate these effects. An experiment was conducted in which workers made decisions regarding their productive effort, while managers had the option of selecting either workers or robots for task completion. The baseline treatment measured the effort exerted by workers who faced potential replacement by robots and elicited firm replacement decisions. Subsequent treatments introduced a universal basic income equivalent to approximately one-fifth of the median worker wage or a tax levied on firms that replaced workers with robots. The impact of automation was further explored by examining the coexistence of workers and robots in part-time job scenarios. The findings revealed that the possibility of robot substitution did not influence the level of effort exerted by workers. Moreover, neither universal basic income nor a robot tax resulted in a decrease in worker effort. However, the robot substitution tax was found to reduce the likelihood of worker replacement. Interestingly, workers who benefited from managerial decisions to retain them despite the availability of more productive robots did not exhibit increased effort levels. These conclusions provide insights into the intricate relationship between policy and worker behavior in an era of widespread automation."
199,Wearing the Same Jersey? The Impact of Players’ Cultural Diversity and Shared Team Tenure on National Soccer Team Performance,"In the present paper, we empirically analyze a dataset from national soccer teams between 2004 and 2019 to investigate the impact of team members’ cultural diversity on final team outcomes. Further, we examine the role of shared team tenure as a proxy of tacit knowledge and within team communications in relation to the cultural diversity–team performance relationship. After addressing a potential endogeneity issue, results from multiple instrumental variable estimation methods implied a higher level of cultural diversity enhanced on-field performance. Meanwhile, increased level of shared team tenure further strengthened the effect associated with cultural diversity on team performance.","In this study, we empirically examine data from national soccer teams spanning 2004 to 2019 to explore the influence of cultural diversity among team members on their ultimate performance outcomes. Additionally, we analyze the significance of shared team tenure as an indicator of implicit knowledge and internal communication within the team concerning the relationship between cultural diversity and team performance. Addressing potential endogeneity, results from various instrumental variable estimation techniques suggested that greater cultural diversity improved on-field outcomes. Furthermore, a higher level of shared team tenure amplified the positive impact of cultural diversity on team performance.","This study undertakes an empirical examination of a comprehensive dataset spanning 15 years, encompassing national soccer teams from 2004 to 2019, to elucidate the influence of cultural heterogeneity among team members on ultimate team achievements. Additionally, we probe the mediating role of collective team experience as a surrogate for implicit knowledge and intra-team collaboration in the context of the cultural diversity-team performance nexus. Following the mitigation of a potential endogeneity concern, our findings, derived from multiple instrumental variable estimation approaches, suggest that a more diverse cultural composition yields improved on-field outcomes, with an increased level of shared team experience serving to amplify this positive correlation.","This study leverages data from national soccer teams spanning 2004 to 2019 to explore the connection between a team's cultural mix and its overall success. The research also delves into the influence of shared team experience, viewed as a measure of implicit knowledge and internal communication, on the relationship between cultural diversity and team performance. Employing several instrumental variable estimation techniques to mitigate potential endogeneity concerns, the findings indicate that greater cultural diversity within a team bolsters its on-field performance. Furthermore, the analysis reveals that a longer shared team tenure amplifies the positive impact of cultural diversity on team performance."
109,Intelligent planning of controllers for improved resilience in multi-area system involving nuclear power,"Increased innovation on finding new ways to generate energy from different sources to meet the growing demand of consumers has led to various challenges in controlling the power network when it faces different disruptions. To address these challenges, a new approach has been proposed in this research paper, which combines a controller with a soft computing technique called Particle Swarm Optimization (PSO). The study considers a power system with four units, where three different energy sources are utilized and distributed across two areas. Each area has two power sources, with one area having a combination of thermal and gas power plants, and the other area consisting of a nuclear power plant and a gas power plant. Transmitting power from the nuclear power plant is particularly complex due to its high sensitivity to disturbances. Therefore, an intelligent and efficient controller is needed to ensure robust control in this type of power network that includes nuclear power. The paper also conducts a thorough analysis of the harmful emissions associated with electricity generation from the different power plants considered. The goal is to reduce the carbon footprint associated with power generation. The proposed work and analysis in the paper are implemented using the MATLAB/SIMULINK environment.","The rise in innovative methods to produce energy from various sources to satisfy increasing consumer demands has brought about numerous challenges in managing the power grid amid disruptions. This research paper introduces a novel strategy that integrates a controller with a soft computing technique called Particle Swarm Optimization (PSO) to tackle these issues. The investigation focuses on a power system comprising four units, where three distinct energy sources are distributed between two regions. Each region houses two power sources: one region has a mix of thermal and gas power plants, while the other contains a nuclear power plant and a gas power plant. Transmitting power from the nuclear plant proves especially challenging due to its high susceptibility to disturbances, necessitating an intelligent and efficient controller for reliable control in such a network. Additionally, the paper provides an in-depth examination of the emissions from the various power plants, aiming to minimize the carbon impact associated with power generation. The proposed methodology and analyses are carried out using the MATLAB/SIMULINK platform.","The escalating quest for novel energy harvesting methods from diverse sources to satiate the burgeoning consumer demand has precipitated a plethora of complexities in stabilizing the power grid amidst disparate disruptions. To mitigate these hurdles, this research paper pioneers an innovative paradigm that synergistically integrates a controller with the soft computing methodology of Particle Swarm Optimization (PSO). The investigation focuses on a quad-unit power system, wherein three disparate energy sources are harnessed and distributed across two distinct zones. Each zone boasts two power sources, with one zone featuring a hybrid thermal and gas power plant configuration, and the other comprising a nuclear power plant and a gas power plant. The transmission of power from the nuclear facility is particularly intricate due to its heightened susceptibility to perturbations, necessitating an intelligent and efficient controller to ensure robust governance in this nuclear-integrated power network. Furthermore, the paper undertakes an exhaustive examination of the noxious emissions associated with electricity generation from the various power plants under consideration, with the ultimate objective of minimizing the carbon footprint attendant to power generation. The proposed framework and analysis are actualized within the MATLAB/SIMULINK environment.","To handle the complex power grid management issues arising from the diversification of energy sources driven by increasing consumer demand, this research proposes a novel control strategy. This strategy leverages a controller enhanced by Particle Swarm Optimization, a sophisticated computational technique. The study focuses on a four-unit power system, encompassing three energy types distributed across two zones. Each zone utilizes two power sources, with one zone featuring both thermal and gas plants, while the other comprises a nuclear and a gas plant. The inclusion of a nuclear power plant, with its inherent susceptibility to disruptions, introduces significant challenges for power transmission. Consequently, an intelligent and high-performance controller is crucial for maintaining reliable control within such a nuclear-integrated power grid. Furthermore, the paper meticulously examines the detrimental emissions linked to electricity generation from each power plant type, aiming to curtail the overall carbon footprint of power production. This proposed methodology and its associated analysis are executed and validated through the MATLAB/SIMULINK platform."
176,"Households’ coping mechanisms with droughts and floods using finance, non-finance and the social safety net measures: evidence from Kenya","This study analysed households’ use of formal and informal finance, non-finance livelihood diversification and the social safety net measures in coping with droughts and floods. It employed a cross-sectional survey of 1370 households across 27 counties in Kenya that are prone to droughts and floods. Bivariate probit regressions reveal that households employ multiple coping measures related to finance, the social safety net and non-finance choices. The use of coping measures vary by household income, household dependency ratio, geographic and agro-climatic contexts, as well as the household head’s age and educational attainment. Further, the findings reveal that the use of the social safety net and non-finance coping mechanisms demonstrate complementarities in coping with droughts, suggesting that opportunities to benefit from the social safety net do not dampen livelihood diversification initiatives by the households. Additionally, households in arid and semi-arid lands (ASALs) depend to a large extent on the social safety net and non-finance livelihood diversification coping mechanisms, signalling the need to explore ways that encourage private sector development in promoting market-oriented coping strategies.","This research explored how households utilize both formal and informal financial options, diversify their livelihoods beyond financial means, and employ social safety net strategies to cope with droughts and floods. The study was based on a cross-sectional survey involving 1,370 households from 27 counties in Kenya susceptible to these environmental challenges. Using bivariate probit regressions, the analysis indicated that households adopt a variety of coping mechanisms involving finance, social safety nets, and non-financial strategies. The choice of coping mechanism is influenced by factors such as household income, dependency ratio, geographic setting, agro-climatic conditions, as well as the age and educational level of the household head. Moreover, the results show that social safety nets and non-financial coping strategies work complementarily in managing droughts, implying that access to social safety nets does not hinder households' diversification of livelihoods. Significantly, households in arid and semi-arid regions predominantly rely on social safety nets and non-financial diversification strategies, underscoring the importance of initiatives that foster private sector involvement in encouraging market-based coping mechanisms.","This research examined the strategies employed by households to mitigate the impacts of droughts and floods, including their utilization of formal and informal financial instruments, diversified livelihoods, and social protection programs. A comprehensive survey of 1370 households across 27 drought- and flood-prone counties in Kenya was conducted to gather insights. The analysis, which utilized bivariate probit regressions, revealed that households adopt a multifaceted approach to coping, combining financial, social, and non-financial measures. The adoption of these strategies varies significantly depending on factors such as household income, dependency ratio, geographical location, agro-climatic conditions, and the age and education level of the household head. Notably, the findings suggest that social protection programs and non-financial coping mechanisms exhibit synergies in responding to droughts, implying that access to social safety nets does not hinder households' efforts to diversify their livelihoods. Furthermore, households in arid and semi-arid regions rely heavily on social protection and non-financial diversification strategies, highlighting the need to explore initiatives that promote private sector growth and market-driven coping approaches.","This research investigated how households in drought and flood-prone regions of Kenya navigate these climatic shocks through various strategies. Utilizing data from a comprehensive survey of 1370 households across 27 counties, the study explored the utilization of formal and informal financial resources, engagement in non-financial livelihood diversification, and reliance on social safety net programs. Statistical analyses highlighted that households adopt a multifaceted approach to coping, drawing on a combination of financial, social, and non-financial mechanisms. The specific strategies employed were found to differ based on factors such as household income, the number of dependents, geographic location, agro-climatic conditions, and the age and education level of the household head. Notably, the study revealed a complementary relationship between social safety net programs and non-financial coping mechanisms, indicating that access to social support does not discourage households from diversifying their livelihoods. Furthermore, the research emphasized the significant dependence of households in arid and semi-arid lands (ASALs) on both social safety nets and non-financial diversification, underscoring the importance of exploring approaches that foster private sector growth to encourage market-driven coping strategies in these vulnerable regions."
76,An Extensive Analysis of Dye-Sensitized Solar Cell (DSSC),"In the 1800s, as the primary energy resource, the industrial revolution started with fossil fuels. Various research efforts have been carried out in finding an alternative for photovoltaic devices to traditional silicon (Si)-based solar cells. During the last three decades, dye-sensitized solar cells (DSSCs) have been investigated largely. DSSCs due to their simple preparation methodology, low cost, ease of production, and low toxicity have been used widely. This review helps the readers to fabricate the DSSC and to familiarize themselves with the operation and overview of DSSC principles and to improve the efficient possible routes for these devices to thrive and emerge. In addition to that, manufacturing, stability, and efficiency improvements need to be addressed in the future for these technologies to be discussed and to represent their suitability as a breakthrough of consumer electronics in the market. An overview of developing current prototypes of DSSC and products of the main companies is included.","In the 1800s, the industrial revolution was fueled primarily by fossil fuels. Researchers have continuously sought alternatives to traditional silicon (Si) solar cells for photovoltaic devices. Over the past thirty years, dye-sensitized solar cells (DSSCs) have been intensively studied due to their straightforward production methods, affordability, ease of manufacturing, and low toxicity. This review aims to guide readers in constructing DSSCs, understanding their operating principles, and exploring efficient pathways for their success. Additionally, it addresses the future challenges of improving manufacturing processes, stability, and efficiency for these technologies to make a significant impact in the consumer electronics market. It also provides an overview of the current prototypes of DSSCs and products developed by leading companies.","During the 19th century, the industrial revolution was sparked by fossil fuels, which served as the primary source of energy. In recent years, scientists have been actively seeking alternatives to traditional silicon-based solar cells, with a particular focus on photovoltaic devices. Over the past three decades, researchers have extensively explored dye-sensitized solar cells (DSSCs), which offer a range of advantages, including ease of fabrication, low production costs, and minimal environmental impact. This review aims to guide readers in the assembly and operation of DSSCs, as well as provide an in-depth understanding of their underlying principles. Furthermore, it highlights the need for future advancements in manufacturing, stability, and efficiency to ensure the widespread adoption of these devices in the consumer electronics market. The review also features an overview of current DSSC prototypes and products developed by leading companies.","The dawn of the industrial revolution in the 19th century was fueled by fossil fuels, the dominant energy source of the era. This reliance on fossil fuels has spurred extensive research into alternative energy sources, particularly in the field of photovoltaics, seeking to replace traditional silicon-based solar cells. Dye-sensitized solar cells (DSSCs) have emerged as a promising alternative over the past thirty years, gaining widespread attention due to their straightforward fabrication, cost-effectiveness, simple production process, and minimal environmental impact. This review aims to guide readers through the construction of DSSCs, providing a comprehensive understanding of their operational mechanisms and underlying principles, while exploring potential avenues for enhancing their efficiency and propelling their advancement. Furthermore, the review highlights the critical need to address manufacturing challenges, enhance stability, and improve efficiency to solidify the position of DSSCs as a groundbreaking technology in the consumer electronics market. An analysis of ongoing DSSC prototype development and an overview of products from leading companies in the field are also presented."
22,Virtual reality evaluation of the spatial learning strategies in gamers,"Psychological memory tests have traditionally relied on paper-based methods. However, the emergence of virtual reality tools, including adaptations of animal lab tests, has opened up new possibilities for evaluating and enhancing attention and memory processes. The impact of virtual environments on spatial memory remains a topic of ongoing debate. To contribute to this discussion, we conducted a comprehensive study with two primary objectives. Firstly, we aimed to develop a virtual reality tool that could effectively assess search strategies in gamers within virtual environments. To achieve this, we designed a virtual water maze inspired by those used in animal research. Secondly, we conducted a cross-sectional study involving participants to analyze how spatial memory strategies evolve in a virtual environment with increasing gaming experience. The results revealed that participants with more gaming experience exhibited faster and more precise learning in the virtual water maze, along with improved search strategies. Additionally, our study allowed for the evaluation of our software and enabled us to track changes in the efficacy of learning strategies. Overall, this study emphasizes the potential of virtual environments for both evaluation and cognitive stimulation purposes.","Psychological assessments of memory have historically been conducted using paper-based techniques. Nevertheless, the advent of virtual reality applications, including adaptations of experiments originally designed for animal laboratories, has introduced new methods for assessing and improving attention and memory skills. The influence of virtual settings on spatial memory is still a subject of considerable debate. To add to this discourse, we performed an extensive study with two main goals. Firstly, our aim was to create a virtual reality application capable of effectively evaluating search strategies in gamers operating within virtual settings. For this purpose, we developed a virtual water maze modeled after those used in animal studies. Secondly, we implemented a cross-sectional analysis involving participants to examine the changes in spatial memory strategies as gaming experience increases in a virtual environment. The findings indicated that participants with greater gaming experience demonstrated faster and more accurate learning in the virtual water maze, along with enhanced search strategies. Moreover, our study facilitated the assessment of our software and enabled the monitoring of the evolution of learning strategy efficiency. In sum, this research highlights the potential of virtual environments for both evaluation and cognitive stimulation.","Conventional assessments of psychological memory have long been rooted in traditional paper-based approaches, but the advent of virtual reality technologies, including adaptations of laboratory tests used with animals, has unlocked innovative avenues for examining and enhancing attentional and mnemonic processes. The influence of immersive virtual settings on spatial memory remains a contentious issue, sparking ongoing deliberation. To inform this discourse, we undertook an exhaustive investigation with two overarching goals. Firstly, we sought to create a virtual reality instrument capable of accurately evaluating navigational tactics employed by gamers within virtual realms. To accomplish this, we crafted a virtual aquatic labyrinth inspired by its animal research counterparts. Secondly, we conducted a cross-sectional analysis involving participants to examine the evolution of spatial memory strategies in response to increasing gaming proficiency within virtual environments. Our findings indicated that participants with greater gaming experience demonstrated accelerated and more precise learning within the virtual aquatic labyrinth, accompanied by enhanced search strategies. Furthermore, our study enabled the evaluation of our software and facilitated the tracking of changes in the effectiveness of learning strategies over time. Ultimately, this study underscores the vast potential of virtual environments for both evaluative and cognitive enhancement purposes.","Cognitive memory assessments have historically depended on traditional paper-and-pencil approaches. However, the advent of virtual reality technology, encompassing adaptations of tests originally designed for animal research, has ushered in novel avenues for evaluating and augmenting attention and memory functions. The influence of immersive virtual environments on spatial memory remains a subject of active discussion. Contributing to this discourse, our research pursued two key goals. First, we sought to create a virtual reality instrument capable of effectively gauging search tactics employed by video game players within virtual settings. This objective led to the development of a virtual water maze, modeled after those used in animal studies. Second, we undertook a cross-sectional investigation, involving a diverse group of individuals, to examine how spatial memory strategies develop within a virtual environment as gaming experience accumulates. Our findings indicated that participants with greater gaming proficiency demonstrated swifter and more accurate learning within the virtual water maze, accompanied by enhanced search strategies. Moreover, our study facilitated the assessment of our software's capabilities and allowed us to monitor shifts in the effectiveness of learning strategies. In conclusion, this research underscores the significant potential of virtual environments for both assessment and cognitive enhancement purposes."
102,Utilization of spent nuclear fuel and transuranic actinides in a two-component nuclear power industry,"According to the strategy for the development of the nuclear power industry in Russia for the first half of the 21st century, the nuclear power industry complex should undergo the initial stage in the formation of a two-component nuclear power system and a closed nuclear fuel cycle (NFC) infrastructure. All elements of such a nuclear power system, including NPPs, facilities for the fabrication of uranium and uranium-plutonium fuel, processing of spent nuclear fuel, and radioactive waste management, must be organically linked for producing competitive electricity in both domestic and foreign markets. The present article demonstrates the systemic benefits from a large-scale introduction of fast reactors for the Russian nuclear power industry in terms of a sustainable resource provision and a solution to the key problems of the final NFC stage, which is associated with the accumulation of spent fuel and transuranic actinides. The study proposes an optimum NFC closure scenario, in which the described problems can be solved without increasing the high parameters of the BR in a fast reactor and using special burners.","As per Russia's nuclear power industry development strategy for the first half of the 21st century, the sector is expected to enter an initial phase of creating a dual-component nuclear power structure alongside a closed nuclear fuel cycle (NFC) infrastructure. This nuclear system should integrate all components, such as nuclear power plants (NPPs), uranium and uranium-plutonium fuel production facilities, spent nuclear fuel processing plants, and radioactive waste management setups, to ensure competitively priced electricity for both domestic and international markets. This article highlights the systemic advantages of widely adopting fast reactors within the Russian nuclear power sector by ensuring sustainable resource use and addressing key challenges at the final NFC stage, particularly the accumulation of spent fuel and transuranic actinides. The study proposes an optimal NFC closure scenario that resolves these issues without escalating the high parameters of the breeder reactor in a fast reactor and utilizing special burners.","Russia's nuclear power industry development strategy for the 21st century's first half envisions a two-pronged system and a self-sustaining nuclear fuel cycle infrastructure as its inaugural phase. A seamless integration of all components, including nuclear power plants, uranium and uranium-plutonium fuel fabrication facilities, spent fuel processing, and radioactive waste management, is crucial for producing cost-competitive electricity in both local and international markets. This article highlights the systemic advantages of large-scale fast reactor adoption in Russia's nuclear power industry, ensuring a sustainable resource supply and resolving key challenges in the final nuclear fuel cycle stage, particularly the accumulation of spent fuel and transuranic actinides. The proposed optimal scenario for closing the nuclear fuel cycle addresses these issues without compromising the high-performance parameters of fast reactors or relying on specialized burners.","Russia's nuclear power strategy for the first half of the 21st century envisions a two-component system with a closed nuclear fuel cycle (NFC). This system, encompassing nuclear power plants, fuel fabrication facilities, spent fuel processing, and radioactive waste management, aims to generate cost-effective electricity for domestic and international markets. This article highlights the advantages of widespread fast reactor implementation in the Russian nuclear power sector, emphasizing its contribution to sustainable resource management and addressing challenges associated with spent fuel and transuranic actinide accumulation in the final NFC stage. The study presents an optimal NFC closure scenario that tackles these issues without necessitating increased breeding ratios in fast reactors or relying on specialized burners."
195,Cultural diversity and innovative entrepreneurship,"A growing empirical literature has established a positive relationship between cultural diversity and entrepreneurship, often attributing this effect to innovation benefits of diversity. However, not all entrepreneurship is inherently innovative, raising the question of whether cultural diversity may increase the likelihood of an entrepreneur pursuing an innovative instead of a more replicative business strategy. This study investigates the relationship between regional cultural diversity and the innovation orientation of early-stage entrepreneurs and considers moderating factors by decomposing shares of foreign-born population by origin (within and outside of the EU) and by education level. Combining survey data from the Global Entrepreneurship Monitor with population-based indicators of cultural diversity, we carry out a multilevel analysis for 140 European regions. The results suggest that entrepreneurs in culturally more diverse regions are significantly more likely to exhibit innovation orientation.","An expanding body of empirical research has shown a positive correlation between cultural diversity and entrepreneurship, often crediting this connection to the innovation advantages offered by diversity. Nonetheless, not all entrepreneurial ventures are innovative by nature, prompting the question of whether cultural diversity might influence an entrepreneur's inclination toward an innovative rather than a replicative business approach. This research examines the link between regional cultural diversity and the innovative tendencies of early-stage entrepreneurs, taking into account moderating factors by breaking down the foreign-born population based on origin (inside and outside the EU) and education level. Utilizing survey data from the Global Entrepreneurship Monitor alongside region-specific cultural diversity indicators, we conduct a multilevel analysis across 140 European regions. The findings indicate that entrepreneurs in regions with greater cultural diversity are significantly more inclined toward innovation.","A burgeoning body of research has unearthed a significant correlation between the richness of cultural heritage and entrepreneurial spirit, frequently crediting this phenomenon to the inventive advantages afforded by diverse perspectives. Nevertheless, not all entrepreneurial ventures are inherently groundbreaking, prompting inquiry into whether culturally diverse environments might incline entrepreneurs towards pioneering business models rather than replicative ones. This investigation delves into the nexus between regional cultural heterogeneity and the innovative proclivities of nascent entrepreneurs, while also examining the moderating influence of foreign-born population demographics, categorized by EU origin and educational attainment. By integrating Global Entrepreneurship Monitor survey data with population-based metrics of cultural diversity, a multilevel analysis of 140 European regions reveals that entrepreneurs operating in culturally rich environments are markedly more prone to exhibit an innovation-oriented mindset.","A burgeoning body of research has demonstrated a link between cultural heterogeneity and entrepreneurial activity, frequently citing the innovation-enhancing benefits of diversity as the underlying mechanism. Yet, not all entrepreneurial endeavors are intrinsically innovative, prompting the inquiry of whether cultural heterogeneity might influence an entrepreneur's propensity to adopt an innovation-focused business strategy over a more imitative one. This investigation examines the connection between regional cultural heterogeneity and the innovative disposition of nascent entrepreneurs, taking into account moderating variables by disaggregating foreign-born populations based on origin (EU and non-EU) and educational attainment. Merging survey data from the Global Entrepreneurship Monitor with population-based metrics of cultural heterogeneity, a multilevel analysis encompassing 140 European regions is conducted. The findings indicate a significantly higher likelihood of exhibiting an innovation-oriented approach among entrepreneurs situated in culturally richer regions."
3,Linking health impact and Post-environmental impact assessments: a case of municipal sewage treatment plant volatile organic compounds,"Deficiencies remain in current health impact assessment (HIA) and environmental impact assessment (EIA) projects. To address the shortcomings in EIA theory, a case of odors from a municipal sewage treatment plant (MSTP) was examined and geographic factors were employed to associate the spatial diffusion of the pollutants with the population’s activities based on land-use attributes. After screening the MSTP priority control pollutants, odors, hydrogen sulfide, and ammonia were selected for this study. Then, the spatial parameters for the pollutant simulation were surveyed, including parameters concerning the meteorological analysis, environmental emission monitoring, and emission source analysis, and a prediction of the pollutant diffusion as imaged and identified. The types of human social activity and exposure patterns were sorted as land-use attributes. An integration of the spatial diffusion of the pollutants with the exposure profiles of the scenario population according to the land-use attributes was achieved using counterpart spatial coordination factors. In our study, the commonly applied method of HIA risk calculation was followed and then extended by the spatial techniques introduced. The results of the scenario HIA contours are presented here, making it easy to determine the acceptable levels of the MSTP odor pollutants on a geographic scale. This study examines a significant approach to associate HIA with post-EIA via spatial factors and addresses the deficiencies of HIA in EIA empirical applications.","Current health impact assessments (HIA) and environmental impact assessments (EIA) have notable deficiencies. To address gaps in EIA theory, a case study on odors from a municipal sewage treatment plant (MSTP) was conducted, using geographic factors to link the spatial spread of pollutants with population activities classified by land-use characteristics. After identifying priority pollutants from the MSTP, odors, hydrogen sulfide, and ammonia were chosen for study. Spatial parameters for pollutant modeling, including meteorological analysis, environmental emission monitoring, and emission source analysis, were then collected, and pollutant diffusion was predicted. Human activity types and exposure patterns were organized by land-use attributes. The spatial diffusion of pollutants was integrated with the exposure profiles of the local population using corresponding spatial coordination factors. The typical HIA risk calculation method was applied and expanded with spatial techniques in our study. The resulting HIA contour maps for the scenario are presented, highlighting the acceptable levels of odor pollutants from the MSTP in relation to geography. This study explores a significant method for linking HIA with post-EIA through spatial factors and addresses the shortcomings of HIA in empirical EIA applications.","""Despite ongoing limitations in health impact assessments (HIAs) and environmental impact assessments (EIAs), a novel approach was developed to bridge the theoretical gaps in EIA by investigating the case of a municipal sewage treatment plant's (MSTP) odor emissions. By leveraging geographic factors, the study correlated the spatial dispersion of pollutants with human activities based on land-use characteristics. Following a thorough screening of priority pollutants, odors, hydrogen sulfide, and ammonia were identified as key targets. Subsequently, spatial parameters were surveyed, encompassing meteorological analysis, environmental emission monitoring, and emission source analysis, to predict pollutant diffusion patterns. Land-use attributes were categorized to reflect various human social activities and exposure patterns. By integrating pollutant dispersion with exposure profiles using spatial coordination factors, the study successfully linked the spatial distribution of pollutants with the scenario population's activities. Building upon conventional HIA risk calculation methods, this research introduced spatial techniques to enhance the approach. The resulting HIA contour maps provide a geographic framework for determining acceptable levels of MSTP odor pollutants, thereby addressing the shortcomings of HIA in EIA applications and offering a significant step towards integrating HIA with post-EIA via spatial factors.""","Existing health and environmental impact assessments still exhibit limitations. To tackle these shortcomings, a case study investigating odors emanating from a municipal sewage treatment plant was conducted. Geographic information was leveraged to link the spread of pollutants with population activities, classified by land use. Odors, hydrogen sulfide, and ammonia were chosen as the focus pollutants after a screening process. Subsequently, data collection for pollutant simulation encompassed meteorological analysis, environmental monitoring, and emission source analysis, leading to a visual representation of predicted pollutant dispersion. Human activities and exposure patterns were categorized by land-use attributes. By utilizing spatial coordination factors, the spatial distribution of pollutants was integrated with the predicted exposure profiles of the population based on their land use. A conventional health impact assessment (HIA) risk calculation method was employed and enhanced through the integration of spatial techniques. The resulting scenario HIA contours allow for the easy identification of acceptable odor pollutant levels on a geographic scale. This research presents a novel approach to connect HIA with post-environmental impact assessment using spatial factors, effectively addressing limitations of HIA in real-world applications."
71,Socio-economic impacts of solar energy technologies for sustainable green energy: a review,"Although fossil fuels leave environmentally hazardous gases like carbon dioxide, to date, global energy production is mostly dependent on these sources. Depletion of fossil resource and changes in the price make it a major concert for the sustainable use in future and utilization of energy resources which is environmentally safe and sustainable. Therefore, an increase in the use natural sustainable energy like solar power observed to be increased recently. Effective use of solar energy depends on the proper knowledge on its use and techniques. This article reviews different solar storage technologies to obtain green sustainable energy generation. We discussed the variation, mechanism, effectiveness, and worth of greenhouse for solar heat storage and concentrated solar power technologies (CSP). The multi-level evaluation method establishes an evaluation index system, adopts the expert scoring method to determine the weight and score of the index, and combines qualitative and quantitative to obtain a comprehensive evaluation value. There is the evaluation of the socio-economic impact of the green power station construction. The detail of for socio-economic environmental factor for on large-scale operation applications dependence on the use and conditions both solar storage and technologies have advantage and disadvantages. Finally, directions for significant of waste to energy for safe environment are presented.","While fossil fuels emit environmentally harmful gases like carbon dioxide, global energy production still largely relies on these resources. The depletion of fossil fuel reserves and fluctuations in their prices pose significant challenges for sustainable future use and the adoption of eco-friendly energy sources. Consequently, there has been a recent rise in the utilization of natural, sustainable energy, such as solar power. The effective deployment of solar energy hinges on a thorough understanding of its applications and technologies. This article examines various solar storage technologies aimed at achieving green, sustainable energy generation. We explore the differences, mechanisms, effectiveness, and value of greenhouses for solar heat storage, as well as concentrated solar power (CSP) technologies. A multi-level evaluation method creates an index system, uses expert scoring to determine index weight and scores, and integrates qualitative and quantitative data to produce a comprehensive evaluation value. The socio-economic impacts of constructing green power stations are assessed. The socio-economic and environmental factors influencing large-scale operations depend on the use and conditions of both solar storage and various technologies, each with its own pros and cons. Lastly, directions for the importance of converting waste to energy for environmental safety are discussed.","Despite the environmental risks associated with fossil fuels, including the release of harmful gases such as carbon dioxide, they remain the primary source of global energy production. However, the finite nature of fossil resources and price volatility have raised concerns about their long-term sustainability, prompting a shift towards environmentally friendly and sustainable energy alternatives. In recent years, there has been a notable increase in the adoption of natural, sustainable energy sources like solar power. The effective harnessing of solar energy relies on a thorough understanding of its applications and techniques. This review examines various solar storage technologies aimed at promoting eco-friendly energy generation. We analyze the differences, mechanisms, efficacy, and value of greenhouse-based solar heat storage and concentrated solar power (CSP) technologies. A multi-faceted evaluation approach is employed, incorporating expert assessments and a scoring system to determine the weight and score of each index, thereby providing a comprehensive evaluation. The socio-economic implications of constructing green power stations are also assessed. The suitability of solar storage and technologies for large-scale applications depends on various factors, including their advantages and disadvantages. Ultimately, the article highlights the importance of waste-to-energy conversion for a safe environment.","While the combustion of fossil fuels releases detrimental greenhouse gases like carbon dioxide, these resources remain the cornerstone of global energy production. However, dwindling reserves and price fluctuations necessitate the exploration of sustainable and environmentally benign energy alternatives for future use. Consequently, the adoption of renewable energy sources like solar power has witnessed a recent surge. Optimizing solar energy utilization hinges on a thorough understanding of its applications and associated technologies. This study delves into various solar storage technologies aimed at achieving sustainable green energy generation. We analyze the variability, mechanisms, efficacy, and economic viability of both greenhouse-based solar heat storage and concentrated solar power (CSP) technologies. A multi-faceted evaluation method, incorporating expert scoring and a hybrid qualitative-quantitative approach, is employed to comprehensively assess these technologies. Furthermore, the socio-economic ramifications of green power plant construction are examined. The analysis reveals that the socio-economic and environmental impacts of large-scale deployment of both solar storage and CSP technologies are contingent on specific usage patterns and prevailing conditions, each presenting its own set of merits and drawbacks. The article concludes by outlining key strategies for harnessing waste-to-energy solutions to foster a secure and sustainable environment."
86,A New Study of AI Artists for Changing the Movie Industries,"Due to the rise of artificial intelligence (AI) in the arts, this paper aims to explore the use of AI for reducing film production costs through the creation of realistic images. Additionally, we investigate whether AI can recreate the same character at the same age. Without needing to replace the original actor, qualitative data collection tools were employed to study three distinct population groups within the film industry: film industry professionals, moviegoers, and technologists. Our research reveals that AI, or AI artists in film production, still face limitations in significantly reducing production costs. Furthermore, it is crucial to engage a text expert in the image production process for films who possesses a comprehensive understanding of film principles in order to achieve images that align with the project’s requirements. Moreover, the introduction of the AI artist technique allows for the recreation of a character at the same age portrayed by the same actor, even if that actor may have passed away. Consequently, obtaining consent from the relatives of the actor or actress becomes a necessary step. Furthermore, the aspect of audience acceptance does not hold significant interest, as it demands a greater level of realism in both the image and the actors, surpassing what AI can provide. Therefore, this paper underscores the increasing influence of AI in the arts, particularly within film production, and examines its potential to reduce costs and recreate characters.","With the emergence of artificial intelligence (AI) in creative disciplines, this paper seeks to investigate AI's potential to lower film production expenses by generating lifelike images. Moreover, the study examines AI's ability to reproduce characters at a consistent age without replacing the initial actor. Utilizing qualitative data collection methods, the research examines three specific groups within the film industry: professionals, audiences, and technology experts. Our findings indicate that AI, or AI-based artists in film creation, face significant challenges in drastically cutting costs. Additionally, it is essential to involve a text expert in the image creation process for films who has an extensive understanding of cinematic principles to ensure the images meet the project’s needs. The AI artist technique also enables the portrayal of a character at the same age by the same actor, even posthumously, thus necessitating consent from the actor's family. Furthermore, audience acceptance is not a predominant concern, as it demands a higher degree of realism in both visuals and performances, which currently surpasses AI's capabilities. This paper thus highlights the growing impact of AI in the arts, especially in film production, and evaluates its potential for cost reduction and character recreation.","The burgeoning presence of artificial intelligence (AI) in the creative sphere has prompted this study to delve into the potential of AI-driven image generation for cost-effective film production. Moreover, we examine whether AI can faithfully recreate a character at a specific age, eliminating the need for original actor replacement. To gauge industry perceptions, we utilized qualitative data collection methods to survey three distinct cohorts: film professionals, cinema enthusiasts, and technology experts. Our findings indicate that, despite its promise, AI-assisted filmmaking still encounters significant hurdles in substantially reducing production expenditures. The integration of a film-savvy expert with a deep understanding of cinematic principles is essential to ensure AI-generated images meet project requirements. Notably, the AI artist technique enables the recreation of a character at a specific age, even if the original actor is no longer alive, necessitating consent from their relatives. However, audience acceptance is not a primary concern, as it demands an unprecedented level of realism, surpassing current AI capabilities. Ultimately, this paper highlights the growing impact of AI on the arts, particularly in film production, and its potential to drive cost savings and character recreation.","This study delves into the burgeoning role of artificial intelligence (AI) in filmmaking, specifically its capacity to generate lifelike imagery and potentially lower production expenses. Utilizing qualitative data from film professionals, audiences, and technology experts, the research examines the feasibility of using AI to recreate characters at specific ages without replacing the original actor. Findings suggest that while AI artists offer some promise in film production, their ability to substantially cut costs remains limited. Achieving imagery that aligns with a project's vision necessitates the involvement of a text expert well-versed in cinematic principles. Interestingly, AI techniques enable the recreation of characters at any age using the original actor's likeness, even posthumously, raising ethical considerations around consent from the actor's family. However, audience acceptance appears less of a concern, as achieving the desired realism in both imagery and performance remains beyond current AI capabilities. Ultimately, this paper highlights the growing influence of AI in the arts, particularly film, and explores its potential for cost reduction and character recreation."
183,Emerging new themes in green finance: a systematic literature review,"There is a need for an extensive understanding of the emerging themes and trends within the domain of green finance, which is still evolving. By conducting a systematic literature review on green finance, the purpose of this study is to identify the emerging themes that have garnered significant attention over the past 12 years. In order to identify the emerging themes in green finance, bibliometric analysis was performed on 978 publications that were published between 2011 and 2023 and were taken from the databases of Scopus and Web of Science. The author examined annual scientific production, journal distribution, countries scientific production, most relevant authors, most frequent words, areas where empirical research is lacking, words' frequency over time, trend topics, and themes of green finance. The outcome of the review identified the following seven themes: (i) green finance and environmental sustainability; (ii) green finance and investments; (iii) green finance and innovation; (iv) green finance policy/green credit guidelines; (v) green finance and economy; (vi) green finance and corporate social responsibility; (vii)trends/challenges/barriers/awareness of green finance. The analysis of these emerging themes will contribute to the existing corpus of knowledge and provide valuable insights into the landscape of green finance as it evolves.","An in-depth comprehension of the burgeoning themes and trends in the field of green finance, which is still in its developmental phase, is essential. This study aims to uncover the key emerging themes that have drawn considerable interest over the last 12 years by performing a systematic literature review on green finance. To pinpoint these emerging themes, a bibliometric analysis was executed on 978 publications sourced from the Scopus and Web of Science databases, covering the period from 2011 to 2023. The author assessed various metrics including annual scientific output, distribution of journals, scientific contributions by country, top authors, most prevalent terms, gaps in empirical research, temporal word frequency, trending topics, and principal themes within green finance. The review identified seven main themes: (i) green finance and environmental sustainability; (ii) green finance and investments; (iii) green finance and innovation; (iv) green finance policy/green credit guidelines; (v) green finance and the economy; (vi) green finance and corporate social responsibility; and (vii) challenges, barriers, and awareness in green finance. Analyzing these themes will enrich the current body of knowledge and offer valuable insights into the evolving landscape of green finance.","""A comprehensive grasp of the burgeoning concepts and patterns within the realm of sustainable financial practices is essential, given its ongoing development. This research aims to pinpoint the prominent areas of focus that have garnered substantial interest over the past dozen years through a meticulous examination of existing literature on green finance. To achieve this, a bibliometric analysis was conducted on 978 publications from 2011 to 2023, sourced from Scopus and Web of Science databases, examining annual research output, journal distribution, national contributions, influential authors, frequently occurring keywords, knowledge gaps, temporal keyword frequency, trending topics, and thematic areas in green finance. The review's outcome revealed seven distinct categories: (i) environmental stewardship through green finance; (ii) investment opportunities in green finance; (iii) innovative applications of green finance; (iv) regulatory frameworks and credit guidelines for green finance; (v) the economic implications of green finance; (vi) corporate social responsibility in green finance; and (vii) challenges, barriers, and awareness surrounding green finance. The analysis of these emerging categories will enrich the existing body of knowledge and offer valuable perspectives on the evolving landscape of green finance.""","This study delves into the evolving field of green finance, aiming to uncover prominent themes and trends that have emerged over the past twelve years. To achieve this, the authors conducted a comprehensive review of 978 publications from 2011 to 2023, sourced from reputable databases like Scopus and Web of Science. Using bibliometric analysis, the study explored various aspects such as publication trends, influential authors, and key terminology. This examination also revealed areas where empirical research is needed. The study ultimately identifies seven core themes within green finance, including its connection to environmental sustainability, investments, innovation, policy and guidelines, economic impact, corporate social responsibility, and the trends, challenges, and awareness surrounding its development. By shedding light on these emerging themes, the study enriches the existing understanding of green finance and provides valuable insights into its dynamic landscape."
7,Virtual reality and gamification in education: a systematic review,"This study aims to analyze the use of virtual reality and gamification in education by examining the existing literature. In addition to virtual reality, this study focuses on gamified virtual reality learning environments which refer to virtual reality learning environments that integrate gamification elements and mechanisms. Based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement, a systematic literature review was carried out. No limitations were set regarding educational level, type of study, subject, and publication year. The related articles were retrieved from 5 databases (ERIC, Google Scholar, IEEE, SCOPUS, and Web of Science). A total of 112 articles were included, 16 research questions were explored, and a thematic analysis was conducted. To evaluate the quality of the articles included, the Mixed Methods Appraisal Tool (MMAT) was used. According to the findings, gamification and virtual reality support several pedagogical theories and approaches. Their adoption to and integration into education can enrich and transform traditional teaching and learning and were assessed positively by students and teachers. Gamification elements significantly affected students’ achievements. In comparison to traditional learning environments, gamified virtual reality learning environments were more motivating, engaging, and interactive and offered more opportunities for personalized and collaborative learning. Through the realistic and interactive experiences offered, students’ immersion and social presence can be enhanced, knowledge acquisition can be improved, and material comprehension can be facilitated. Positive changes in student attitude, behavior, and mentality as well as improved cognitive, physical, and social–emotional development were observed. When using learning environments that integrate both virtual reality and gamification, students’ learning outcomes, motivation, engagement, and self-efficacy were increased. Additionally, students’ academic performance, active involvement, and satisfaction were improved. Students’ curiosity, imagination, focus, and interest were enhanced and their skills and competences were developed. Finally, gamified virtual reality emerged as an effective educational tool that can improve learning at all educational levels, subjects, and contexts.","This research seeks to evaluate the application of virtual reality and gamification in education through a comprehensive review of current literature. Beyond examining virtual reality, the focus extends to gamified virtual reality educational environments, which incorporate gamification features into VR settings. Utilizing the PRISMA guidelines, a systematic literature review was performed with no restrictions on educational level, study type, subject matter, or publication date. Relevant articles were sourced from five databases: ERIC, Google Scholar, IEEE, SCOPUS, and Web of Science. A total of 112 articles were selected, from which 16 research questions were investigated, and thematic analysis was executed. The Mixed Methods Appraisal Tool (MMAT) was employed to assess the quality of the included studies. The findings indicate that both gamification and virtual reality align with several educational theories and methodologies. Their integration into education can enhance and transform conventional teaching and learning approaches, receiving positive feedback from both students and educators. Gamification elements notably impacted students’ performance. Compared to traditional educational environments, gamified virtual reality settings were more motivating, engaging, and interactive, offering greater opportunities for personalized and collaborative learning. The realistic and engaging experiences provided served to enhance students’ immersion and social presence, improve knowledge acquisition, and aid in material comprehension. Observations showed positive shifts in student attitudes, behaviors, and mindsets, along with improvements in cognitive, physical, and social-emotional development. These integrated learning environments bolstered students’ learning outcomes, motivation, engagement, and self-efficacy. Additionally, increases in academic performance, active participation, and satisfaction were noted. The integration also fostered growth in students’ curiosity, imagination, focus, and interests, along with the development of their skills and competencies. Ultimately, gamified virtual reality proved to be an effective educational tool that can enhance learning across various educational levels, subjects, and contexts.","This research endeavors to investigate the application of immersive virtual reality and engaging gamification strategies in educational settings through a comprehensive examination of existing research literature. Specifically, it delves into the realm of gamified virtual reality learning platforms, which seamlessly integrate game design elements and mechanics into virtual reality environments. Guided by the rigorous methodology outlined in the PRISMA statement, a systematic review of the literature was conducted without imposing any constraints on educational level, study type, subject area, or publication date. A total of 112 relevant articles were retrieved from five prominent databases, yielding 16 research questions and a thematic analysis. The quality of the included studies was assessed using the MMAT tool. The findings suggest that the synergistic combination of gamification and virtual reality aligns with various pedagogical theories and approaches, holding the potential to revitalize and transform traditional teaching and learning practices. Both students and educators responded positively to the adoption of these innovative tools. Notably, gamification elements had a significant impact on student achievement. In contrast to conventional learning environments, gamified virtual reality platforms were found to be more stimulating, interactive, and engaging, offering enhanced opportunities for personalized and collaborative learning. By providing realistic and interactive experiences, these platforms can foster increased student immersion, social presence, and knowledge acquisition, while facilitating material comprehension. Furthermore, the integration of gamification and virtual reality was observed to positively influence student attitudes, behaviors, and mentalities, as well as cognitive, physical, and socio-emotional development. The use of these innovative learning environments was found to boost student learning outcomes, motivation, engagement, and self-efficacy, while improving academic performance, active participation, and satisfaction. Additionally, students' curiosity, imagination, focus, and interest were enhanced, and their skills and competencies were developed. Ultimately, gamified virtual reality emerged as a potent educational tool capable of enhancing learning across diverse educational levels, subjects, and contexts.","This research investigates the application of virtual reality and gamification in educational settings through a comprehensive review of existing research. Specifically, the study examines virtual reality learning environments enriched with gamification elements. Employing the PRISMA guidelines, a systematic literature review was conducted, encompassing publications from five prominent databases without constraints on educational level, study type, subject matter, or publication date. A total of 112 articles were analyzed to address 16 research questions through a thematic analysis approach. The quality of the included articles was rigorously evaluated using the MMAT tool. The results highlight that both gamification and virtual reality align with various pedagogical theories and practices. Integrating these technologies into education holds the potential to enhance and revolutionize traditional teaching and learning, garnering positive feedback from both students and educators. Notably, gamification elements were found to have a significant impact on student achievement. Compared to conventional learning environments, gamified virtual reality environments fostered higher levels of motivation, engagement, and interactivity, while also providing greater opportunities for personalized and collaborative learning. The immersive and interactive experiences facilitated by these environments led to enhanced student immersion, social presence, knowledge acquisition, and material comprehension. Positive shifts in student attitudes, behaviors, and mindsets were observed, accompanied by improvements in cognitive, physical, and socio-emotional development. Learning environments incorporating both virtual reality and gamification demonstrated a positive influence on student learning outcomes, motivation, engagement, and self-efficacy. Furthermore, they resulted in improvements in academic performance, active participation, and overall satisfaction. Students exhibited increased curiosity, imagination, focus, and interest, while also developing valuable skills and competencies. The study concludes that gamified virtual reality serves as a powerful educational tool capable of enhancing learning across all levels of education, subjects, and contexts."
188,Advancing green finance: a review of sustainable development,"This study comprehensively reviews the relationship between green finance and sustainable development, specifically focusing on combatting climate change and achieving carbon neutrality. Utilizing a narrative review methodology, the study examines a range of scholarly articles and publications to identify key themes, findings, and future directions in green finance. The review emphasizes the crucial role of substantial investments in green and low-carbon initiatives to address climate change effectively and promote sustainable economic growth. It highlights the necessity of robust regulatory frameworks that facilitate the availability of green finance and the integration of carbon–neutral practices. Additionally, the paper explores the potential of impact investing, wherein investors accept lower financial returns in exchange for non-financial benefits in green finance. It underscores the influential role of institutional ownership in guiding companies toward enhanced environmental and social performance. Moreover, integrating environmental, social, and governance (ESG) factors in investment decisions is critical for sustainable finance. Addressing the intersection of climate change and risk management, the review highlights the implications of environmental risks on financial decision-making. Effective communication strategies can raise public awareness and support for climate policies. The study concludes by calling for collaboration, further research, and policy measures to advance green finance and foster sustainable economic growth. It recommends aligning financial incentives with sustainable outcomes, fostering transparency, and incorporating social equity in green finance initiatives to contribute towards achieving sustainable development goals and promoting a greener future.","This research thoroughly examines the connection between green finance and sustainable development, with a particular emphasis on mitigating climate change and attaining carbon neutrality. Adopting a narrative review methodology, the study analyzes various academic papers and publications to pinpoint key themes, discoveries, and future directions in green finance. The review underscores the essential role of significant investments in green and low-carbon projects to address climate change effectively and stimulate sustainable economic growth. It stresses the need for strong regulatory frameworks to facilitate access to green finance and the adoption of carbon-neutral practices. Furthermore, the paper investigates the potential of impact investing, where investors accept reduced financial returns in exchange for non-financial benefits in green finance. It highlights the key role of institutional ownership in driving companies toward improved environmental and social performance. Additionally, the integration of environmental, social, and governance (ESG) factors in investment decisions is deemed crucial for sustainable finance. By addressing the nexus of climate change and risk management, the review underscores the influence of environmental risks on financial decision-making. Effective communication strategies are identified as a means to elevate public awareness and support for climate policies. The study concludes with a call for collaboration, further research, and policy interventions to progress green finance and encourage sustainable economic growth. It advocates aligning financial incentives with sustainable goals, promoting transparency, and incorporating social equity within green finance initiatives to aid in achieving sustainable development goals and fostering a greener future.","This comprehensive analysis delves into the intricate connection between environmentally responsible financial practices and long-term sustainability, with a particular emphasis on mitigating the impacts of climate change and realizing a carbon-neutral economy. Through a meticulous narrative review of existing literature, the study distills key patterns, insights, and future research avenues in the realm of green finance. The authors stress the imperative of substantial investments in eco-friendly and low-carbon projects to combat climate change effectively and drive sustainable economic expansion. They also underscore the need for robust regulatory frameworks that facilitate access to green finance and the adoption of carbon-neutral practices. Furthermore, the paper explores the potential of socially responsible investing, where investors trade off financial returns for non-monetary benefits in the green finance sector. It highlights the pivotal role of institutional investors in guiding companies toward enhanced environmental stewardship and social responsibility. The integration of environmental, social, and governance considerations into investment decisions is deemed crucial for sustainable financial practices. The review also examines the nexus between climate change and risk management, emphasizing the far-reaching implications of environmental risks on financial decision-making. Effective communication strategies can galvanize public support for climate policies. Ultimately, the study advocates for collaborative efforts, further research, and policy interventions to propel green finance and foster sustainable economic growth, recommending the alignment of financial incentives with sustainable outcomes, enhanced transparency, and the incorporation of social equity in green finance initiatives to contribute to the attainment of sustainable development goals and a more environmentally conscious future.","This in-depth analysis explores the interconnectedness of green finance and sustainable development, particularly in mitigating climate change and achieving carbon neutrality. Employing a narrative review approach, the study delves into a wide array of academic literature to discern key patterns, insights, and potential trajectories within the field. It stresses the vital importance of substantial financial commitments to environmentally friendly and low-carbon projects to effectively tackle climate change and drive sustainable economic progress. The analysis underscores the need for robust regulatory structures that streamline access to green financing and the incorporation of carbon-neutral practices. Furthermore, the paper probes the promise of impact investing, where investors prioritize environmental and social benefits over maximized financial returns. It emphasizes the significant influence of institutional investors in steering corporations towards improved environmental and social responsibility. The integration of ESG principles into investment strategies is also deemed crucial for sustainable finance. Examining the nexus of climate change and risk management, the review illuminates the impact of environmental risks on financial decision-making. The study emphasizes the need for effective communication strategies to enhance public understanding and support for climate-related policies. The analysis concludes by advocating for collaborative efforts, expanded research, and policy interventions to propel green finance and stimulate sustainable economic growth. It proposes aligning financial incentives with sustainable outcomes, promoting transparency, and embedding social equity within green finance initiatives to contribute to the realization of sustainable development goals and a more environmentally sound future."
1,Peer relationship increasing the risk of social media addiction among Chinese adolescents who have negative emotions,"Social media has expanded the scope and method of interpersonal communication, and presents the risk of social media addiction as well. This study reported the psychometrics of the Bergen Social Media Addiction Scale (BSMAS) and further analyzed the moderating role of peer relationship on the association between negative emotions and risk of social media addiction. A total of 1258 survey participants were asked to complete a set of scales online. Exploratory structural equation modeling enabled BSMAS to reveal a solid one-factor structure with satisfactory internal consistency coefficient. Criterion-related validity and variance explanation rate analysis implied a positive relationship between negative emotions and risk of social media addiction, and identified the significant contributions of negative emotions and peer relationship to the risk of social media addiction. Moderation analysis demonstrated that peer relationship positively moderated the effect of negative emotions on the risk of social media addiction. However, gender was not a moderator affecting negative emotions on the risk of social media addiction. BSMAS was a valid tool for measuring the risk of social media addiction among Chinese adolescents. Lastly, peer relationship is a positive moderator in influencing negative emotions and risk of social media addiction.","The reach and means of personal communication have been broadened by social media, which also brings the potential danger of social media addiction. This research detailed the psychometric properties of the Bergen Social Media Addiction Scale (BSMAS) and examined the moderating influence of peer relationships on the link between negative emotions and the likelihood of social media addiction. A total of 1258 individuals participated in an online survey, completing various scales. Through exploratory structural equation modeling, BSMAS was found to exhibit a robust one-factor structure with a reliable internal consistency coefficient. The analysis of criterion-related validity and explained variance suggested a positive correlation between negative emotions and the risk of social media addiction while noting the significant impact of negative emotions and peer relationships. Moderation analysis revealed that peer relationships enhanced the effect of negative emotions on the risk of social media addiction, whereas gender did not serve as a moderator. The BSMAS proved to be a valid measure for assessing the risk of social media addiction in Chinese adolescents. Ultimately, peer relationships were identified as a beneficial moderator in the context of negative emotions and social media addiction risk.","The proliferation of social media has revolutionized the dynamics of interpersonal interaction, but concurrently, it poses the threat of excessive usage. This investigation examined the psychometric properties of the Bergen Social Media Addiction Scale and probed the mediating influence of peer relationships on the link between adverse emotional states and the propensity for social media addiction. A total of 1258 online survey respondents completed a battery of scales, which were subsequently analyzed using exploratory structural equation modeling. The results revealed a robust one-factor structure for the BSMAS, accompanied by satisfactory internal consistency. Furthermore, the analysis demonstrated a positive correlation between negative emotions and the risk of social media addiction, with both negative emotions and peer relationships emerging as significant predictors of this risk. Notably, peer relationships were found to exert a positive moderating effect on the relationship between negative emotions and social media addiction, whereas gender did not exhibit a moderating influence. The BSMAS was thus validated as a reliable instrument for assessing social media addiction among Chinese adolescents, with peer relationships playing a crucial role in mitigating the impact of negative emotions on social media addiction.","The pervasiveness of social media, while revolutionizing interpersonal communication, brings with it the potential for addiction. This research examined the psychometric properties of the Bergen Social Media Addiction Scale (BSMAS) among 1258 Chinese adolescents, and investigated the influence of peer relationships on the link between negative emotions and the likelihood of social media addiction. Structural equation modeling confirmed the robust one-factor structure of the BSMAS, demonstrating its reliability. Further analysis revealed a strong correlation between negative emotions and social media addiction risk, highlighting the significant role of both factors. Importantly, peer relationships were found to positively moderate the impact of negative emotions on addiction risk, signifying their protective influence. Conversely, gender did not demonstrate a moderating effect. The study validates the BSMAS as an effective instrument for assessing social media addiction risk in this population and underscores the importance of positive peer relationships in mitigating the detrimental effects of negative emotions on this risk."
